diff --git a/Makefile b/Makefile
index f1e6a02..9c905da 100644
--- a/Makefile
+++ b/Makefile
@@ -1,7 +1,7 @@
 VERSION = 4
 PATCHLEVEL = 10
 SUBLEVEL = 0
-EXTRAVERSION =
+EXTRAVERSION = tlb-lazy-flush-optimize-sweep
 NAME = Fearless Coyote
 
 # *DOCUMENTATION*
diff --git a/arch/Kconfig b/arch/Kconfig
index 99839c2..52e67f8 100644
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@ -781,4 +781,14 @@ config VMAP_STACK
 	  the stack to map directly to the KASAN shadow map using a formula
 	  that is incorrect if the stack is in vmalloc space.
 
+# latr
+config LAZY_TLB_SHOOTDOWN
+	def_bool y
+
+config LAZY_MEM_FREE
+	def_bool n
+
+config LAZY_MIGRATION
+        def_bool n
+
 source "kernel/gcov/Kconfig"
diff --git a/arch/x86/include/asm/mmu.h b/arch/x86/include/asm/mmu.h
index f9813b6..65d5242 100644
--- a/arch/x86/include/asm/mmu.h
+++ b/arch/x86/include/asm/mmu.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 #ifndef _ASM_X86_MMU_H
 #define _ASM_X86_MMU_H
 
@@ -38,6 +39,17 @@ typedef struct {
 } mm_context_t;
 
 #ifdef CONFIG_SMP
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+struct flush_tlb_info {
+	struct mm_struct *flush_mm;
+	unsigned long flush_start;
+	unsigned long flush_end;
+};
+int __process_lazy_mm(struct mm_struct *mm, int force);
+int native_flush_save_state(struct flush_tlb_info *info,
+			const struct cpumask *cpumask,
+			int migration);
+#endif
 void leave_mm(int cpu);
 #else
 static inline void leave_mm(int cpu)
diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h
index 6fa8594..3c523a9 100644
--- a/arch/x86/include/asm/tlbflush.h
+++ b/arch/x86/include/asm/tlbflush.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 #ifndef _ASM_X86_TLBFLUSH_H
 #define _ASM_X86_TLBFLUSH_H
 
@@ -8,6 +9,28 @@
 #include <asm/cpufeature.h>
 #include <asm/special_insns.h>
 
+/* latr */
+#define get_min_time(t1, t2) do { \
+        if (!(t1)) \
+                (t1) = (t2); \
+        else if ((t1) > (t2)) \
+                (t1) = (t2); \
+} while (0)
+
+#define get_max_time(t1, t2) do { \
+        if (!(t1)) \
+                (t1) = (t2); \
+        else if ((t1) < (t2)) \
+                (t1) = (t2); \
+} while (0)
+
+#define update_tlbinfo_stats(t, time) do { \
+        get_min_time((t)->min_time, (time)); \
+        get_max_time((t)->max_time, (time)); \
+        (t)->total_time += (time); \
+} while(0)
+/*******/
+
 static inline void __invpcid(unsigned long pcid, unsigned long addr,
 			     unsigned long type)
 {
@@ -140,8 +163,16 @@ static inline void __native_flush_tlb(void)
 	 * task switch and therefore we must not be preempted while we write CR3
 	 * back:
 	 */
+/* latr */
+        u64 time = rdtsc();
+/*******/
 	preempt_disable();
 	native_write_cr3(native_read_cr3());
+/* latr */
+        this_cpu_inc(tlbinfo.global_tlbs.num_tlbs); 
+        update_tlbinfo_stats(&this_cpu_ptr(&tlbinfo)->global_tlbs,
+                             rdtsc() - time);
+/*******/
 	preempt_enable();
 }
 
@@ -159,6 +190,9 @@ static inline void __native_flush_tlb_global_irq_disabled(void)
 static inline void __native_flush_tlb_global(void)
 {
 	unsigned long flags;
+/* latr */
+        u64 time;
+/*******/
 
 	if (static_cpu_has(X86_FEATURE_INVPCID)) {
 		/*
@@ -175,15 +209,29 @@ static inline void __native_flush_tlb_global(void)
 	 * be called from deep inside debugging code.)
 	 */
 	raw_local_irq_save(flags);
+/* latr */
+        time = rdtsc();
+/*******/
 
 	__native_flush_tlb_global_irq_disabled();
 
+/* latr */
+        this_cpu_inc(tlbinfo.global_tlbs.num_tlbs); 
+        update_tlbinfo_stats(&this_cpu_ptr(&tlbinfo)->global_tlbs,
+                             rdtsc() - time);
+/*******/
 	raw_local_irq_restore(flags);
 }
 
 static inline void __native_flush_tlb_single(unsigned long addr)
 {
-	asm volatile("invlpg (%0)" ::"r" (addr) : "memory");
+/* latr */
+        /* u64 time = rdtsc(); */
+        asm volatile("invlpg (%0)" ::"r" (addr) : "memory");
+        /* this_cpu_inc(tlbinfo.inv_tlbs.num_tlbs);  */
+        /* update_tlbinfo_stats(&this_cpu_ptr(&tlbinfo)->inv_tlbs, */
+        /*                      rdtsc_ordered() - time); */
+/*******/
 }
 
 static inline void __flush_tlb_all(void)
@@ -330,5 +378,36 @@ static inline void reset_lazy_tlbstate(void)
 #define flush_tlb_others(mask, mm, start, end)	\
 	native_flush_tlb_others(mask, mm, start, end)
 #endif
+ 
+/* latr */
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+
+#define LAZY_TLB_NUM_ENTRIES 256
+
+void native_flush_saved_states(int cpu, int tick);
+
+typedef struct lazytlb_shootdown {
+        struct mm_struct *flush_mm;
+        struct cpumask cpu_mask;
+        unsigned long flush_start;
+        unsigned long flush_end;
+        unsigned long refs;
+        atomic_t valid;
+#ifdef CONFIG_LAZY_MIGRATION
+        atomic_t flags;
+#endif
+} lazytlb_shootdown_t;
+
+typedef struct shootdown_entries {
+        /* Identify where the current entries start and end. */
+        uint64_t start;
+        uint64_t end;
+        lazytlb_shootdown_t entries[LAZY_TLB_NUM_ENTRIES];
+} shootdown_entries_t;
+
+extern shootdown_entries_t __percpu *lazy_tlb_entries;
+
+#endif
+/*******/
 
 #endif /* _ASM_X86_TLBFLUSH_H */
diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c
index a7655f6..a054db7 100644
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@ -1,5 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 #include <linux/init.h>
-
+/* latr */
+#include <linux/kthread.h>
+/*******/
 #include <linux/mm.h>
 #include <linux/spinlock.h>
 #include <linux/smp.h>
@@ -28,13 +31,27 @@
  *	Implement flush IPI by CALL_FUNCTION_VECTOR, Alex Shi
  */
 
+/* latr */
+DEFINE_PER_CPU(struct tlbinfo, tlbinfo);
+
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+shootdown_entries_t __percpu *lazy_tlb_entries;
+static struct cpumask cpu_none = {CPU_BITS_NONE};
+#define TLB_SHOOTDOWN_MAX 2
+#endif
+/*******/
+
 #ifdef CONFIG_SMP
 
+/* latr */
+#ifndef CONFIG_LAZY_TLB_SHOOTDOWN
 struct flush_tlb_info {
 	struct mm_struct *flush_mm;
 	unsigned long flush_start;
 	unsigned long flush_end;
 };
+#endif
+/*******/
 
 /*
  * We cannot call mmdrop() because we are in interrupt context,
@@ -257,12 +274,390 @@ static void flush_tlb_func(void *info)
 
 }
 
+/* latr */
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+
+void native_flush_tlb_fallback(const struct cpumask *cpumask,
+			struct mm_struct *mm, unsigned long start,
+			unsigned long end)
+{
+	struct flush_tlb_info info;
+
+	if (end == 0)
+		end = start + PAGE_SIZE;
+	info.flush_mm = mm;
+	info.flush_start = start;
+	info.flush_end = end;
+
+	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH);
+        this_cpu_inc(tlbinfo.remote_tlbs.num_tlbs);
+	if (end == TLB_FLUSH_ALL)
+		trace_tlb_flush(TLB_REMOTE_SEND_IPI, TLB_FLUSH_ALL);
+	else
+		trace_tlb_flush(TLB_REMOTE_SEND_IPI,
+				(end - start) >> PAGE_SHIFT);
+
+	if (is_uv_system()) {
+		unsigned int cpu;
+
+		cpu = smp_processor_id();
+		cpumask = uv_flush_tlb_others(cpumask, mm, start, end, cpu);
+		if (cpumask)
+			smp_call_function_many(cpumask, flush_tlb_func,
+					&info, 1);
+		return;
+	}
+
+	smp_call_function_many(cpumask, flush_tlb_func, &info, 1);
+}
+
+// per cpu background thread handler - check cpu-mask and
+// invalidate entries
+void background_update_saved_states(int cpu)
+{
+	unsigned int i;
+	shootdown_entries_t *cpu_entry = NULL;
+	lazytlb_shootdown_t *state;
+
+	cpu_entry = this_cpu_ptr(lazy_tlb_entries);
+
+	for (i = 0; i < LAZY_TLB_NUM_ENTRIES; i++) {
+		state = &cpu_entry->entries[i];
+		if (atomic_read(&state->valid)) {
+			if (cpumask_equal(&state->cpu_mask, &cpu_none)) {
+				atomic_set(&state->valid, 0);
+			}
+
+			/* After MAX tries fallback to sending IPI */
+			if (atomic_read(&state->valid)) {
+				++state->refs;
+				if (state->refs >= TLB_SHOOTDOWN_MAX) {
+					/* reset valid to prevent flushes */
+					atomic_set(&state->valid, 0);
+					native_flush_tlb_fallback(&state->cpu_mask,
+								state->flush_mm,
+								state->flush_start,
+								state->flush_end);
+				}
+			}
+		}
+	}
+}
+
+#ifdef CONFIG_LAZY_MEM_FREE
+static int __process_lazy_vma(struct mm_struct *mm, int force)
+{
+	struct vm_area_struct *vma = mm->lazy_mmap, *next;
+	int pending = 0;
+
+        while (vma) {
+		next = vma->lazy_vmnext;
+		/* only free the entries after 2 HZ (1 extra buffer) */
+		if (((get_jiffies_64() - vma->lazy_jiffy) > 2*HZ) || force) {
+			/* lazy_detach_vmas_to_be_unmapped(mm, vma); */
+			/* Now lets free the VMAs */
+			lazy_remove_vma_list(mm, vma);
+		} else {
+			++pending;
+		}
+		vma = next;
+	}
+	return (!pending);
+}
+
+static int __process_lazy_pages(struct mm_struct *mm, int force)
+{
+	struct lazy_page_list *lpages, *tmp;
+	int pending = 0;
+
+	list_for_each_entry_safe(lpages, tmp, &mm->lazy_page_list_head,
+				page_node) {
+		if (((get_jiffies_64() - lpages->lazy_jiffy) > 2*HZ) || force) {
+			/* free pages */
+			list_del(&lpages->page_node);
+			lazy_free_pages(mm, lpages);
+#ifdef CONFIG_LAZY_MEM_FREE_DEBUG
+			if (force) {
+				lazy_free_pages(mm, lpages);
+			} else {
+				printk("Deleting %lld number of pages \n",lpages->nr);
+				for (i=0; i<10; i++)
+					printk("%d - %p \n",i,lpages->pages[i]);
+			}
+#endif
+		} else {
+			++pending;
+		}
+	}
+	return (!pending);
+}
+
+int __process_lazy_mm(struct mm_struct *mm, int force)
+{
+	int vret=0, pret=0;
+
+	/*
+	 * Remove lazy VMA list
+	 */
+	vret = __process_lazy_vma(mm, force);
+
+	/*
+	 * Remove pages
+	 */
+	pret = __process_lazy_pages(mm, force);
+	return !(vret|pret);
+}
+EXPORT_SYMBOL_GPL(__process_lazy_mm);
+
+static int process_lazy_mm(struct mm_struct *mm, int force)
+{
+	int ret = 1;
+	if (down_write_killable(&mm->mmap_sem)) {
+		goto done;
+	}
+	if (!atomic_read(&mm->in_lazy_list)) {
+		goto done;
+	}
+	ret = __process_lazy_mm(mm, force);
+	up_write(&mm->mmap_sem);
+done:
+	return ret;
+}
+#endif
+
+#ifdef CONFIG_LAZY_MEM_FREE
+static int background_lazy_memory(void *__unused)
+{
+	int done=0;
+	struct mm_struct *mm = NULL, *tmp = NULL;
+
+	while (!kthread_should_stop()) {
+		schedule_timeout_interruptible(HZ*1);
+		// Process the VMAs and pages in each MM.
+		spin_lock(&lazy_mm_lock);
+		list_for_each_entry_safe(mm, tmp, &lazy_mm_list,
+					lazy_mm_list) {
+			spin_unlock(&lazy_mm_lock);
+			done = process_lazy_mm(mm, 0);
+			spin_lock(&lazy_mm_lock);
+			if (done) {
+				list_del(&mm->lazy_mm_list);
+				/* Mark the entry as not present */
+				atomic_set(&mm->in_lazy_list, 0);
+			}
+		}
+		spin_unlock(&lazy_mm_lock);
+		/* printk("background lazy memory thread running \n"); */
+	}
+	return 0;
+}
+#endif
+
+/* High iter value, reduce if needed */
+static int background_tlb_invalidate(void *__unused)
+{
+	int cpu;
+
+	printk("TLB flush background thread started \n");
+	while (!kthread_should_stop()) {
+		schedule_timeout_interruptible(HZ*1);
+		/* printk("TLB flush background thread running \n"); */
+		for_each_online_cpu(cpu) {
+			background_update_saved_states(cpu);
+		}
+
+	}
+	return 0;
+}
+
+static uint32_t get_slot(const uint64_t counter) {
+  return counter % LAZY_TLB_NUM_ENTRIES;
+}
+
+// Function to flush all the saved states
+
+void native_flush_saved_states(int cpu_id, int tick)
+{
+	unsigned int cpu;
+	shootdown_entries_t *cpu_entry;
+	lazytlb_shootdown_t *state;
+	struct flush_tlb_info info;
+
+	/* TODO: This should be modified to cores in a socket */
+	for_each_online_cpu(cpu) {
+		cpu_entry = per_cpu_ptr(lazy_tlb_entries, cpu);
+
+        // Store current entry, starting with the start, incrementing towards
+        // end.
+        uint64_t end = cpu_entry->end;
+
+        uint64_t current_entry = cpu_entry->start;
+        uint32_t local_counter = 0;
+
+        while(current_entry != end && local_counter <= LAZY_TLB_NUM_ENTRIES) {
+            smp_rmb();
+            uint32_t current_slot = get_slot(current_entry);
+
+            state = &cpu_entry->entries[current_slot];
+	    if (!atomic_read(&state->valid)) {
+                goto loop_end;
+            }
+
+			if (cpumask_test_cpu(cpu_id, &state->cpu_mask)) {
+#ifdef CONFIG_LAZY_MIGRATION
+				// check to use task_work_add
+				if (atomic_read(&state->flags)) {
+					if (!tick)
+						continue;
+					if (cpumask_equal(&state->cpu_mask,
+							  cpu_online_mask)) {
+						change_prot_numa(
+							(struct vm_area_struct*)state->flush_mm,
+							state->flush_start,
+							state->flush_end, 2);
+					}
+					info.flush_mm = ((struct vm_area_struct*)state->flush_mm)->vm_mm;
+				} else {
+					info.flush_mm = state->flush_mm;
+				}
+#else
+				info.flush_mm = state->flush_mm;
+#endif
+				info.flush_start = state->flush_start;
+				info.flush_end = state->flush_end;
+
+				// Flush only the local TLB
+				flush_tlb_func(&info);
+
+				// Clear the CPU mask
+				cpumask_clear_cpu(cpu_id, &state->cpu_mask);
+
+                // Invalidate current entry, potentially incrementing start
+                // until the next valid entry (or end) is reached.
+				if (cpumask_equal(&state->cpu_mask, &cpu_none)) {
+                    // First, invalidate current entry.
+#ifdef CONFIG_LAZY_MIGRATION
+					atomic_set(&state->flags, 0);
+#endif
+					atomic_set(&state->valid, 0);
+				}
+			}
+loop_end:
+            ++current_entry;
+            ++local_counter;
+            if(local_counter > LAZY_TLB_NUM_ENTRIES) {
+                printk("BUG BUG BUG, more than LAZY_TLB_BUM_ENTRIES traversed, %llu, %llu, %llu!\n", cpu_entry->start, current_entry, end);
+            }
+        }
+	}
+
+    // Skip over all invalid entries until reaching a valid one or the end
+    // poiner for the local list.
+    // NOTE: This is currently not synchronized with other CPUs, an invalid
+    // entry might only get cleared after two rounds.
+
+	cpu_entry = this_cpu_ptr(lazy_tlb_entries);
+    // Store current entry, starting with the start, incrementing towards
+    // end.
+    uint64_t end = cpu_entry->end;
+    uint64_t current_entry = cpu_entry->start;
+    uint32_t local_counter = 0;
+
+    state = &cpu_entry->entries[get_slot(current_entry)];
+
+    // Iterate invalid entries until either reaching the end or a still valid
+    // entry.
+    while(!atomic_read(&state->valid) && current_entry != end) {
+        // Advance start by one and advance current_entry
+        ++cpu_entry->start;
+        ++current_entry;
+        ++local_counter;
+        state = &cpu_entry->entries[get_slot(current_entry)];
+        if(local_counter > LAZY_TLB_NUM_ENTRIES) {
+            printk("BUG BUG BUG, more than LAZY_TLB_BUM_ENTRIES traversed while reclaiming, %llu, %llu, %llu!\n", cpu_entry->start, current_entry, end);
+            smp_wmb();
+            return;
+        }
+    }
+    // Publish changes to start.
+    smp_wmb();
+}
+
+// Function to save the states
+
+int native_flush_save_state(struct flush_tlb_info *info,
+			const struct cpumask *cpumask,
+			int migration)
+{
+	shootdown_entries_t *cpu_entry;
+	lazytlb_shootdown_t *state;
+	uint32_t entry;
+    int valid;
+
+    // Be sure to get up-to-date information.
+    smp_rmb();
+	cpu_entry = this_cpu_ptr(lazy_tlb_entries);
+
+    // Check if there are any free entries.
+    // This means that start and end + 1 would fall on the same slot.
+    if(get_slot(cpu_entry->start) == get_slot(cpu_entry->end + 1)) {
+        return 0;
+    }
+
+	// identify free entry
+	entry = get_slot(cpu_entry->end);
+	state = &cpu_entry->entries[entry];
+	valid = atomic_read(&state->valid);
+    // In case the next entry is still valid, i.e. it is not flushed yet,
+    // return false and fall back.
+	if (valid) {
+        printk("BUG BUG BUG, entry should have been invalid but it is not, %llu, %llu!\n", cpu_entry->start, cpu_entry->end);
+        return 0;
+	}
+
+	// Update local TLB state
+	// atomic_set(&state->valid, 0);
+	state->flush_mm = info->flush_mm;
+	state->flush_start = info->flush_start;
+	state->flush_end = info->flush_end;
+	state->cpu_mask = *cpumask;
+	state->refs = 0;
+#ifdef CONFIG_LAZY_MIGRATION
+	if (migration) {
+		state->flags.counter = 1;
+	} else {
+		state->flags.counter = 0;
+	}
+#endif
+
+	// write barrier. increment count only after the
+	// data values are updated, to avoid instruction
+	// reordering.
+	smp_wmb();
+	atomic_set(&state->valid, 1);
+    ++cpu_entry->end;
+	smp_wmb();
+
+	return 1;
+}
+EXPORT_SYMBOL_GPL(native_flush_save_state);
+
+#endif
+/*******/
+
 void native_flush_tlb_others(const struct cpumask *cpumask,
 				 struct mm_struct *mm, unsigned long start,
 				 unsigned long end)
 {
 	struct flush_tlb_info info;
+/* latr */
+        u64 time;
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+	int ret=0;
+#endif
 
+        time = rdtsc();
+/*******/
 	if (end == 0)
 		end = start + PAGE_SIZE;
 	info.flush_mm = mm;
@@ -270,6 +665,9 @@ void native_flush_tlb_others(const struct cpumask *cpumask,
 	info.flush_end = end;
 
 	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH);
+/* latr */
+        this_cpu_inc(tlbinfo.remote_tlbs.num_tlbs);
+/*******/
 	if (end == TLB_FLUSH_ALL)
 		trace_tlb_flush(TLB_REMOTE_SEND_IPI, TLB_FLUSH_ALL);
 	else
@@ -286,7 +684,32 @@ void native_flush_tlb_others(const struct cpumask *cpumask,
 								&info, 1);
 		return;
 	}
+
+/* latr */
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+	// Flush the local TLB and save state only for unmap operation
+	if ((mm) && (atomic_read(&mm->munmap_inprogress))) {
+		flush_tlb_func(&info);
+		ret = native_flush_save_state(&info, cpumask, 0);
+		if(!ret) {
+			this_cpu_inc(tlbinfo.remote_tlbs.fallback_ipi);
+		}
+	}
+
+	if (!ret) {
+		/* No free entries available, fall back */
+		smp_call_function_many(cpumask, flush_tlb_func, &info, 1);
+		this_cpu_inc(tlbinfo.remote_tlbs.count_ipi);
+        }
+                update_tlbinfo_stats(&this_cpu_ptr(&tlbinfo)->remote_tlbs,
+                                     rdtsc_ordered() - time);
+#else
 	smp_call_function_many(cpumask, flush_tlb_func, &info, 1);
+        update_tlbinfo_stats(&this_cpu_ptr(&tlbinfo)->remote_tlbs,
+                             rdtsc_ordered() - time);
+    this_cpu_inc(tlbinfo.remote_tlbs.count_ipi);
+#endif
+/*******/
 }
 
 void flush_tlb_current_task(void)
@@ -322,10 +745,16 @@ void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
 				unsigned long end, unsigned long vmflag)
 {
 	unsigned long addr;
+/* latr */
+	u64 time;
+/*******/
 	/* do a global flush by default */
 	unsigned long base_pages_to_flush = TLB_FLUSH_ALL;
 
 	preempt_disable();
+/* latr */
+	time = rdtsc();
+/*******/
 	if (current->active_mm != mm) {
 		/* Synchronize with switch_mm. */
 		smp_mb();
@@ -361,6 +790,11 @@ void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
 		}
 	}
 	trace_tlb_flush(TLB_LOCAL_MM_SHOOTDOWN, base_pages_to_flush);
+/* latr */
+	this_cpu_inc(tlbinfo.inv_tlbs.num_tlbs);
+	update_tlbinfo_stats(&this_cpu_ptr(&tlbinfo)->inv_tlbs,
+			rdtsc_ordered() - time);
+/*******/
 out:
 	if (base_pages_to_flush == TLB_FLUSH_ALL) {
 		start = 0UL;
@@ -477,6 +911,53 @@ static const struct file_operations fops_tlbflush = {
 
 static int __init create_tlb_single_page_flush_ceiling(void)
 {
+/* latr */
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+	int align = 32;
+	struct task_struct *tlb_task;
+
+	 /* create per-core TLB memory to store state */
+	lazy_tlb_entries = __alloc_percpu(sizeof(shootdown_entries_t),
+					align);
+
+	shootdown_entries_t *cpu_entry;
+	lazytlb_shootdown_t *state;
+    int i;
+	unsigned int cpu;
+	for_each_possible_cpu(cpu) {
+		cpu_entry = per_cpu_ptr(lazy_tlb_entries, cpu);
+        cpu_entry->start = 0;
+        cpu_entry->end = 0;
+        for (i = 0; i < LAZY_TLB_NUM_ENTRIES; ++i) {
+            state = &cpu_entry->entries[i];
+            atomic_set(&state->valid, 0);
+        }
+    }
+
+	 /* Current one kthread. TODO: per-core or per-node kthread */
+	tlb_task = kthread_create(background_tlb_invalidate,
+			NULL, "TLB flush");
+	if (IS_ERR(tlb_task)) {
+		printk("TLB flush kernel thread not created\n");
+	}
+	wake_up_process(tlb_task);
+#endif
+
+#ifdef CONFIG_LAZY_MEM_FREE
+	struct task_struct *mem_task;
+
+	/* kthread to handle memory reclamation*/
+	mem_task = kthread_create(background_lazy_memory,
+			NULL, "Memory reclam");
+	if (IS_ERR(mem_task)) {
+		printk("Memory reclamation kernel thread not created\n");
+	}
+	wake_up_process(mem_task);
+#endif
+
+	// create the per-core TLB flush scratchpad memory needed
+/*******/
+
 	debugfs_create_file("tlb_single_page_flush_ceiling", S_IRUSR | S_IWUSR,
 			    arch_debugfs_dir, NULL, &fops_tlbflush);
 	return 0;
diff --git a/fs/proc/Makefile b/fs/proc/Makefile
index 12c6922..de3ecb4 100644
--- a/fs/proc/Makefile
+++ b/fs/proc/Makefile
@@ -14,6 +14,7 @@ proc-$(CONFIG_TTY)      += proc_tty.o
 proc-y	+= cmdline.o
 proc-y	+= consoles.o
 proc-y	+= cpuinfo.o
+proc-y	+= tlbinfo.o
 proc-y	+= devices.o
 proc-y	+= interrupts.o
 proc-y	+= loadavg.o
diff --git a/fs/proc/tlbinfo.c b/fs/proc/tlbinfo.c
new file mode 100644
index 0000000..28e174f
--- /dev/null
+++ b/fs/proc/tlbinfo.c
@@ -0,0 +1,109 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/proc_fs.h>
+#include <linux/sched.h>
+#include <linux/seq_file.h>
+#include <linux/time.h>
+#include <linux/kernel_stat.h>
+#include <linux/cputime.h>
+#include <linux/mm.h>
+
+#define get_min_time(t1, t2) do { \
+        if ((t1) > (t2)) \
+                (t1) = (t2); \
+} while (0)
+
+#define get_max_time(t1, t2) do { \
+        if ((t1) < (t2)) \
+                (t1) = (t2); \
+} while (0)
+
+
+DEFINE_PER_CPU(struct tlbinfo, copy_tlbinfo);
+
+static void percore_tlbinfo_func(void *d)
+{
+        struct tlbinfo *t = this_cpu_ptr(&copy_tlbinfo);
+        memcpy(t, this_cpu_ptr(&tlbinfo), sizeof(struct tlbinfo));
+}
+
+static void percore_tlbinforeset_func(void *d)
+{
+        memset(this_cpu_ptr(&tlbinfo), 0, sizeof(struct tlbinfo));
+}
+
+static int tlbinfo_proc_show(struct seq_file *m, void *v)
+{
+	int i;
+        u64 total_time = 0, total_intlb_time = 0;
+        u64 total_gtlb_time = 0, total_rtlb_time = 0;
+        u64 total_rtlbs = 0, total_gtlbs = 0, total_intlbs = 0;
+        u32 min_rtlb = 1UL << 31, min_gtlb = 1UL << 31, min_intlb = 1UL << 31;
+        u32 max_rtlb = 0, max_gtlb = 0, max_intlb = 0;
+        u32 total_fallback_ipi =0;
+        u32 total_count_ipi =0;
+
+        smp_call_function_many(cpu_online_mask, percore_tlbinfo_func, NULL, 1);
+
+        seq_printf(m, "CPU\tStlb\tTime\tMin\tMax\tGtlb\tTime\tMin\tMax\tRtlb\tTime\tMin\tMax (cycles)\tCount IPI\tFallback IPI\n");
+	for_each_possible_cpu(i) {
+                struct tlbinfo *t = per_cpu_ptr(&copy_tlbinfo, i);
+		seq_printf(m, "%d\t%Lu\t%Lu\t%u\t%u\t%Lu\t%Lu\t%u\t%u\t%Lu"
+                           "\t%Lu\t%u\t%u\t%u\t%u\n",
+                           i, t->inv_tlbs.num_tlbs, t->inv_tlbs.total_time,
+                           t->inv_tlbs.min_time, t->inv_tlbs.max_time,
+                           t->global_tlbs.num_tlbs, t->global_tlbs.total_time,
+                           t->global_tlbs.min_time, t->global_tlbs.max_time,
+                           t->remote_tlbs.num_tlbs, t->remote_tlbs.total_time,
+                           t->remote_tlbs.min_time, t->remote_tlbs.max_time,
+                           t->remote_tlbs.count_ipi, t->remote_tlbs.fallback_ipi);
+                total_time += t->remote_tlbs.total_time +
+                        t->inv_tlbs.total_time + t->global_tlbs.total_time;
+                total_intlb_time += t->inv_tlbs.total_time;
+                total_gtlb_time += t->global_tlbs.total_time;
+                total_rtlb_time += t->remote_tlbs.total_time;
+                total_intlbs += t->inv_tlbs.num_tlbs;
+                total_gtlbs += t->global_tlbs.num_tlbs;
+                total_rtlbs += t->remote_tlbs.num_tlbs;
+                total_count_ipi += t->remote_tlbs.count_ipi;
+                total_fallback_ipi += t->remote_tlbs.fallback_ipi;
+                get_min_time(min_intlb, t->inv_tlbs.min_time);
+                get_max_time(max_intlb, t->inv_tlbs.max_time);
+                get_min_time(min_gtlb, t->global_tlbs.min_time);
+                get_max_time(max_gtlb, t->global_tlbs.max_time);
+                get_min_time(min_rtlb, t->remote_tlbs.min_time);
+                get_max_time(max_rtlb, t->remote_tlbs.max_time);
+	}
+        seq_printf(m, "Total\t%Lu\t%Lu\t%Lu\t%u\t%u\t%Lu\t%Lu\t%u\t%u\t%Lu"
+                   "\t%Lu\t%u\t%u\t%u\t%u\n",
+                   total_time, total_intlbs, total_intlb_time,
+                   min_intlb, max_intlb,
+                   total_gtlbs, total_gtlb_time, min_gtlb, max_gtlb,
+                   total_rtlbs, total_rtlb_time, min_rtlb, max_rtlb,
+                   total_count_ipi, total_fallback_ipi);
+
+        smp_call_function_many(cpu_online_mask, percore_tlbinforeset_func,
+                        NULL, 1);
+
+	return 0;
+}
+
+static int tlbinfo_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, tlbinfo_proc_show, NULL);
+}
+
+static const struct file_operations tlbinfo_proc_fops = {
+	.open		= tlbinfo_proc_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int __init proc_tlbinfo_init(void)
+{
+	proc_create("tlbinfo", 0, NULL, &tlbinfo_proc_fops);
+	return 0;
+}
+fs_initcall(proc_tlbinfo_init);
diff --git a/include/linux/mm.h b/include/linux/mm.h
index b84615b..a8b047f 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 #ifndef _LINUX_MM_H
 #define _LINUX_MM_H
 
@@ -281,6 +282,27 @@ extern pgprot_t protection_map[16];
 #define FAULT_FLAG_REMOTE	0x80	/* faulting for non current tsk/mm */
 #define FAULT_FLAG_INSTRUCTION  0x100	/* The fault was during an instruction fetch */
 
+/* latr */
+struct __tlbinfo {
+        u32 min_time;
+        u32 max_time;
+        u64 total_time;
+
+        u64 num_tlbs;
+
+        u32 count_ipi;
+        u32 fallback_ipi;
+};
+
+struct tlbinfo {
+        struct __tlbinfo inv_tlbs;
+        struct __tlbinfo global_tlbs;
+        struct __tlbinfo remote_tlbs;
+};
+
+DECLARE_PER_CPU(struct tlbinfo, tlbinfo);
+/*******/
+
 /*
  * vm_fault is filled by the the pagefault handler and passed to the vma's
  * ->fault function. The vma's ->fault is responsible for returning a bitmask
@@ -2176,10 +2198,12 @@ static inline void vma_set_page_prot(struct vm_area_struct *vma)
 }
 #endif
 
+/* latr */
 #ifdef CONFIG_NUMA_BALANCING
 unsigned long change_prot_numa(struct vm_area_struct *vma,
-			unsigned long start, unsigned long end);
+			unsigned long start, unsigned long end, int prot_numa);
 #endif
+/*******/
 
 struct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);
 int remap_pfn_range(struct vm_area_struct *, unsigned long addr,
@@ -2338,6 +2362,17 @@ void vmemmap_free(unsigned long start, unsigned long end);
 void register_page_bootmem_memmap(unsigned long section_nr, struct page *map,
 				  unsigned long size);
 
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+void lazy_remove_vma_list(struct mm_struct *mm,
+			struct vm_area_struct *vma);
+void lazy_detach_vmas_to_be_unmapped(struct mm_struct *mm,
+				struct vm_area_struct *vma);
+void lazy_free_pages(struct mm_struct *mm,
+		struct lazy_page_list *lpages);
+#endif
+/*******/
+
 enum mf_flags {
 	MF_COUNT_INCREASED = 1 << 0,
 	MF_ACTION_REQUIRED = 1 << 1,
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 808751d..bb83b5f 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 #ifndef _LINUX_MM_TYPES_H
 #define _LINUX_MM_TYPES_H
 
@@ -358,6 +359,14 @@ struct vm_area_struct {
 	struct mempolicy *vm_policy;	/* NUMA policy for the VMA */
 #endif
 	struct vm_userfaultfd_ctx vm_userfaultfd_ctx;
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+	/* List to add the VMAs to the lazy list */
+	struct vm_area_struct *lazy_vmnext, *lazy_vmprev;
+        u64 lazy_jiffy;
+        atomic_t is_marked_lazy;
+#endif
+/*******/
 };
 
 struct core_thread {
@@ -392,6 +401,25 @@ struct mm_rss_stat {
 	atomic_long_t count[NR_MM_COUNTERS];
 };
 
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+struct lazy_page_list {
+	unsigned int nr;
+	u64 lazy_jiffy;
+        struct list_head page_node;
+	struct page *pages[0];
+};
+
+struct lazy_mm_list {
+	/* spin lock for this global list */
+	spinlock_t lock;
+	struct list_head head;
+};
+extern spinlock_t lazy_mm_lock;
+extern struct list_head lazy_mm_list;
+#endif
+/*******/
+
 struct kioctx_table;
 struct mm_struct {
 	struct vm_area_struct *mmap;		/* list of VMAs */
@@ -513,6 +541,20 @@ struct mm_struct {
 	atomic_long_t hugetlb_usage;
 #endif
 	struct work_struct async_put_work;
+
+/* latr */
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+	atomic_t munmap_inprogress;
+#endif
+
+#ifdef CONFIG_LAZY_MEM_FREE
+	/* list of vmas and pages for lazy free */
+	struct vm_area_struct *lazy_mmap;
+	struct list_head lazy_page_list_head;
+	struct list_head lazy_mm_list;
+	atomic_t in_lazy_list;
+#endif
+/*******/
 };
 
 static inline void mm_init_cpumask(struct mm_struct *mm)
@@ -612,4 +654,10 @@ typedef struct {
 	unsigned long val;
 } swp_entry_t;
 
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+int update_lazy_list(struct mm_struct *mm, struct page **page, unsigned int nr);
+#endif
+/*******/
+
 #endif /* _LINUX_MM_TYPES_H */
diff --git a/kernel/fork.c b/kernel/fork.c
index 11c5c8a..fec843e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  linux/kernel/fork.c
  *
@@ -611,6 +612,12 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 			goto fail_nomem;
 		*tmp = *mpnt;
 		INIT_LIST_HEAD(&tmp->anon_vma_chain);
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+		atomic_set(&tmp->is_marked_lazy, 0);
+
+#endif
+/*******/
 		retval = vma_dup_policy(mpnt, tmp);
 		if (retval)
 			goto fail_nomem_policy;
@@ -773,6 +780,14 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	mm->pmd_huge_pte = NULL;
 #endif
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+	mm->lazy_mmap = NULL;
+	atomic_set(&mm->in_lazy_list, 0);
+	INIT_LIST_HEAD(&mm->lazy_mm_list);
+	INIT_LIST_HEAD(&mm->lazy_page_list_head);
+#endif
+/*******/
 
 	if (current->mm) {
 		mm->flags = current->mm->flags & MMF_INIT_MASK;
@@ -870,6 +885,18 @@ static inline void __mmput(struct mm_struct *mm)
 		list_del(&mm->mmlist);
 		spin_unlock(&mmlist_lock);
 	}
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+	/* free the lazy VMAs and pages too */
+	if (atomic_read(&mm->in_lazy_list)) {
+		__process_lazy_mm(mm, 1);
+		spin_lock(&lazy_mm_lock);
+		list_del(&mm->lazy_mm_list);
+		spin_unlock(&lazy_mm_lock);
+		atomic_set(&mm->in_lazy_list, 0);
+	}
+#endif
+/*******/
 	if (mm->binfmt)
 		module_put(mm->binfmt->module);
 	set_bit(MMF_OOM_SKIP, &mm->flags);
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index c56fb57..ebec741 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1,3 +1,5 @@
+// SPDX-License-Identifier: GPL-2.0-only
+:qa
 /*
  *  kernel/sched/core.c
  *
@@ -3094,6 +3096,12 @@ void scheduler_tick(void)
 	rq->idle_balance = idle_cpu(cpu);
 	trigger_load_balance(rq);
 #endif
+/* latr */
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+	/* flush TLB entries during scheduler tick */
+	native_flush_saved_states(cpu, 1);
+#endif
+/*******/
 	rq_last_tick_reset(rq);
 }
 
@@ -3394,6 +3402,13 @@ static void __sched notrace __schedule(bool preempt)
 	clear_preempt_need_resched();
 	rq->clock_skip_update = 0;
 
+/* latr */
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+	/* before context switch flush the saved TLB entries */
+	native_flush_saved_states(cpu, 0);
+#endif
+/*******/
+
 	if (likely(prev != next)) {
 		rq->nr_switches++;
 		rq->curr = next;
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 6559d19..face1a9 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Completely Fair Scheduling (CFS) Class (SCHED_NORMAL/SCHED_BATCH)
  *
@@ -2423,6 +2424,12 @@ void task_numa_work(struct callback_head *work)
 	unsigned long start, end;
 	unsigned long nr_pte_updates = 0;
 	long pages, virtpages;
+/* latr */
+#ifdef CONFIG_LAZY_MIGRATION
+	struct flush_tlb_info info;
+#endif
+/*******/
+
 
 	SCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));
 
@@ -2507,7 +2514,26 @@ void task_numa_work(struct callback_head *work)
 			start = max(start, vma->vm_start);
 			end = ALIGN(start + (pages << PAGE_SHIFT), HPAGE_SIZE);
 			end = min(end, vma->vm_end);
-			nr_pte_updates = change_prot_numa(vma, start, end);
+/* latr */
+#ifndef CONFIG_LAZY_MIGRATION
+			nr_pte_updates = change_prot_numa(vma, start, end, 1);
+#else
+			if (!is_vm_hugetlb_page(vma)) {
+				info.flush_mm = (struct mm_struct*) vma;
+				info.flush_start = start;
+				info.flush_end = end;
+				nr_pte_updates = native_flush_save_state(&info,
+								cpu_online_mask, 1);
+				if (!nr_pte_updates)
+					nr_pte_updates = change_prot_numa(vma,
+									start,
+									end, 1);
+			} else {
+				nr_pte_updates = change_prot_numa(vma,
+								start, end, 1);
+			}
+#endif
+/*******/
 
 			/*
 			 * Try to scan sysctl_numa_balancing_size worth of
diff --git a/mm/internal.h b/mm/internal.h
index 7aa2ea0..b784951 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /* internal.h: mm/ internal definitions
  *
  * Copyright (C) 2004 Red Hat, Inc. All Rights Reserved.
@@ -269,6 +270,13 @@ static inline bool is_data_mapping(vm_flags_t flags)
 void __vma_link_list(struct mm_struct *mm, struct vm_area_struct *vma,
 		struct vm_area_struct *prev, struct rb_node *rb_parent);
 
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+void __vma_add_lazy_list(struct mm_struct *mm, struct vm_area_struct *vma);
+void __vma_del_lazy_list(struct mm_struct *mm, struct vm_area_struct *vma);
+#endif
+/*******/
+
 #ifdef CONFIG_MMU
 extern long populate_vma_page_range(struct vm_area_struct *vma,
 		unsigned long start, unsigned long end, int *nonblocking);
diff --git a/mm/madvise.c b/mm/madvise.c
index 0e3828e..e243454 100644
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *	linux/mm/madvise.c
  *
@@ -476,7 +477,15 @@ static long madvise_dontneed(struct vm_area_struct *vma,
 	if (vma->vm_flags & (VM_LOCKED|VM_HUGETLB|VM_PFNMAP))
 		return -EINVAL;
 
-	zap_page_range(vma, start, end - start, NULL);
+/* latr */
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+    atomic_set(&vma->vm_mm->munmap_inprogress, 1);
+#endif
+    zap_page_range(vma, start, end - start, NULL);
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+    atomic_set(&vma->vm_mm->munmap_inprogress, 0);
+#endif
+/*******/
 	return 0;
 }
 
diff --git a/mm/memory.c b/mm/memory.c
index 6bf2b47..467326f 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  linux/mm/memory.c
  *
@@ -254,10 +255,27 @@ static void tlb_flush_mmu_tlbonly(struct mmu_gather *tlb)
 static void tlb_flush_mmu_free(struct mmu_gather *tlb)
 {
 	struct mmu_gather_batch *batch;
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+	int ret=0;
+#endif
 
 	for (batch = &tlb->local; batch && batch->nr; batch = batch->next) {
+/* latr */
+#ifndef CONFIG_LAZY_MEM_FREE
 		free_pages_and_swap_cache(batch->pages, batch->nr);
-		batch->nr = 0;
+#else
+		if ((atomic_read(&tlb->mm->munmap_inprogress))
+			&& (batch->nr<500)) {
+			ret = update_lazy_list(tlb->mm, batch->pages, batch->nr);
+		}
+
+		if (!ret) {
+			free_pages_and_swap_cache(batch->pages, batch->nr);
+		}
+#endif
+                batch->nr = 0;
+/*******/
 	}
 	tlb->active = &tlb->local;
 }
@@ -801,6 +819,33 @@ out:
 	return pfn_to_page(pfn);
 }
 
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+int update_lazy_list(struct mm_struct *mm, struct page **page, unsigned int nr)
+{
+	struct lazy_page_list *l;
+	int i;
+
+	l = (void *)__get_free_pages(GFP_NOWAIT | __GFP_NOWARN, 0);
+        if (!l) {
+                /* Need to fix this part */
+                /* Maybe go with a static allocation of the
+                 * arrays
+                 */
+		/* BUG(); */
+		return 0;
+	}
+	l->nr = nr;
+	for (i=0; i<l->nr; i++)
+		l->pages[i] = page[i];
+	l->lazy_jiffy = get_jiffies_64();
+	INIT_LIST_HEAD(&l->page_node);
+	list_add(&l->page_node, &mm->lazy_page_list_head);
+	return 1;
+}
+#endif
+/*******/
+
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 struct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,
 				pmd_t pmd)
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 1e7873e..a36ec81 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Simple NUMA memory policy for the Linux kernel.
  *
@@ -598,17 +599,19 @@ unlock:
  * an architecture makes a different choice, it will need further
  * changes to the core.
  */
+/* latr */
 unsigned long change_prot_numa(struct vm_area_struct *vma,
-			unsigned long addr, unsigned long end)
+			unsigned long addr, unsigned long end, int prot_numa)
 {
 	int nr_updated;
 
-	nr_updated = change_protection(vma, addr, end, PAGE_NONE, 0, 1);
+	nr_updated = change_protection(vma, addr, end, PAGE_NONE, 0, prot_numa);
 	if (nr_updated)
 		count_vm_numa_events(NUMA_PTE_UPDATES, nr_updated);
 
 	return nr_updated;
 }
+/*******/
 #else
 static unsigned long change_prot_numa(struct vm_area_struct *vma,
 			unsigned long addr, unsigned long end)
@@ -647,7 +650,9 @@ static int queue_pages_test_walk(unsigned long start, unsigned long end,
 		if (!is_vm_hugetlb_page(vma) &&
 			(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)) &&
 			!(vma->vm_flags & VM_MIXEDMAP))
-			change_prot_numa(vma, start, endvma);
+/* latr */
+			change_prot_numa(vma, start, endvma, 1);
+/********/
 		return 1;
 	}
 
diff --git a/mm/mmap.c b/mm/mmap.c
index dc4291d..7f5a611 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * mm/mmap.c
  *
@@ -67,6 +68,13 @@ const int mmap_rnd_compat_bits_max = CONFIG_ARCH_MMAP_RND_COMPAT_BITS_MAX;
 int mmap_rnd_compat_bits __read_mostly = CONFIG_ARCH_MMAP_RND_COMPAT_BITS;
 #endif
 
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+DEFINE_SPINLOCK(lazy_mm_lock);
+LIST_HEAD(lazy_mm_list);
+#endif
+/********/
+
 static bool ignore_rlimit_data;
 core_param(ignore_rlimit_data, ignore_rlimit_data, bool, 0644);
 
@@ -1649,6 +1657,11 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 	vma->vm_page_prot = vm_get_page_prot(vm_flags);
 	vma->vm_pgoff = pgoff;
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+	atomic_set(&vma->is_marked_lazy, 0);
+#endif
+/********/
 
 	if (file) {
 		if (vm_flags & VM_DENYWRITE) {
@@ -2516,6 +2529,11 @@ static int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 	*new = *vma;
 
 	INIT_LIST_HEAD(&new->anon_vma_chain);
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+	atomic_set(&new->is_marked_lazy, 0);
+#endif
+/********/
 
 	if (new_below)
 		new->vm_end = addr;
@@ -2574,6 +2592,96 @@ int split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 	return __split_vma(mm, vma, addr, new_below);
 }
 
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+
+void lazy_free_pages(struct mm_struct *mm,
+		struct lazy_page_list *lpages)
+{
+	if (lpages) {
+		free_pages_and_swap_cache(lpages->pages, lpages->nr);
+		free_pages((unsigned long)lpages, 0);
+	}
+}
+EXPORT_SYMBOL(lazy_free_pages);
+
+void lazy_remove_vma_list(struct mm_struct *mm,
+			struct vm_area_struct *vma)
+{
+	unsigned long nr_accounted = 0;
+	struct vm_area_struct *first_vma = vma;
+
+	if (!(atomic_read(&vma->is_marked_lazy))) {
+		__vma_del_lazy_list(mm, vma);
+		return;
+	}
+
+	/* Update high watermark before we lower total_vm */
+	update_hiwater_vm(mm);
+	do {
+		long nrpages = vma_pages(vma);
+
+		if (vma->vm_flags & VM_ACCOUNT)
+			nr_accounted += nrpages;
+		vm_stat_account(mm, vma->vm_flags, -nrpages);
+		atomic_set(&vma->is_marked_lazy, 0);
+		vma = remove_vma(vma);
+	} while (vma && (atomic_read(&vma->is_marked_lazy)));
+
+	/* delete VMA from the lazy list */
+	__vma_del_lazy_list(mm, first_vma);
+	vm_unacct_memory(nr_accounted);
+	validate_mm(mm);
+}
+EXPORT_SYMBOL(lazy_remove_vma_list);
+
+void lazy_detach_vmas_to_be_unmapped(struct mm_struct *mm,
+				struct vm_area_struct *vma)
+{
+	struct vm_area_struct **insertion_point;
+	struct vm_area_struct *tail_vma = NULL, *prev= vma->vm_prev;
+
+	if (!(atomic_read(&vma->is_marked_lazy)))
+		return;
+
+	insertion_point = (prev ? &prev->vm_next : &mm->mmap);
+	vma->vm_prev = NULL;
+	do {
+		vma_rb_erase(vma, &mm->mm_rb);
+		mm->map_count--;
+		tail_vma = vma;
+		vma = vma->vm_next;
+	} while (vma && (atomic_read(&vma->is_marked_lazy)));
+	*insertion_point = vma;
+	if (vma) {
+		vma->vm_prev = prev;
+		vma_gap_update(vma);
+	} else
+		mm->highest_vm_end = prev ? prev->vm_end : 0;
+	tail_vma->vm_next = NULL;
+
+	/* Kill the cache */
+	vmacache_invalidate(mm);
+}
+EXPORT_SYMBOL(lazy_detach_vmas_to_be_unmapped);
+
+void add_vmas_to_lazy_list(struct mm_struct *mm,
+			struct vm_area_struct *vma,
+			unsigned long start,
+			unsigned long end)
+{
+	/* add the first on to the list */
+	vma->lazy_vmprev = vma->lazy_vmnext = NULL;
+	__vma_add_lazy_list(mm, vma);
+	do {
+		atomic_set(&vma->is_marked_lazy, 1);
+		vma = vma->vm_next;
+	} while (vma && vma->vm_start < end);
+}
+
+#endif
+/********/
+
 /* Munmap is split into 2 main parts -- this part which finds
  * what needs doing, and the areas themselves, which do the
  * work.  This now handles partial unmappings.
@@ -2650,6 +2758,8 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len)
 		}
 	}
 
+/* latr */
+#ifndef CONFIG_LAZY_MEM_FREE
 	/*
 	 * Remove the vma's, and unmap the actual pages
 	 */
@@ -2660,6 +2770,35 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len)
 
 	/* Fix up all other VM information */
 	remove_vma_list(mm, vma);
+#else
+	detach_vmas_to_be_unmapped(mm, vma, prev, end);
+	if (atomic_read(&mm->munmap_inprogress)) {
+		/* add entries to the lazy list */
+		add_vmas_to_lazy_list(mm, vma, start, end);
+	}
+
+	/*
+	 * Remove only the PTE entries without detaching
+	 * the vma. vmas and physical pages will be
+	 * detached and removed lazily
+	 */
+	unmap_region(mm, vma, prev, start, end);
+
+	/* Notify MPX about the unmap using arch_unmap */
+	arch_unmap(mm, vma, start, end);
+
+	if (atomic_read(&mm->munmap_inprogress)) {
+		/* add mm to global list, only if *not* already added*/
+		if (!atomic_cmpxchg(&mm->in_lazy_list, 0, 1)) {
+			spin_lock(&lazy_mm_lock);
+			list_add(&mm->lazy_mm_list, &lazy_mm_list);
+			spin_unlock(&lazy_mm_lock);
+		}
+	} else {
+		remove_vma_list(mm, vma);
+	}
+#endif
+/********/
 
 	return 0;
 }
@@ -2686,7 +2825,17 @@ SYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)
 	profile_munmap(addr);
 	if (down_write_killable(&mm->mmap_sem))
 		return -EINTR;
+/* latr */
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+	atomic_set(&mm->munmap_inprogress, 1);
+#endif
+/********/
 	ret = do_munmap(mm, addr, len);
+/* latr */
+#ifdef CONFIG_LAZY_TLB_SHOOTDOWN
+	atomic_set(&mm->munmap_inprogress, 0);
+#endif
+/********/
 	up_write(&mm->mmap_sem);
 	return ret;
 }
@@ -2872,6 +3021,11 @@ static int do_brk(unsigned long addr, unsigned long request)
 	}
 
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+	atomic_set(&vma->is_marked_lazy, 0);
+#endif
+/********/
 	vma->vm_mm = mm;
 	vma->vm_start = addr;
 	vma->vm_end = addr + len;
@@ -3053,6 +3207,11 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 		if (vma_dup_policy(vma, new_vma))
 			goto out_free_vma;
 		INIT_LIST_HEAD(&new_vma->anon_vma_chain);
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+		atomic_set(&new_vma->is_marked_lazy, 0);
+#endif
+/********/
 		if (anon_vma_clone(new_vma, vma))
 			goto out_free_mempol;
 		if (new_vma->vm_file)
@@ -3191,6 +3350,11 @@ static struct vm_area_struct *__install_special_mapping(
 		return ERR_PTR(-ENOMEM);
 
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+	atomic_set(&vma->is_marked_lazy, 0);
+#endif
+/********/
 	vma->vm_mm = mm;
 	vma->vm_start = addr;
 	vma->vm_end = addr + len;
diff --git a/mm/mprotect.c b/mm/mprotect.c
index f9c07f5..1f3cd4b 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  mm/mprotect.c
  *
@@ -251,8 +252,10 @@ static unsigned long change_protection_range(struct vm_area_struct *vma,
 	} while (pgd++, addr = next, addr != end);
 
 	/* Only flush the TLB if we actually modified any entries: */
-	if (pages)
+/* latr */
+	if (pages && (prot_numa == 1))
 		flush_tlb_range(vma, start, end);
+/********/
 	clear_tlb_flush_pending(mm);
 
 	return pages;
diff --git a/mm/util.c b/mm/util.c
index 3cb2164..e2c2a82 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 #include <linux/mm.h>
 #include <linux/slab.h>
 #include <linux/string.h>
@@ -229,6 +230,48 @@ void __vma_link_list(struct mm_struct *mm, struct vm_area_struct *vma,
 		next->vm_prev = vma;
 }
 
+/* latr */
+#ifdef CONFIG_LAZY_MEM_FREE
+
+void __vma_add_lazy_list(struct mm_struct *mm,
+			struct vm_area_struct *vma)
+{
+	vma->lazy_vmnext = mm->lazy_mmap;
+	if (vma->lazy_vmnext)
+		vma->lazy_vmnext->lazy_vmprev = vma;
+
+	mm->lazy_mmap = vma;
+
+	/* add the jiffy when the VMA is added */
+	vma->lazy_jiffy = get_jiffies_64();
+}
+
+void __vma_del_lazy_list(struct mm_struct *mm,
+			struct vm_area_struct *vma)
+{
+	struct vm_area_struct *node = mm->lazy_mmap;
+
+	if ((node) && (node == vma)) {
+		mm->lazy_mmap = vma->lazy_vmnext;
+		if (vma->lazy_vmnext)
+			vma->lazy_vmnext->lazy_vmprev = NULL;
+		return;
+	}
+
+	while (node) {
+		if ((node->lazy_vmnext) && (node->lazy_vmnext == vma)) {
+				node->lazy_vmnext = vma->lazy_vmnext;
+				if (node->lazy_vmnext)
+					node->lazy_vmnext->lazy_vmprev = node;
+				return;
+		}
+		node=node->lazy_vmnext;
+	}
+}
+
+#endif
+/********/
+
 /* Check if the vma is being used as a stack by this task */
 int vma_is_stack_for_current(struct vm_area_struct *vma)
 {
diff --git a/tools/perf/pmu-events/jevents b/tools/perf/pmu-events/jevents
new file mode 100755
index 0000000..1404810
Binary files /dev/null and b/tools/perf/pmu-events/jevents differ
diff --git a/tools/perf/pmu-events/pmu-events.c b/tools/perf/pmu-events/pmu-events.c
new file mode 100644
index 0000000..d4a83a0
--- /dev/null
+++ b/tools/perf/pmu-events/pmu-events.c
@@ -0,0 +1,50945 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+#include "../../pmu-events/pmu-events.h"
+struct pmu_event pme_broadwell[] = {
+{
+	.name = "other_assists.avx_to_sse",
+	.event = "event=0xC1,umask=0x8,period=100003",
+	.desc = "Number of transitions from AVX-256 to legacy SSE when penalty applicable  Spec update: BDM30",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of transitions from AVX-256 to legacy SSE when penalty is applicable  Spec update: BDM30",
+},
+{
+	.name = "other_assists.sse_to_avx",
+	.event = "event=0xC1,umask=0x10,period=100003",
+	.desc = "Number of transitions from SSE to AVX-256 when penalty applicable  Spec update: BDM30",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of transitions from legacy SSE to AVX-256 when penalty is applicable  Spec update: BDM30",
+},
+{
+	.name = "fp_arith_inst_retired.scalar_double",
+	.event = "event=0xC7,umask=0x1,period=2000003",
+	.desc = "Number of SSE/AVX computational scalar double precision floating-point instructions retired.  Each count represents 1 computation. Applies to SSE* and AVX* scalar double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT FM(N)ADD/SUB.  FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.scalar_single",
+	.event = "event=0xC7,umask=0x2,period=2000003",
+	.desc = "Number of SSE/AVX computational scalar single precision floating-point instructions retired.  Each count represents 1 computation. Applies to SSE* and AVX* scalar single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT FM(N)ADD/SUB.  FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.128b_packed_double",
+	.event = "event=0xC7,umask=0x4,period=2000003",
+	.desc = "Number of SSE/AVX computational 128-bit packed double precision floating-point instructions retired.  Each count represents 2 computations. Applies to SSE* and AVX* packed double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.128b_packed_single",
+	.event = "event=0xC7,umask=0x8,period=2000003",
+	.desc = "Number of SSE/AVX computational 128-bit packed single precision floating-point instructions retired.  Each count represents 4 computations. Applies to SSE* and AVX* packed single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.256b_packed_double",
+	.event = "event=0xC7,umask=0x10,period=2000003",
+	.desc = "Number of SSE/AVX computational 256-bit packed double precision floating-point instructions retired.  Each count represents 4 computations. Applies to SSE* and AVX* packed double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.x87_output",
+	.event = "event=0xCA,umask=0x2,period=100003",
+	.desc = "Number of X87 assists due to output value",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of x87 floating point (FP) micro-code assist (numeric overflow/underflow, inexact result) when the output value (destination register) is invalid",
+},
+{
+	.name = "fp_assist.x87_input",
+	.event = "event=0xCA,umask=0x4,period=100003",
+	.desc = "Number of X87 assists due to input value",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts x87 floating point (FP) micro-code assist (invalid operation, denormal operand, SNaN operand) when the input value (one of the source operands to an FP instruction) is invalid",
+},
+{
+	.name = "fp_assist.simd_output",
+	.event = "event=0xCA,umask=0x8,period=100003",
+	.desc = "Number of SIMD FP assists due to Output values",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of SSE* floating point (FP) micro-code assist (numeric overflow/underflow) when the output value (destination register) is invalid. Counting covers only cases involving penalties that require micro-code assist intervention",
+},
+{
+	.name = "fp_assist.simd_input",
+	.event = "event=0xCA,umask=0x10,period=100003",
+	.desc = "Number of SIMD FP assists due to input values",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts any input SSE* FP assist - invalid operation, denormal operand, dividing by zero, SNaN operand. Counting includes only cases involving penalties that required micro-code assist intervention",
+},
+{
+	.name = "fp_assist.any",
+	.event = "event=0xCA,umask=0x1e,period=100003,cmask=1",
+	.desc = "Cycles with any input/output SSE or FP assist",
+	.topic = "floating point",
+	.long_desc = "This event counts cycles with any input and output SSE or x87 FP assist. If an input and output assist are detected on the same cycle the event increments by 1",
+},
+{
+	.name = "fp_arith_inst_retired.256b_packed_single",
+	.event = "event=0xc7,umask=0x20,period=2000003",
+	.desc = "Number of SSE/AVX computational 256-bit packed single precision floating-point instructions retired.  Each count represents 8 computations. Applies to SSE* and AVX* packed single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.scalar",
+	.event = "event=0xC7,umask=0x3,period=2000003",
+	.desc = "Number of SSE/AVX computational scalar floating-point instructions retired. Applies to SSE* and AVX* scalar, double and single precision floating-point: ADD SUB MUL DIV MIN MAX RSQRT RCP SQRT FM(N)ADD/SUB. FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.packed",
+	.event = "event=0xC7,umask=0x3c,period=2000004",
+	.desc = "Number of SSE/AVX computational packed floating-point instructions retired. Applies to SSE* and AVX*, packed, double and single precision floating-point: ADD SUB MUL DIV MIN MAX RSQRT RCP SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.single",
+	.event = "event=0xC7,umask=0x2a,period=2000005",
+	.desc = "Number of SSE/AVX computational single precision floating-point instructions retired. Applies to SSE* and AVX*scalar, double and single precision floating-point: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element. ?",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.double",
+	.event = "event=0xC7,umask=0x15,period=2000006",
+	.desc = "Number of SSE/AVX computational double precision floating-point instructions retired. Applies to SSE* and AVX*scalar, double and single precision floating-point: ADD SUB MUL DIV MIN MAX SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element.  ?",
+	.topic = "floating point",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired from execution",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions retired from execution. For instructions that consist of multiple micro-ops, this event counts the retirement of the last micro-op of the instruction. Counting continues during hardware interrupts, traps, and inside interrupt handlers. \nNotes: INST_RETIRED.ANY is counted by a designated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events. INST_RETIRED.ANY_P is counted by a programmable counter and it is an architectural performance event. \nCounting: Faulting executions of GETSEC/VM entry/VM Exit/MWait will not count as retired instructions",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Core cycles when the thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of core cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. This event is a component in many key event ratios. The core frequency may change from time to time due to transitions associated with Enhanced Intel SpeedStep Technology or TM2. For this reason this event may have a changing ratio with regards to time. When the core frequency is constant, this event can approximate elapsed time while the core was not in the halt state. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Reference cycles when the core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of reference cycles when the core is not in a halt state. The core enters the halt state when it is running the HLT instruction or the MWAIT instruction. This event is not affected by core frequency changes (for example, P states, TM2 transitions) but has the same incrementing frequency as the time stamp counter. This event can approximate elapsed time while the core was not in a halt state. This event has a constant ratio with the CPU_CLK_UNHALTED.REF_XCLK event. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events. \nNote: On all current platforms this event stops counting during 'throttling (TM)' states duty off periods the processor is 'halted'.  This event is clocked by base clock (100 Mhz) on Sandy Bridge. The counter update is done at a lower clock rate then the core clock the overflow status bit for this counter may appear 'sticky'.  After the counter has overflowed and software clears the overflow status bit and resets the counter to less than MAX. The reset value to the counter is not clocked immediately so the overflow status bit will flip 'high (1)' and generate another PMI (if enabled) after which the reset value gets clocked into the counter. Therefore, software will get the interrupt, read the overflow status bit '1 for bit 34 while the counter value is less than MAX. Software should ignore this case",
+},
+{
+	.name = "ld_blocks.store_forward",
+	.event = "event=0x03,umask=0x2,period=100003",
+	.desc = "Cases when loads get true Block-on-Store blocking code preventing store forwarding",
+	.topic = "pipeline",
+	.long_desc = "This event counts how many times the load operation got the true Block-on-Store blocking code preventing store forwarding. This includes cases when:\n - preceding store conflicts with the load (incomplete overlap);\n - store forwarding is impossible due to u-arch limitations;\n - preceding lock RMW operations are not forwarded;\n - store has the no-forward bit set (uncacheable/page-split/masked stores);\n - all-blocking stores are used (mostly, fences and port I/O);\nand others.\nThe most common case is a load blocked due to its address range overlapping with a preceding smaller uncompleted store. Note: This event does not take into account cases of out-of-SW-control (for example, SbTailHit), unknown physical STA, and cases of blocking loads on store due to being non-WB memory type or a lock. These cases are covered by other events.\nSee the table of not supported store forwards in the Optimization Guide",
+},
+{
+	.name = "ld_blocks.no_sr",
+	.event = "event=0x03,umask=0x8,period=100003",
+	.desc = "This event counts the number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks_partial.address_alias",
+	.event = "event=0x07,umask=0x1,period=100003",
+	.desc = "False dependencies in MOB due to partial compare",
+	.topic = "pipeline",
+	.long_desc = "This event counts false dependencies in MOB when the partial comparison upon loose net check and dependency was resolved by the Enhanced Loose net mechanism. This may not result in high performance penalties. Loose net checks can fail when loads and stores are 4k aliased",
+},
+{
+	.name = "int_misc.rat_stall_cycles",
+	.event = "event=0x0D,umask=0x8,period=2000003",
+	.desc = "Cycles when Resource Allocation Table (RAT) external stall is sent to Instruction Decode Queue (IDQ) for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of cycles during which Resource Allocation Table (RAT) external stall is sent to Instruction Decode Queue (IDQ) for the current thread. This also includes the cycles during which the Allocator is serving another thread",
+},
+{
+	.name = "int_misc.recovery_cycles",
+	.event = "event=0x0D,umask=0x3,period=2000003,cmask=1",
+	.desc = "Number of cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc...)",
+	.topic = "pipeline",
+	.long_desc = "Cycles checkpoints in Resource Allocation Table (RAT) are recovering from JEClear or machine clear",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0x0E,umask=0x1,period=2000003",
+	.desc = "Uops that Resource Allocation Table (RAT) issues to Reservation Station (RS)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of Uops issued by the Resource Allocation Table (RAT) to the reservation station (RS)",
+},
+{
+	.name = "uops_issued.flags_merge",
+	.event = "event=0x0E,umask=0x10,period=2000003",
+	.desc = "Number of flags-merge uops being allocated. Such uops considered perf sensitive; added by GSR u-arch",
+	.topic = "pipeline",
+	.long_desc = "Number of flags-merge uops being allocated. Such uops considered perf sensitive\n added by GSR u-arch",
+},
+{
+	.name = "uops_issued.slow_lea",
+	.event = "event=0x0E,umask=0x20,period=2000003",
+	.desc = "Number of slow LEA uops being allocated. A uop is generally considered SlowLea if it has 3 sources (e.g. 2 sources + immediate) regardless if as a result of LEA instruction or not",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.single_mul",
+	.event = "event=0x0E,umask=0x40,period=2000003",
+	.desc = "Number of Multiply packed/scalar single precision uops allocated",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which the Resource Allocation Table (RAT) does not issue any Uops to the reservation station (RS) for the current thread",
+},
+{
+	.name = "arith.fpu_div_active",
+	.event = "event=0x14,umask=0x1,period=2000003",
+	.desc = "Cycles when divider is busy executing divide operations",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of the divide operations executed. Uses edge-detect and a cmask value of 1 on ARITH.FPU_DIV_ACTIVE to get the number of the divide operations executed",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "This is a fixed-frequency event programmed to general counters. It counts when the core is unhalted at 100 Mhz",
+},
+{
+	.name = "cpu_clk_thread_unhalted.one_thread_active",
+	.event = "event=0x3c,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.sw_pf",
+	.event = "event=0x4c,umask=0x1,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for software prefetch",
+	.topic = "pipeline",
+	.long_desc = "This event counts all not software-prefetch load dispatches that hit the fill buffer (FB) allocated for the software prefetch. It can also be incremented by some lock instructions. So it should only be used with profiling so that the locks can be excluded by asm inspection of the nearby instructions",
+},
+{
+	.name = "load_hit_pre.hw_pf",
+	.event = "event=0x4C,umask=0x2,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for hardware prefetch",
+	.topic = "pipeline",
+	.long_desc = "This event counts all not software-prefetch load dispatches that hit the fill buffer (FB) allocated for the hardware prefetch",
+},
+{
+	.name = "move_elimination.int_eliminated",
+	.event = "event=0x58,umask=0x1,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.simd_eliminated",
+	.event = "event=0x58,umask=0x2,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.int_not_eliminated",
+	.event = "event=0x58,umask=0x4,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.simd_not_eliminated",
+	.event = "event=0x58,umask=0x8,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_cycles",
+	.event = "event=0x5E,umask=0x1,period=2000003",
+	.desc = "Cycles when Reservation Station (RS) is empty for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which the reservation station (RS) is empty for the thread.\nNote: In ST-mode, not active thread should drive 0. This is usually caused by severely costly branch mispredictions, or allocator/FE issues",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000003",
+	.desc = "Stalls caused by changing prefix length of the instruction",
+	.topic = "pipeline",
+	.long_desc = "This event counts stalls occured due to changing prefix length (66, 67 or REX.W when they change the length of the decoded instruction). Occurrences counting is proportional to the number of prefixes in a 16B-line. This may result in the following penalties: three-cycle penalty for each LCP in a 16-byte chunk",
+},
+{
+	.name = "br_inst_exec.nontaken_conditional",
+	.event = "event=0x88,umask=0x41,period=200003",
+	.desc = "Not taken macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts not taken macro-conditional branch instructions",
+},
+{
+	.name = "br_inst_exec.taken_conditional",
+	.event = "event=0x88,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired macro-conditional branch instructions",
+},
+{
+	.name = "br_inst_exec.taken_direct_jump",
+	.event = "event=0x88,umask=0x82,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branch instructions excluding calls and indirects",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired macro-conditional branch instructions excluding calls and indirect branches",
+},
+{
+	.name = "br_inst_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired indirect branches excluding calls and return branches",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_return",
+	.event = "event=0x88,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired indirect branches with return mnemonic",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired indirect branches that have a return mnemonic",
+},
+{
+	.name = "br_inst_exec.taken_direct_near_call",
+	.event = "event=0x88,umask=0x90,period=200003",
+	.desc = "Taken speculative and retired direct near calls",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired direct near calls",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_call",
+	.event = "event=0x88,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired indirect calls",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired indirect calls including both register and memory indirect",
+},
+{
+	.name = "br_inst_exec.all_conditional",
+	.event = "event=0x88,umask=0xc1,period=200003",
+	.desc = "Speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired macro-conditional branch instructions",
+},
+{
+	.name = "br_inst_exec.all_direct_jmp",
+	.event = "event=0x88,umask=0xc2,period=200003",
+	.desc = "Speculative and retired macro-unconditional branches excluding calls and indirects",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired macro-unconditional branch instructions, excluding calls and indirects",
+},
+{
+	.name = "br_inst_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0xc4,period=200003",
+	.desc = "Speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired indirect branches excluding calls and return branches",
+},
+{
+	.name = "br_inst_exec.all_indirect_near_return",
+	.event = "event=0x88,umask=0xc8,period=200003",
+	.desc = "Speculative and retired indirect return branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired indirect branches that have a return mnemonic",
+},
+{
+	.name = "br_inst_exec.all_direct_near_call",
+	.event = "event=0x88,umask=0xd0,period=200003",
+	.desc = "Speculative and retired direct near calls",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired direct near calls",
+},
+{
+	.name = "br_inst_exec.all_branches",
+	.event = "event=0x88,umask=0xff,period=200003",
+	.desc = "Speculative and retired  branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired branch instructions",
+},
+{
+	.name = "br_misp_exec.nontaken_conditional",
+	.event = "event=0x89,umask=0x41,period=200003",
+	.desc = "Not taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts not taken speculative and retired mispredicted macro conditional branch instructions",
+},
+{
+	.name = "br_misp_exec.taken_conditional",
+	.event = "event=0x89,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired mispredicted macro conditional branch instructions",
+},
+{
+	.name = "br_misp_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired mispredicted indirect branches excluding calls and returns",
+},
+{
+	.name = "br_misp_exec.taken_return_near",
+	.event = "event=0x89,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches with return mnemonic",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired mispredicted indirect branches that have a return mnemonic",
+},
+{
+	.name = "br_misp_exec.all_conditional",
+	.event = "event=0x89,umask=0xc1,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired mispredicted macro conditional branch instructions",
+},
+{
+	.name = "br_misp_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0xc4,period=200003",
+	.desc = "Mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken mispredicted indirect branches excluding calls and returns",
+},
+{
+	.name = "br_misp_exec.all_branches",
+	.event = "event=0x89,umask=0xff,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired mispredicted branch instructions",
+},
+{
+	.name = "uops_dispatched_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 0",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 0",
+},
+{
+	.name = "uops_dispatched_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 1",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 1",
+},
+{
+	.name = "uops_dispatched_port.port_2",
+	.event = "event=0xA1,umask=0x4,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 2",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 2",
+},
+{
+	.name = "uops_dispatched_port.port_3",
+	.event = "event=0xA1,umask=0x8,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 3",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 3",
+},
+{
+	.name = "uops_dispatched_port.port_4",
+	.event = "event=0xA1,umask=0x10,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 4",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 4",
+},
+{
+	.name = "uops_dispatched_port.port_5",
+	.event = "event=0xA1,umask=0x20,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 5",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 5",
+},
+{
+	.name = "uops_dispatched_port.port_6",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 6",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 6",
+},
+{
+	.name = "uops_dispatched_port.port_7",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 7",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 7",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000003",
+	.desc = "Resource-related stall cycles",
+	.topic = "pipeline",
+	.long_desc = "This event counts resource-related stall cycles. Reasons for stalls can be as follows:\n - *any* u-arch structure got full (LB, SB, RS, ROB, BOB, LM, Physical Register Reclaim Table (PRRT), or Physical History Table (PHT) slots)\n - *any* u-arch structure got empty (like INT/SIMD FreeLists)\n - FPU control word (FPCW), MXCSR\nand others. This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "resource_stalls.rs",
+	.event = "event=0xA2,umask=0x4,period=2000003",
+	.desc = "Cycles stalled due to no eligible RS entry available",
+	.topic = "pipeline",
+	.long_desc = "This event counts stall cycles caused by absence of eligible entries in the reservation station (RS). This may result from RS overflow, or from RS deallocation because of the RS array Write Port allocation scheme (each RS entry has two write ports instead of four. As a result, empty entries could not be used, although RS is not really full). This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "resource_stalls.sb",
+	.event = "event=0xA2,umask=0x8,period=2000003",
+	.desc = "Cycles stalled due to no store buffers available. (not including draining form sync)",
+	.topic = "pipeline",
+	.long_desc = "This event counts stall cycles caused by the store buffer (SB) overflow (excluding draining from synch). This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "resource_stalls.rob",
+	.event = "event=0xA2,umask=0x10,period=2000003",
+	.desc = "Cycles stalled due to re-order buffer full",
+	.topic = "pipeline",
+	.long_desc = "This event counts ROB full stall cycles. This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "cycle_activity.cycles_l2_pending",
+	.event = "event=0xA3,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles the CPU has at least one pending  demand* load request missing the L2 cache",
+},
+{
+	.name = "cycle_activity.cycles_l1d_pending",
+	.event = "event=0xA3,umask=0x8,period=2000003,cmask=8",
+	.desc = "Cycles while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles the CPU has at least one pending  demand load request missing the L1 data cache",
+},
+{
+	.name = "cycle_activity.cycles_ldm_pending",
+	.event = "event=0xA3,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles the CPU has at least one pending  demand load request (that is cycles with non-completed load waiting for its data from memory subsystem)",
+},
+{
+	.name = "cycle_activity.cycles_no_execute",
+	.event = "event=0xA3,umask=0x4,period=2000003,cmask=4",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port",
+},
+{
+	.name = "cycle_activity.stalls_l2_pending",
+	.event = "event=0xA3,umask=0x5,period=2000003,cmask=5",
+	.desc = "Execution stalls while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port, while there was at least one pending demand* load request missing the L2 cache.(as a footprint) * includes also L1 HW prefetch requests that may or may not be required by demands",
+},
+{
+	.name = "cycle_activity.stalls_ldm_pending",
+	.event = "event=0xA3,umask=0x6,period=2000003,cmask=6",
+	.desc = "Execution stalls while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port, while there was at least one pending demand load request",
+},
+{
+	.name = "cycle_activity.stalls_l1d_pending",
+	.event = "event=0xA3,umask=0xc,period=2000003,cmask=12",
+	.desc = "Execution stalls while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port, while there was at least one pending demand load request missing the L1 data cache",
+},
+{
+	.name = "lsd.uops",
+	.event = "event=0xA8,umask=0x1,period=2000003",
+	.desc = "Number of Uops delivered by the LSD",
+	.topic = "pipeline",
+	.long_desc = "Number of Uops delivered by the LSD",
+},
+{
+	.name = "uops_executed.thread",
+	.event = "event=0xB1,umask=0x1,period=2000003",
+	.desc = "Counts the number of uops to be executed per-thread each cycle",
+	.topic = "pipeline",
+	.long_desc = "Number of uops to be executed per-thread each cycle",
+},
+{
+	.name = "uops_executed.core",
+	.event = "event=0xB1,umask=0x2,period=2000003",
+	.desc = "Number of uops executed on the core",
+	.topic = "pipeline",
+	.long_desc = "Number of uops executed from any thread",
+},
+{
+	.name = "uops_executed.stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts number of cycles no uops were dispatched to be executed on this thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which no uops were dispatched from the Reservation Station (RS) per thread",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Number of instructions retired. General Counter   - architectural event  Spec update: BDM61",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions (EOMs) retired. Counting covers macro-fused instructions individually (that is, increments by two)  Spec update: BDM61",
+},
+{
+	.name = "inst_retired.x87",
+	.event = "event=0xC0,umask=0x2,period=2000003",
+	.desc = "FP operations  retired. X87 FP operations that have no exceptions:",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts FP operations retired. For X87 FP operations that have no exceptions counting also includes flows that have several X87, or flows that use X87 uops in the exception handling",
+},
+{
+	.name = "inst_retired.prec_dist",
+	.event = "event=0xC0,umask=0x1,period=2000003",
+	.desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution  Spec update: BDM11, BDM55 (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version (that is, uses PEBS) of the event that counts instructions retired  Spec update: BDM11, BDM55 (Must be precise)",
+},
+{
+	.name = "other_assists.any_wb_assist",
+	.event = "event=0xC1,umask=0x40,period=100003",
+	.desc = "Number of times any microcode assist is invoked by HW upon uop writeback",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.all",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "Actually retired uops  Supports address when precise (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This event counts all actually retired uops. Counting increments by two for micro-fused uops, and by one for macro-fused and other uops. Maximal increment value for one cycle is eight  Supports address when precise (Precise event)",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000003",
+	.desc = "Retirement slots used (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of retirement slots used (Precise event)",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts cycles without actually retired uops",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=10",
+	.desc = "Cycles with less than 10 actually retired uops",
+	.topic = "pipeline",
+	.long_desc = "Number of cycles using always true condition (uops_ret < 16) applied to non PEBS uops retired event",
+},
+{
+	.name = "machine_clears.cycles",
+	.event = "event=0xC3,umask=0x1,period=2000003",
+	.desc = "Cycles there was a Nuke. Account for both thread-specific and All Thread Nukes",
+	.topic = "pipeline",
+	.long_desc = "This event counts both thread-specific (TS) and all-thread (AT) nukes",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=100003",
+	.desc = "Self-modifying code (SMC) detected",
+	.topic = "pipeline",
+	.long_desc = "This event counts self-modifying code (SMC) detected, which causes a machine clear",
+},
+{
+	.name = "machine_clears.maskmov",
+	.event = "event=0xC3,umask=0x20,period=100003",
+	.desc = "This event counts the number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0",
+	.topic = "pipeline",
+	.long_desc = "Maskmov false fault - counts number of time ucode passes through Maskmov flow due to instruction's mask being 0 while the flow was completed without raising a fault",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=400009",
+	.desc = "Conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=100007",
+	.desc = "Direct and indirect near call instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts both direct and indirect near call instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=400009",
+	.desc = "All (macro) branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts all (macro) branch instructions retired",
+},
+{
+	.name = "br_inst_retired.near_return",
+	.event = "event=0xC4,umask=0x8,period=100007",
+	.desc = "Return instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts return instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.not_taken",
+	.event = "event=0xC4,umask=0x10,period=400009",
+	.desc = "Not taken branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts not taken branch instructions retired",
+},
+{
+	.name = "br_inst_retired.near_taken",
+	.event = "event=0xC4,umask=0x20,period=400009",
+	.desc = "Taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts taken branch instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0x40,period=100007",
+	.desc = "Far branch instructions retired  Spec update: BDW98",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts far branch instructions retired  Spec update: BDW98",
+},
+{
+	.name = "br_inst_retired.all_branches_pebs",
+	.event = "event=0xC4,umask=0x4,period=400009",
+	.desc = "All (macro) branch instructions retired. (Precise Event - PEBS)  Spec update: BDW98 (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version of BR_INST_RETIRED.ALL_BRANCHES that counts all (macro) branch instructions retired  Spec update: BDW98 (Must be precise)",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=400009",
+	.desc = "Mispredicted conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts mispredicted conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=400009",
+	.desc = "All mispredicted macro branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts all mispredicted macro branch instructions retired",
+},
+{
+	.name = "br_misp_retired.ret",
+	.event = "event=0xC5,umask=0x8,period=100007",
+	.desc = "This event counts the number of mispredicted ret instructions retired. Non PEBS (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts mispredicted return instructions retired (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches_pebs",
+	.event = "event=0xC5,umask=0x4,period=400009",
+	.desc = "Mispredicted macro branch instructions retired. (Precise Event - PEBS) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version of BR_MISP_RETIRED.ALL_BRANCHES that counts all mispredicted macro branch instructions retired (Must be precise)",
+},
+{
+	.name = "rob_misc_events.lbr_inserts",
+	.event = "event=0xCC,umask=0x20,period=2000003",
+	.desc = "Count cases of saving new LBR",
+	.topic = "pipeline",
+	.long_desc = "This event counts cases of saving new LBR records by hardware. This assumes proper enabling of LBRs and takes into account LBR filtering done by the LBR_SELECT register",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Thread cycles when thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This is an architectural event that counts the number of thread cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. The core frequency may change from time to time due to power or thermal throttling. For this reason, this event may have a changing ratio with regards to wall clock time",
+},
+{
+	.name = "br_misp_exec.taken_indirect_near_call",
+	.event = "event=0x89,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_0_core",
+	.event = "event=0xA1,umask=0x1,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_1_core",
+	.event = "event=0xA1,umask=0x2,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_2_core",
+	.event = "event=0xA1,umask=0x4,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 2",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_3_core",
+	.event = "event=0xA1,umask=0x8,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 3",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_4_core",
+	.event = "event=0xA1,umask=0x10,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_5_core",
+	.event = "event=0xA1,umask=0x20,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_6_core",
+	.event = "event=0xA1,umask=0x40,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 6",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_7_core",
+	.event = "event=0xA1,umask=0x80,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 7",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_taken",
+	.event = "event=0xC5,umask=0x20,period=400009",
+	.desc = "number of near branch instructions retired that were mispredicted and taken (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Number of near branch instructions retired that were mispredicted and taken (Precise event)",
+},
+{
+	.name = "uops_executed.cycles_ge_1_uop_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles where at least 1 uop was executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.cycles_ge_2_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles where at least 2 uops were executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.cycles_ge_3_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles where at least 3 uops were executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.cycles_ge_4_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles where at least 4 uops were executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "baclears.any",
+	.event = "event=0xe6,umask=0x1f,period=100003",
+	.desc = "Counts the total number when the front end is resteered, mainly when the BPU cannot provide a correct prediction and this is corrected by other branch handling mechanisms at the front end",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l1d_miss",
+	.event = "event=0xA3,umask=0x8,period=2000003,cmask=8",
+	.desc = "Cycles while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l2_miss",
+	.event = "event=0xA3,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_mem_any",
+	.event = "event=0xA3,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_total",
+	.event = "event=0xA3,umask=0x4,period=2000003,cmask=4",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l1d_miss",
+	.event = "event=0xA3,umask=0xc,period=2000003,cmask=12",
+	.desc = "Execution stalls while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l2_miss",
+	.event = "event=0xA3,umask=0x5,period=2000003,cmask=5",
+	.desc = "Execution stalls while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_mem_any",
+	.event = "event=0xA3,umask=0x6,period=2000003,cmask=6",
+	.desc = "Execution stalls while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.count",
+	.event = "event=0xC3,umask=0x1,edge=1,period=100003,cmask=1",
+	.desc = "Number of machine clears (nukes) of any type",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_4_uops",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_end",
+	.event = "event=0x5E,inv=1,umask=0x1,edge=1,period=200003,cmask=1",
+	.desc = "Counts end of periods where the Reservation Station (RS) was empty. Could be useful to precisely locate Frontend Latency Bound issues",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_active",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 0",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 0",
+},
+{
+	.name = "uops_executed_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 1",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 1",
+},
+{
+	.name = "uops_executed_port.port_2",
+	.event = "event=0xA1,umask=0x4,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 2",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 2",
+},
+{
+	.name = "uops_executed_port.port_3",
+	.event = "event=0xA1,umask=0x8,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 3",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 3",
+},
+{
+	.name = "uops_executed_port.port_4",
+	.event = "event=0xA1,umask=0x10,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 4",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 4",
+},
+{
+	.name = "uops_executed_port.port_5",
+	.event = "event=0xA1,umask=0x20,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 5",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 5",
+},
+{
+	.name = "uops_executed_port.port_6",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 6",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 6",
+},
+{
+	.name = "uops_executed_port.port_7",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 7",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 7",
+},
+{
+	.name = "uop_dispatches_cancelled.simd_prf",
+	.event = "event=0xA0,umask=0x3,period=2000003",
+	.desc = "Micro-op dispatches cancelled due to insufficient SIMD physical register file read ports",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of micro-operations cancelled after they were dispatched from the scheduler to the execution units when the total number of physical register read ports across all dispatch ports exceeds the read bandwidth of the physical register file.  The SIMD_PRF subevent applies to the following instructions: VDPPS, DPPS, VPCMPESTRI, PCMPESTRI, VPCMPESTRM, PCMPESTRM, VFMADD*, VFMADDSUB*, VFMSUB*, VMSUBADD*, VFNMADD*, VFNMSUB*.  See the Broadwell Optimization Guide for more information",
+},
+{
+	.name = "cpu_clk_unhalted.thread_any",
+	.event = "event=0x3c,any=1",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p_any",
+	.event = "event=0x3C,umask=0x0,any=1,period=2000003",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_cycles_any",
+	.event = "event=0x0D,umask=0x3,any=1,period=2000003,cmask=1",
+	.desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_1",
+	.event = "event=0xb1,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles at least 1 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_2",
+	.event = "event=0xb1,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles at least 2 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_3",
+	.event = "event=0xb1,umask=0x2,period=2000003,cmask=3",
+	.desc = "Cycles at least 3 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_4",
+	.event = "event=0xb1,umask=0x2,period=2000003,cmask=4",
+	.desc = "Cycles at least 4 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_none",
+	.event = "event=0xb1,inv=1,umask=0x2,period=2000003",
+	.desc = "Cycles with no micro-ops executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "idq.empty",
+	.event = "event=0x79,umask=0x2,period=2000003",
+	.desc = "Instruction Decode Queue (IDQ) empty cycles",
+	.topic = "frontend",
+	.long_desc = "This counts the number of cycles that the instruction decoder queue is empty and can indicate that the application may be bound in the front end.  It does not determine whether there are uops being delivered to the Alloc stage since uops can be delivered by bypass skipping the Instruction Decode Queue (IDQ) when it is empty",
+},
+{
+	.name = "idq.mite_uops",
+	.event = "event=0x79,umask=0x4,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq.dsb_uops",
+	.event = "event=0x79,umask=0x8,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_dsb_uops",
+	.event = "event=0x79,umask=0x10,period=2000003",
+	.desc = "Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while the Microcode Sequencer (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_mite_uops",
+	.event = "event=0x79,umask=0x20,period=2000003",
+	.desc = "Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_uops",
+	.event = "event=0x79,umask=0x30,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the total number of uops delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may \"bypass\" the IDQ. Uops maybe initiated by Decode Stream Buffer (DSB) or MITE",
+},
+{
+	.name = "idq.ms_cycles",
+	.event = "event=0x79,umask=0x30,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may \"bypass\" the IDQ. Uops maybe initiated by Decode Stream Buffer (DSB) or MITE",
+},
+{
+	.name = "idq.mite_cycles",
+	.event = "event=0x79,umask=0x4,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.dsb_cycles",
+	.event = "event=0x79,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_dsb_cycles",
+	.event = "event=0x79,umask=0x10,period=2000003,cmask=1",
+	.desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while the Microcode Sequencer (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_dsb_occur",
+	.event = "event=0x79,umask=0x10,edge=1,period=2000003,cmask=1",
+	.desc = "Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while the Microcode Sequencer (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.all_dsb_cycles_4_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=4",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles 4  uops were  delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.all_dsb_cycles_any_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=1",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles  uops were  delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.all_mite_cycles_4_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=4",
+	.desc = "Cycles MITE is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles 4  uops were  delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq.all_mite_cycles_any_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=1",
+	.desc = "Cycles MITE is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles  uops were delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq.mite_all_uops",
+	.event = "event=0x79,umask=0x3c,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=2000003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Reads. both cacheable and noncacheable, including UC fetches",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of both cacheable and noncacheable Instruction Cache, Streaming Buffer and Victim Cache Reads including UC fetches",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Misses. Includes Uncacheable accesses",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of instruction cache, streaming buffer and victim cache misses. Counting includes UC accesses",
+},
+{
+	.name = "icache.ifdata_stall",
+	.event = "event=0x80,umask=0x4,period=2000003",
+	.desc = "Cycles where a code fetch is stalled due to L1 instruction-cache miss",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which the demand fetch waits for data (wfdM104H) from L2 or iSB (opportunistic hit)",
+},
+{
+	.name = "idq_uops_not_delivered.core",
+	.event = "event=0x9C,umask=0x1,period=2000003",
+	.desc = "Uops not delivered to Resource Allocation Table (RAT) per thread when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops not delivered to Resource Allocation Table (RAT) per thread adding ?4 ? x? when Resource Allocation Table (RAT) is not stalled and Instruction Decode Queue (IDQ) delivers x uops to Resource Allocation Table (RAT) (where x belongs to {0,1,2,3}). Counting does not cover cases when:\n a. IDQ-Resource Allocation Table (RAT) pipe serves the other thread;\n b. Resource Allocation Table (RAT) is stalled for the thread (including uop drops and clear BE conditions); \n c. Instruction Decode Queue (IDQ) delivers four uops",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_0_uops_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles per thread when 4 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts, on the per-thread basis, cycles when no uops are delivered to Resource Allocation Table (RAT). IDQ_Uops_Not_Delivered.core =4",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_1_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles per thread when 3 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts, on the per-thread basis, cycles when less than 1 uop is  delivered to Resource Allocation Table (RAT). IDQ_Uops_Not_Delivered.core >=3",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_2_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles with less than 2 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_3_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with less than 3 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_fe_was_ok",
+	.event = "event=0x9C,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts cycles FE delivered 4 uops or Resource Allocation Table (RAT) was stalling FE",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.penalty_cycles",
+	.event = "event=0xAB,umask=0x2,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles",
+	.topic = "frontend",
+	.long_desc = "This event counts Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles. These cycles do not include uops routed through because of the switch itself, for example, when Instruction Decode Queue (IDQ) pre-allocation is unavailable, or Instruction Decode Queue (IDQ) is full. SBD-to-MITE switch true penalty cycles happen after the merge mux (MM) receives Decode Stream Buffer (DSB) Sync-indication until receiving the first MITE uop. \nMM is placed before Instruction Decode Queue (IDQ) to merge uops being fed from the MITE and Decode Stream Buffer (DSB) paths. Decode Stream Buffer (DSB) inserts the Sync-indication whenever a Decode Stream Buffer (DSB)-to-MITE switch occurs.\nPenalty: A Decode Stream Buffer (DSB) hit followed by a Decode Stream Buffer (DSB) miss can cost up to six cycles in which no uops are delivered to the IDQ. Most often, such switches from the Decode Stream Buffer (DSB) to the legacy pipeline cost 0?2 cycles",
+},
+{
+	.name = "idq.ms_switches",
+	.event = "event=0x79,umask=0x30,edge=1,period=2000003,cmask=1",
+	.desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+	.topic = "frontend",
+},
+{
+	.name = "cpl_cycles.ring0",
+	.event = "event=0x5C,umask=0x1,period=2000003",
+	.desc = "Unhalted core cycles when the thread is in ring 0",
+	.topic = "other",
+	.long_desc = "This event counts the unhalted core cycles during which the thread is in the ring 0 privileged mode",
+},
+{
+	.name = "cpl_cycles.ring123",
+	.event = "event=0x5C,umask=0x2,period=2000003",
+	.desc = "Unhalted core cycles when thread is in rings 1, 2, or 3",
+	.topic = "other",
+	.long_desc = "This event counts unhalted core cycles during which the thread is in rings 1, 2, or 3",
+},
+{
+	.name = "cpl_cycles.ring0_trans",
+	.event = "event=0x5C,umask=0x1,edge=1,period=100007,cmask=1",
+	.desc = "Number of intervals between processor halts while thread is in ring 0",
+	.topic = "other",
+	.long_desc = "This event counts when there is a transition from ring 1,2 or 3 to ring0",
+},
+{
+	.name = "lock_cycles.split_lock_uc_lock_duration",
+	.event = "event=0x63,umask=0x1,period=2000003",
+	.desc = "Cycles when L1 and L2 are locked due to UC or split lock",
+	.topic = "other",
+	.long_desc = "This event counts cycles in which the L1 and L2 are locked due to a UC lock or split lock. A lock is asserted in case of locked memory access, due to noncacheable memory, locked operation that spans two cache lines, or a page walk from the noncacheable page table. L1D and L2 locks have a very high performance penalty and it is highly recommended to avoid such access",
+},
+{
+	.name = "misalign_mem_ref.loads",
+	.event = "event=0x05,umask=0x1,period=2000003",
+	.desc = "Speculative cache line split load uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "This event counts speculative cache-line split load uops dispatched to the L1 cache",
+},
+{
+	.name = "misalign_mem_ref.stores",
+	.event = "event=0x05,umask=0x2,period=2000003",
+	.desc = "Speculative cache line split STA uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "This event counts speculative cache line split store-address (STA) uops dispatched to the L1 cache",
+},
+{
+	.name = "tx_mem.abort_conflict",
+	.event = "event=0x54,umask=0x1,period=2000003",
+	.desc = "Number of times a TSX line had a cache conflict",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX line had a cache conflict",
+},
+{
+	.name = "tx_mem.abort_capacity_write",
+	.event = "event=0x54,umask=0x2,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to an evicted line caused by a transaction overflow",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to an evicted line caused by a transaction overflow",
+},
+{
+	.name = "tx_mem.abort_hle_store_to_elided_lock",
+	.event = "event=0x54,umask=0x4,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to a non-release/commit store to lock",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to a non-release/commit store to lock",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_not_empty",
+	.event = "event=0x54,umask=0x8,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to commit but Lock Buffer not empty",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to commit but Lock Buffer not empty",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_mismatch",
+	.event = "event=0x54,umask=0x10,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to release/commit but data and address mismatch",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to release/commit but data and address mismatch",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_unsupported_alignment",
+	.event = "event=0x54,umask=0x20,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to attempting an unsupported alignment from Lock Buffer",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to attempting an unsupported alignment from Lock Buffer",
+},
+{
+	.name = "tx_mem.hle_elision_buffer_full",
+	.event = "event=0x54,umask=0x40,period=2000003",
+	.desc = "Number of times we could not allocate Lock Buffer",
+	.topic = "memory",
+	.long_desc = "Number of times we could not allocate Lock Buffer",
+},
+{
+	.name = "tx_exec.misc1",
+	.event = "event=0x5d,umask=0x1,period=2000003",
+	.desc = "Counts the number of times a class of instructions that may cause a transactional abort was executed. Since this is the count of execution, it may not always cause a transactional abort",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by  a flowmarker",
+},
+{
+	.name = "tx_exec.misc2",
+	.event = "event=0x5d,umask=0x2,period=2000003",
+	.desc = "Counts the number of times a class of instructions (e.g., vzeroupper) that may cause a transactional abort was executed inside a transactional region",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by  a vzeroupper instruction",
+},
+{
+	.name = "tx_exec.misc3",
+	.event = "event=0x5d,umask=0x4,period=2000003",
+	.desc = "Counts the number of times an instruction execution caused the transactional nest count supported to be exceeded",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by a nest count that is too deep",
+},
+{
+	.name = "tx_exec.misc4",
+	.event = "event=0x5d,umask=0x8,period=2000003",
+	.desc = "Counts the number of times a XBEGIN instruction was executed inside an HLE transactional region",
+	.topic = "memory",
+	.long_desc = "RTM region detected inside HLE",
+},
+{
+	.name = "tx_exec.misc5",
+	.event = "event=0x5d,umask=0x10,period=2000003",
+	.desc = "Counts the number of times an HLE XACQUIRE instruction was executed inside an RTM transactional region",
+	.topic = "memory",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=100003",
+	.desc = "Counts the number of machine clears due to memory order conflicts",
+	.topic = "memory",
+	.long_desc = "This event counts the number of memory ordering Machine Clears detected. Memory Ordering Machine Clears can result from one of the following:\n1. memory disambiguation,\n2. external snoop, or\n3. cross SMT-HW-thread snoop (stores) hitting load buffer",
+},
+{
+	.name = "hle_retired.start",
+	.event = "event=0xc8,umask=0x1,period=2000003",
+	.desc = "Number of times we entered an HLE region; does not count nested transactions",
+	.topic = "memory",
+	.long_desc = "Number of times we entered an HLE region\n does not count nested transactions",
+},
+{
+	.name = "hle_retired.commit",
+	.event = "event=0xc8,umask=0x2,period=2000003",
+	.desc = "Number of times HLE commit succeeded",
+	.topic = "memory",
+	.long_desc = "Number of times HLE commit succeeded",
+},
+{
+	.name = "hle_retired.aborted",
+	.event = "event=0xc8,umask=0x4,period=2000003",
+	.desc = "Number of times HLE abort was triggered (Precise event)",
+	.topic = "memory",
+	.long_desc = "Number of times HLE abort was triggered (Precise event)",
+},
+{
+	.name = "hle_retired.aborted_misc1",
+	.event = "event=0xc8,umask=0x8,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to various memory events (e.g., read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times an HLE abort was attributed to a Memory condition (See TSX_Memory event for additional details)",
+},
+{
+	.name = "hle_retired.aborted_misc2",
+	.event = "event=0xc8,umask=0x10,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to uncommon conditions",
+	.topic = "memory",
+	.long_desc = "Number of times the TSX watchdog signaled an HLE abort",
+},
+{
+	.name = "hle_retired.aborted_misc3",
+	.event = "event=0xc8,umask=0x20,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to HLE-unfriendly instructions",
+	.topic = "memory",
+	.long_desc = "Number of times a disallowed operation caused an HLE abort",
+},
+{
+	.name = "hle_retired.aborted_misc4",
+	.event = "event=0xc8,umask=0x40,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to incompatible memory type",
+	.topic = "memory",
+	.long_desc = "Number of times HLE caused a fault",
+},
+{
+	.name = "hle_retired.aborted_misc5",
+	.event = "event=0xc8,umask=0x80,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to none of the previous 4 categories (e.g. interrupts)",
+	.topic = "memory",
+	.long_desc = "Number of times HLE aborted and was not due to the abort conditions in subevents 3-6",
+},
+{
+	.name = "rtm_retired.start",
+	.event = "event=0xc9,umask=0x1,period=2000003",
+	.desc = "Number of times we entered an RTM region; does not count nested transactions",
+	.topic = "memory",
+	.long_desc = "Number of times we entered an RTM region\n does not count nested transactions",
+},
+{
+	.name = "rtm_retired.commit",
+	.event = "event=0xc9,umask=0x2,period=2000003",
+	.desc = "Number of times RTM commit succeeded",
+	.topic = "memory",
+	.long_desc = "Number of times RTM commit succeeded",
+},
+{
+	.name = "rtm_retired.aborted",
+	.event = "event=0xc9,umask=0x4,period=2000003",
+	.desc = "Number of times RTM abort was triggered (Precise event)",
+	.topic = "memory",
+	.long_desc = "Number of times RTM abort was triggered  (Precise event)",
+},
+{
+	.name = "rtm_retired.aborted_misc1",
+	.event = "event=0xc9,umask=0x8,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to various memory events (e.g. read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times an RTM abort was attributed to a Memory condition (See TSX_Memory event for additional details)",
+},
+{
+	.name = "rtm_retired.aborted_misc2",
+	.event = "event=0xc9,umask=0x10,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to various memory events (e.g., read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times the TSX watchdog signaled an RTM abort",
+},
+{
+	.name = "rtm_retired.aborted_misc3",
+	.event = "event=0xc9,umask=0x20,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to HLE-unfriendly instructions",
+	.topic = "memory",
+	.long_desc = "Number of times a disallowed operation caused an RTM abort",
+},
+{
+	.name = "rtm_retired.aborted_misc4",
+	.event = "event=0xc9,umask=0x40,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to incompatible memory type",
+	.topic = "memory",
+	.long_desc = "Number of times a RTM caused a fault",
+},
+{
+	.name = "rtm_retired.aborted_misc5",
+	.event = "event=0xc9,umask=0x80,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to none of the previous 4 categories (e.g. interrupt)",
+	.topic = "memory",
+	.long_desc = "Number of times RTM aborted and was not due to the abort conditions in subevents 3-6",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_4",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x4",
+	.desc = "Loads with latency value being above 4  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above four  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_8",
+	.event = "event=0xCD,umask=0x1,period=50021,ldlat=0x8",
+	.desc = "Loads with latency value being above 8  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above eight  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_16",
+	.event = "event=0xCD,umask=0x1,period=20011,ldlat=0x10",
+	.desc = "Loads with latency value being above 16  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 16  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_32",
+	.event = "event=0xCD,umask=0x1,period=100007,ldlat=0x20",
+	.desc = "Loads with latency value being above 32  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 32  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_64",
+	.event = "event=0xCD,umask=0x1,period=2003,ldlat=0x40",
+	.desc = "Loads with latency value being above 64  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 64  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_128",
+	.event = "event=0xCD,umask=0x1,period=1009,ldlat=0x80",
+	.desc = "Loads with latency value being above 128  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 128  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_256",
+	.event = "event=0xCD,umask=0x1,period=503,ldlat=0x100",
+	.desc = "Loads with latency value being above 256  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 256  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_512",
+	.event = "event=0xCD,umask=0x1,period=101,ldlat=0x200",
+	.desc = "Loads with latency value being above 512  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 512  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0001 ",
+	.desc = "Counts demand data reads that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000001 ",
+	.desc = "Counts demand data reads that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000001 ",
+	.desc = "Counts demand data reads that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0002 ",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000002 ",
+	.desc = "DEMAND_RFO & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000002 ",
+	.desc = "Counts all demand data writes (RFOs) that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000002 ",
+	.desc = "DEMAND_RFO & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000002 ",
+	.desc = "Counts all demand data writes (RFOs) that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000002 ",
+	.desc = "DEMAND_RFO & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0004 ",
+	.desc = "Counts all demand code reads that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000004 ",
+	.desc = "Counts all demand code reads that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000004 ",
+	.desc = "Counts all demand code reads that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020008 ",
+	.desc = "COREWB & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0008 ",
+	.desc = "Counts writebacks (modified to exclusive) that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000008 ",
+	.desc = "COREWB & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000008 ",
+	.desc = "COREWB & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000008 ",
+	.desc = "COREWB & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000008 ",
+	.desc = "COREWB & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000008 ",
+	.desc = "COREWB & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000008 ",
+	.desc = "COREWB & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000008 ",
+	.desc = "COREWB & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000008 ",
+	.desc = "Counts writebacks (modified to exclusive) that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000008 ",
+	.desc = "COREWB & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000008 ",
+	.desc = "Counts writebacks (modified to exclusive) that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000008 ",
+	.desc = "COREWB & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020010 ",
+	.desc = "PF_L2_DATA_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0010 ",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000010 ",
+	.desc = "PF_L2_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000010 ",
+	.desc = "PF_L2_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000010 ",
+	.desc = "PF_L2_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000010 ",
+	.desc = "PF_L2_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000010 ",
+	.desc = "PF_L2_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000010 ",
+	.desc = "PF_L2_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000010 ",
+	.desc = "PF_L2_DATA_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000010 ",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000010 ",
+	.desc = "PF_L2_DATA_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000010 ",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000010 ",
+	.desc = "PF_L2_DATA_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020020 ",
+	.desc = "PF_L2_RFO & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0020 ",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000020 ",
+	.desc = "PF_L2_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000020 ",
+	.desc = "PF_L2_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000020 ",
+	.desc = "PF_L2_RFO & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000020 ",
+	.desc = "PF_L2_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000020 ",
+	.desc = "PF_L2_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000020 ",
+	.desc = "PF_L2_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000020 ",
+	.desc = "PF_L2_RFO & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000020 ",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000020 ",
+	.desc = "PF_L2_RFO & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000020 ",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000020 ",
+	.desc = "PF_L2_RFO & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020040 ",
+	.desc = "PF_L2_CODE_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0040 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000040 ",
+	.desc = "PF_L2_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000040 ",
+	.desc = "PF_L2_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000040 ",
+	.desc = "PF_L2_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000040 ",
+	.desc = "PF_L2_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000040 ",
+	.desc = "PF_L2_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000040 ",
+	.desc = "PF_L2_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000040 ",
+	.desc = "PF_L2_CODE_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000040 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000040 ",
+	.desc = "PF_L2_CODE_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000040 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000040 ",
+	.desc = "PF_L2_CODE_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000100 ",
+	.desc = "PF_L3_RFO & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000100 ",
+	.desc = "PF_L3_RFO & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020200 ",
+	.desc = "PF_L3_CODE_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0200 ",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000200 ",
+	.desc = "PF_L3_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000200 ",
+	.desc = "PF_L3_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000200 ",
+	.desc = "PF_L3_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000200 ",
+	.desc = "PF_L3_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000200 ",
+	.desc = "PF_L3_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000200 ",
+	.desc = "PF_L3_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000200 ",
+	.desc = "PF_L3_CODE_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000200 ",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000200 ",
+	.desc = "PF_L3_CODE_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000200 ",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000200 ",
+	.desc = "PF_L3_CODE_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c8000 ",
+	.desc = "Counts any other requests that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc008000 ",
+	.desc = "Counts any other requests that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c008000 ",
+	.desc = "OTHER & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c008000 ",
+	.desc = "Counts any other requests that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c008000 ",
+	.desc = "OTHER & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020090 ",
+	.desc = "ALL_PF_DATA_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0090 ",
+	.desc = "Counts all prefetch data reads that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000090 ",
+	.desc = "ALL_PF_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000090 ",
+	.desc = "ALL_PF_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000090 ",
+	.desc = "ALL_PF_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000090 ",
+	.desc = "ALL_PF_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000090 ",
+	.desc = "ALL_PF_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000090 ",
+	.desc = "ALL_PF_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000090 ",
+	.desc = "ALL_PF_DATA_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000090 ",
+	.desc = "Counts all prefetch data reads that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000090 ",
+	.desc = "ALL_PF_DATA_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000090 ",
+	.desc = "Counts all prefetch data reads that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000090 ",
+	.desc = "ALL_PF_DATA_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020120 ",
+	.desc = "ALL_PF_RFO & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0120 ",
+	.desc = "Counts prefetch RFOs that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000120 ",
+	.desc = "ALL_PF_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000120 ",
+	.desc = "ALL_PF_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000120 ",
+	.desc = "ALL_PF_RFO & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000120 ",
+	.desc = "ALL_PF_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000120 ",
+	.desc = "ALL_PF_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000120 ",
+	.desc = "ALL_PF_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000120 ",
+	.desc = "ALL_PF_RFO & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000120 ",
+	.desc = "Counts prefetch RFOs that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000120 ",
+	.desc = "ALL_PF_RFO & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000120 ",
+	.desc = "Counts prefetch RFOs that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000120 ",
+	.desc = "ALL_PF_RFO & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020240 ",
+	.desc = "ALL_PF_CODE_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0240 ",
+	.desc = "Counts all prefetch code reads that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000240 ",
+	.desc = "ALL_PF_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000240 ",
+	.desc = "ALL_PF_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000240 ",
+	.desc = "ALL_PF_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000240 ",
+	.desc = "ALL_PF_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000240 ",
+	.desc = "ALL_PF_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000240 ",
+	.desc = "ALL_PF_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000240 ",
+	.desc = "ALL_PF_CODE_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000240 ",
+	.desc = "Counts all prefetch code reads that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000240 ",
+	.desc = "ALL_PF_CODE_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000240 ",
+	.desc = "Counts all prefetch code reads that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000240 ",
+	.desc = "ALL_PF_CODE_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020091 ",
+	.desc = "ALL_DATA_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0091 ",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000091 ",
+	.desc = "ALL_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000091 ",
+	.desc = "ALL_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000091 ",
+	.desc = "ALL_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000091 ",
+	.desc = "ALL_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000091 ",
+	.desc = "ALL_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000091 ",
+	.desc = "ALL_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000091 ",
+	.desc = "ALL_DATA_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000091 ",
+	.desc = "Counts all demand & prefetch data reads that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000091 ",
+	.desc = "ALL_DATA_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000091 ",
+	.desc = "Counts all demand & prefetch data reads that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000091 ",
+	.desc = "ALL_DATA_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020122 ",
+	.desc = "ALL_RFO & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20003c0122 ",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3 and the target was non-DRAM system address",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000122 ",
+	.desc = "ALL_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000122 ",
+	.desc = "ALL_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000122 ",
+	.desc = "ALL_RFO & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000122 ",
+	.desc = "ALL_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000122 ",
+	.desc = "ALL_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000122 ",
+	.desc = "ALL_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f84000122 ",
+	.desc = "ALL_RFO & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000122 ",
+	.desc = "Counts all demand & prefetch RFOs that miss the L3 with no details on snoop-related information",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000122 ",
+	.desc = "ALL_RFO & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000122 ",
+	.desc = "Counts all demand & prefetch RFOs that miss the L3 with a snoop miss response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000122 ",
+	.desc = "ALL_RFO & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "dtlb_load_misses.miss_causes_a_walk",
+	.event = "event=0x08,umask=0x1,period=100003",
+	.desc = "Load misses in all DTLB levels that cause page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_4k",
+	.event = "event=0x08,umask=0x2,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (4K)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_2m_4m",
+	.event = "event=0x08,umask=0x4,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (2M/4M)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_1g",
+	.event = "event=0x08,umask=0x8,period=2000003",
+	.desc = "Load miss in all TLB levels causes a page walk that completes. (1G)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_duration",
+	.event = "event=0x08,umask=0x10,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of cycles while PMH is busy with the page walk  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit_4k",
+	.event = "event=0x08,umask=0x20,period=2000003",
+	.desc = "Load misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit_2m",
+	.event = "event=0x08,umask=0x40,period=2000003",
+	.desc = "Load misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.miss_causes_a_walk",
+	.event = "event=0x49,umask=0x1,period=100003",
+	.desc = "Store misses in all DTLB levels that cause page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_4k",
+	.event = "event=0x49,umask=0x2,period=100003",
+	.desc = "Store miss in all TLB levels causes a page walk that completes. (4K)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_2m_4m",
+	.event = "event=0x49,umask=0x4,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks (2M/4M)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_1g",
+	.event = "event=0x49,umask=0x8,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks (1G)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_duration",
+	.event = "event=0x49,umask=0x10,period=100003",
+	.desc = "Cycles when PMH is busy with page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of cycles while PMH is busy with the page walk  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit_4k",
+	.event = "event=0x49,umask=0x20,period=100003",
+	.desc = "Store misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit_2m",
+	.event = "event=0x49,umask=0x40,period=100003",
+	.desc = "Store misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4F,umask=0x10,period=2000003",
+	.desc = "Cycle count for an Extended Page table walk",
+	.topic = "virtual memory",
+	.long_desc = "This event counts cycles for an extended page table walk. The Extended Page directory cache differs from standard TLB caches by the operating system that use it. Virtual machine operating systems use the extended page directory cache, while guest operating systems use the standard TLB caches",
+},
+{
+	.name = "itlb_misses.miss_causes_a_walk",
+	.event = "event=0x85,umask=0x1,period=100003",
+	.desc = "Misses at all ITLB levels that cause page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_completed_4k",
+	.event = "event=0x85,umask=0x2,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (4K)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_completed_2m_4m",
+	.event = "event=0x85,umask=0x4,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (2M/4M)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_completed_1g",
+	.event = "event=0x85,umask=0x8,period=100003",
+	.desc = "Store miss in all TLB levels causes a page walk that completes. (1G)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_duration",
+	.event = "event=0x85,umask=0x10,period=100003",
+	.desc = "Cycles when PMH is busy with page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of cycles while PMH is busy with the page walk  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.stlb_hit_4k",
+	.event = "event=0x85,umask=0x20,period=100003",
+	.desc = "Core misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.stlb_hit_2m",
+	.event = "event=0x85,umask=0x40,period=100003",
+	.desc = "Code misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb.itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=100007",
+	.desc = "Flushing of the Instruction TLB (ITLB) pages, includes 4k/2M/4M pages",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of flushes of the big or small ITLB pages. Counting include both TLB Flush (covering all sets) and TLB Set Clear (set-specific)",
+},
+{
+	.name = "page_walker_loads.dtlb_l1",
+	.event = "event=0xBC,umask=0x11,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L1+FB  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.itlb_l1",
+	.event = "event=0xBC,umask=0x21,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L1+FB  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_l2",
+	.event = "event=0xBC,umask=0x12,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L2  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.itlb_l2",
+	.event = "event=0xBC,umask=0x22,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L2  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_l3",
+	.event = "event=0xBC,umask=0x14,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L3 + XSNP  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.itlb_l3",
+	.event = "event=0xBC,umask=0x24,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L3 + XSNP  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_memory",
+	.event = "event=0xBC,umask=0x18,period=2000003",
+	.desc = "Number of DTLB page walker hits in Memory  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "tlb_flush.dtlb_thread",
+	.event = "event=0xBD,umask=0x1,period=100007",
+	.desc = "DTLB flush attempts of the thread-specific entries",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of DTLB flush attempts of the thread-specific entries",
+},
+{
+	.name = "tlb_flush.stlb_any",
+	.event = "event=0xBD,umask=0x20,period=100007",
+	.desc = "STLB flush attempts",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of any STLB flush attempts (such as entire, VPID, PCID, InvPage, CR3 write, and so on)",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x08,umask=0xe,period=100003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes of any page size  Spec update: BDM69",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x08,umask=0x60,period=2000003",
+	.desc = "Load operations that miss the first DTLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.walk_completed",
+	.event = "event=0x49,umask=0xe,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit",
+	.event = "event=0x49,umask=0x60,period=100003",
+	.desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0xe,period=100003",
+	.desc = "Misses in all ITLB levels that cause completed page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.stlb_hit",
+	.event = "event=0x85,umask=0x60,period=100003",
+	.desc = "Operations that miss the first ITLB level but hit the second and do not cause any page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_miss",
+	.event = "event=0x24,umask=0x21,period=200003",
+	.desc = "Demand Data Read miss L2, no rejects",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests that miss L2 cache. Only not rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_hit",
+	.event = "event=0x24,umask=0x41,period=200003",
+	.desc = "Demand Data Read requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests that hit L2 cache. Only not rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.l2_pf_miss",
+	.event = "event=0x24,umask=0x30,period=200003",
+	.desc = "L2 prefetch requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of requests from the L2 hardware prefetchers that miss L2 cache",
+},
+{
+	.name = "l2_rqsts.l2_pf_hit",
+	.event = "event=0x24,umask=0x50,period=200003",
+	.desc = "L2 prefetch requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of requests from the L2 hardware prefetchers that hit L2 cache. L3 prefetch new types",
+},
+{
+	.name = "l2_rqsts.all_demand_data_rd",
+	.event = "event=0x24,umask=0xe1,period=200003",
+	.desc = "Demand Data Read requests",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests (including requests from L1D hardware prefetchers). These loads may hit or miss L2 cache. Only non rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.all_rfo",
+	.event = "event=0x24,umask=0xe2,period=200003",
+	.desc = "RFO requests to L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of RFO (read for ownership) requests to L2 cache. L2 RFO requests include both L1D demand RFO misses as well as L1D RFO prefetches",
+},
+{
+	.name = "l2_rqsts.all_code_rd",
+	.event = "event=0x24,umask=0xe4,period=200003",
+	.desc = "L2 code requests",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of L2 code requests",
+},
+{
+	.name = "l2_rqsts.all_pf",
+	.event = "event=0x24,umask=0xf8,period=200003",
+	.desc = "Requests from L2 hardware prefetchers",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of requests from the L2 hardware prefetchers",
+},
+{
+	.name = "l2_demand_rqsts.wb_hit",
+	.event = "event=0x27,umask=0x50,period=200003",
+	.desc = "Not rejected writebacks that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of WB requests that hit L2 cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100003",
+	.desc = "Core-originated cacheable demand requests missed L3",
+	.topic = "cache",
+	.long_desc = "This event counts core-originated cacheable demand requests that miss the last level cache (LLC). Demand requests include loads, RFOs, and hardware prefetches from L1D, and instruction fetches from IFU",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=100003",
+	.desc = "Core-originated cacheable demand requests that refer to L3",
+	.topic = "cache",
+	.long_desc = "This event counts core-originated cacheable demand requests that refer to the last level cache (LLC). Demand requests include loads, RFOs, and hardware prefetches from L1D, and instruction fetches from IFU",
+},
+{
+	.name = "l1d_pend_miss.pending",
+	.event = "event=0x48,umask=0x1,period=2000003",
+	.desc = "L1D miss oustandings duration in cycles",
+	.topic = "cache",
+	.long_desc = "This event counts duration of L1D miss outstanding, that is each cycle number of Fill Buffers (FB) outstanding required by Demand Reads. FB either is held by demand loads, or it is held by non-demand loads and gets hit at least once by demand. The valid outstanding interval is defined until the FB deallocation by one of the following ways: from FB allocation, if FB is allocated by demand; from the demand Hit FB, if it is allocated by hardware or software prefetch.\nNote: In the L1D, a Demand Read contains cacheable or noncacheable demand loads, including ones causing cache-line splits and reads due to page walks resulted from any request type",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles",
+	.event = "event=0x48,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding",
+	.topic = "cache",
+	.long_desc = "This event counts duration of L1D miss outstanding in cycles",
+},
+{
+	.name = "l1d.replacement",
+	.event = "event=0x51,umask=0x1,period=2000003",
+	.desc = "L1D data line replacements",
+	.topic = "cache",
+	.long_desc = "This event counts L1D data line replacements including opportunistic replacements, and replacements that require stall-for-replace or block-for-replace",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003",
+	.desc = "Offcore outstanding Demand Data Read transactions in uncore queue  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding Demand Data Read transactions in the super queue (SQ) every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor. See the corresponding Umask under OFFCORE_REQUESTS.\nNote: A prefetch promoted to Demand is counted from the promotion point  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.demand_code_rd",
+	.event = "event=0x60,umask=0x2,period=2000003",
+	.desc = "Offcore outstanding code reads transactions in SuperQueue (SQ), queue to uncore, every cycle  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding Code Reads transactions in the super queue every cycle. The \"Offcore outstanding\" state of the transaction lasts from the L2 miss until the sending transaction completion to requestor (SQ deallocation). See the corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003",
+	.desc = "Offcore outstanding RFO store transactions in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding RFO (store) transactions in the super queue (SQ) every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.all_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003",
+	.desc = "Offcore outstanding cacheable Core Data Read transactions in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding cacheable Core Data Read transactions in the super queue every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts cycles when offcore outstanding Demand Data Read transactions are present in the super queue (SQ). A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation)  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts cycles when offcore outstanding cacheable Core Data Read transactions are present in the super queue. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003,cmask=1",
+	.desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding demand rfo Reads transactions in the super queue every cycle. The \"Offcore outstanding\" state of the transaction lasts from the L2 miss until the sending transaction completion to requestor (SQ deallocation). See the corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "lock_cycles.cache_lock_duration",
+	.event = "event=0x63,umask=0x2,period=2000003",
+	.desc = "Cycles when L1D is locked",
+	.topic = "cache",
+	.long_desc = "This event counts the number of cycles when the L1D is locked. It is a superset of the 0x1 mask (BUS_LOCK_CLOCKS.BUS_LOCK_DURATION)",
+},
+{
+	.name = "offcore_requests.demand_data_rd",
+	.event = "event=0xB0,umask=0x1,period=100003",
+	.desc = "Demand Data Read requests sent to uncore",
+	.topic = "cache",
+	.long_desc = "This event counts the Demand Data Read requests sent to uncore. Use it in conjunction with OFFCORE_REQUESTS_OUTSTANDING to determine average latency in the uncore",
+},
+{
+	.name = "offcore_requests.demand_code_rd",
+	.event = "event=0xB0,umask=0x2,period=100003",
+	.desc = "Cacheable and noncachaeble code read requests",
+	.topic = "cache",
+	.long_desc = "This event counts both cacheable and noncachaeble code read requests",
+},
+{
+	.name = "offcore_requests.demand_rfo",
+	.event = "event=0xB0,umask=0x4,period=100003",
+	.desc = "Demand RFO requests including regular RFOs, locks, ItoM",
+	.topic = "cache",
+	.long_desc = "This event counts the demand RFO (read for ownership) requests including regular RFOs, locks, ItoM",
+},
+{
+	.name = "offcore_requests.all_data_rd",
+	.event = "event=0xB0,umask=0x8,period=100003",
+	.desc = "Demand and prefetch data reads",
+	.topic = "cache",
+	.long_desc = "This event counts the demand and prefetch data reads. All Core Data Reads include cacheable \"Demands\" and L2 prefetchers (not L3 prefetchers). Counting also covers reads due to page walks resulted from any request type",
+},
+{
+	.name = "offcore_requests_buffer.sq_full",
+	.event = "event=0xb2,umask=0x1,period=2000003",
+	.desc = "Offcore requests buffer cannot take more entries for this thread core",
+	.topic = "cache",
+	.long_desc = "This event counts the number of cases when the offcore requests buffer cannot take more entries for the core. This can happen when the superqueue does not contain eligible entries, or when L1D writeback pending FIFO requests is full.\nNote: Writeback pending FIFO has six entries",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_loads",
+	.event = "event=0xD0,umask=0x11,period=100003",
+	.desc = "Retired load uops that miss the STLB  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts load uops with true STLB miss retired to the architected path. True STLB miss is an uop triggering page walk that gets completed without blocks, and later gets retired. This page walk can end up with or without a fault  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_stores",
+	.event = "event=0xD0,umask=0x12,period=100003",
+	.desc = "Retired store uops that miss the STLB  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts store uops with true STLB miss retired to the architected path. True STLB miss is an uop triggering page walk that gets completed without blocks, and later gets retired. This page walk can end up with or without a fault  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.lock_loads",
+	.event = "event=0xD0,umask=0x21,period=100007",
+	.desc = "Retired load uops with locked access  Spec update: BDM35.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts load uops with locked access retired to the architected path  Spec update: BDM35.  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.split_loads",
+	.event = "event=0xD0,umask=0x41,period=100003",
+	.desc = "Retired load uops that split across a cacheline boundary  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts line-splitted load uops retired to the architected path. A line split is across 64B cache-line which includes a page split (4K)  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.split_stores",
+	.event = "event=0xD0,umask=0x42,period=100003",
+	.desc = "Retired store uops that split across a cacheline boundary  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts line-splitted store uops retired to the architected path. A line split is across 64B cache-line which includes a page split (4K)  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0xD0,umask=0x81,period=2000003",
+	.desc = "All retired load uops  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts load uops retired to the architected path with a filter on bits 0 and 1 applied.\nNote: This event counts AVX-256bit load/store double-pump memory uops as a single uop at retirement. This event also counts SW prefetches  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0xD0,umask=0x82,period=2000003",
+	.desc = "All retired store uops  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts store uops retired to the architected path with a filter on bits 0 and 1 applied.\nNote: This event counts AVX-256bit load/store double-pump memory uops as a single uop at retirement  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_hit",
+	.event = "event=0xD1,umask=0x1,period=2000003",
+	.desc = "Retired load uops with L1 cache hits as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were hits in the nearest-level (L1) cache.\nNote: Only two data-sources of L1/FB are applicable for AVX-256bit  even though the corresponding AVX load could be serviced by a deeper level in the memory hierarchy. Data source is reported for the Low-half load. This event also counts SW prefetches independent of the actual data source  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_hit",
+	.event = "event=0xD1,umask=0x2,period=100003",
+	.desc = "Retired load uops with L2 cache hits as data sources  Spec update: BDM35.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were hits in the mid-level (L2) cache  Spec update: BDM35.  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l3_hit",
+	.event = "event=0xD1,umask=0x4,period=50021",
+	.desc = "Retired load uops which data sources were data hits in L3 without snoops required  Spec update: BDM100.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were data hits in the last-level (L3) cache without snoops required  Spec update: BDM100.  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_miss",
+	.event = "event=0xD1,umask=0x8,period=100003",
+	.desc = "Retired load uops misses in L1 cache as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were misses in the nearest-level (L1) cache. Counting excludes unknown and UC data source  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_miss",
+	.event = "event=0xD1,umask=0x10,period=50021",
+	.desc = "Miss in mid-level (L2) cache. Excludes Unknown data-source  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were misses in the mid-level (L2) cache. Counting excludes unknown and UC data source  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l3_miss",
+	.event = "event=0xD1,umask=0x20,period=100007",
+	.desc = "Miss in last-level (L3) cache. Excludes Unknown data-source  Spec update: BDM100, BDE70.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.hit_lfb",
+	.event = "event=0xD1,umask=0x40,period=100003",
+	.desc = "Retired load uops which data sources were load uops missed L1 but hit FB due to preceding miss to the same cache line with data not ready  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were load uops missed L1 but hit a fill buffer due to a preceding miss to the same cache line with the data not ready.\nNote: Only two data-sources of L1/FB are applicable for AVX-256bit  even though the corresponding AVX load could be serviced by a deeper level in the memory hierarchy. Data source is reported for the Low-half load  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_miss",
+	.event = "event=0xD2,umask=0x1,period=20011",
+	.desc = "Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache  Spec update: BDM100.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were L3 Hit and a cross-core snoop missed in the on-pkg core cache  Spec update: BDM100.  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_hit",
+	.event = "event=0xD2,umask=0x2,period=20011",
+	.desc = "Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache  Spec update: BDM100.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were L3 hit and a cross-core snoop hit in the on-pkg core cache  Spec update: BDM100.  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_hitm",
+	.event = "event=0xD2,umask=0x4,period=20011",
+	.desc = "Retired load uops which data sources were HitM responses from shared L3  Spec update: BDM100.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were HitM responses from a core on same socket (shared L3)  Spec update: BDM100.  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_none",
+	.event = "event=0xD2,umask=0x8,period=100003",
+	.desc = "Retired load uops which data sources were hits in L3 without snoops required  Spec update: BDM100.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were hits in the last-level (L3) cache without snoops required  Spec update: BDM100.  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_miss_retired.local_dram",
+	.event = "event=0xD3,umask=0x1,period=100007",
+	.desc = "Data from local DRAM either Snoop not needed or Snoop Miss (RspI)  Spec update: BDE70, BDM100.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uop whose Data Source was: local DRAM either Snoop not needed or Snoop Miss (RspI)  Spec update: BDE70, BDM100.  Supports address when precise (Precise event)",
+},
+{
+	.name = "l2_trans.demand_data_rd",
+	.event = "event=0xF0,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts Demand Data Read requests that access L2 cache, including rejects",
+},
+{
+	.name = "l2_trans.rfo",
+	.event = "event=0xF0,umask=0x2,period=200003",
+	.desc = "RFO requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts Read for Ownership (RFO) requests that access L2 cache",
+},
+{
+	.name = "l2_trans.code_rd",
+	.event = "event=0xF0,umask=0x4,period=200003",
+	.desc = "L2 cache accesses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache accesses when fetching instructions",
+},
+{
+	.name = "l2_trans.all_pf",
+	.event = "event=0xF0,umask=0x8,period=200003",
+	.desc = "L2 or L3 HW prefetches that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L2 or L3 HW prefetches that access L2 cache including rejects",
+},
+{
+	.name = "l2_trans.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200003",
+	.desc = "L1D writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L1D writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_fill",
+	.event = "event=0xF0,umask=0x20,period=200003",
+	.desc = "L2 fill requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L2 fill requests that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_wb",
+	.event = "event=0xF0,umask=0x40,period=200003",
+	.desc = "L2 writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L2 writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.all_requests",
+	.event = "event=0xF0,umask=0x80,period=200003",
+	.desc = "Transactions accessing L2 pipe",
+	.topic = "cache",
+	.long_desc = "This event counts transactions that access the L2 pipe including snoops, pagewalks, and so on",
+},
+{
+	.name = "l2_lines_in.i",
+	.event = "event=0xF1,umask=0x1,period=100003",
+	.desc = "L2 cache lines in I state filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines in the Invalidate state filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_in.s",
+	.event = "event=0xF1,umask=0x2,period=100003",
+	.desc = "L2 cache lines in S state filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines in the Shared state filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_in.e",
+	.event = "event=0xF1,umask=0x4,period=100003",
+	.desc = "L2 cache lines in E state filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines in the Exclusive state filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_in.all",
+	.event = "event=0xF1,umask=0x7,period=100003",
+	.desc = "L2 cache lines filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x5,period=100003",
+	.desc = "Clean L2 cache lines evicted by demand",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xf4,umask=0x10,period=100003",
+	.desc = "Split locks in SQ",
+	.topic = "cache",
+	.long_desc = "This event counts the number of split locks in the super queue",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x42,period=200003",
+	.desc = "RFO requests that hit L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x22,period=200003",
+	.desc = "RFO requests that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.code_rd_hit",
+	.event = "event=0x24,umask=0x44,period=200003",
+	.desc = "L2 cache hits when fetching instructions, code reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.code_rd_miss",
+	.event = "event=0x24,umask=0x24,period=200003",
+	.desc = "L2 cache misses when fetching instructions",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_demand_miss",
+	.event = "event=0x24,umask=0x27,period=200003",
+	.desc = "Demand requests that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_demand_references",
+	.event = "event=0x24,umask=0xe7,period=200003",
+	.desc = "Demand requests to L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.miss",
+	.event = "event=0x24,umask=0x3f,period=200003",
+	.desc = "All requests that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.references",
+	.event = "event=0x24,umask=0xff,period=200003",
+	.desc = "All L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response",
+	.event = "event=0xB7,umask=0x1,period=100003",
+	.desc = "Offcore response can be programmed only with a specific pair of event select and counter MSR, and with specific event codes and predefine mask bit value in a dedicated MSR to specify attributes of the offcore transaction",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd_ge_6",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=6",
+	.desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue  Spec update: BDM76",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles_any",
+	.event = "event=0x48,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.fb_full",
+	.event = "event=0x48,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles a demand request was blocked due to Fill Buffers inavailability",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010001 ",
+	.desc = "Counts demand data reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0001 ",
+	.desc = "Counts demand data reads that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0001 ",
+	.desc = "Counts demand data reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0001 ",
+	.desc = "Counts demand data reads that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0001 ",
+	.desc = "Counts demand data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0001 ",
+	.desc = "Counts demand data reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010002 ",
+	.desc = "Counts all demand data writes (RFOs) that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0002 ",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0002 ",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0002 ",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0002 ",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0002 ",
+	.desc = "DEMAND_RFO & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0002 ",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010004 ",
+	.desc = "Counts all demand code reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0004 ",
+	.desc = "Counts all demand code reads that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0004 ",
+	.desc = "Counts all demand code reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0004 ",
+	.desc = "Counts all demand code reads that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0004 ",
+	.desc = "Counts all demand code reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0004 ",
+	.desc = "Counts all demand code reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010008 ",
+	.desc = "Counts writebacks (modified to exclusive) that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020008 ",
+	.desc = "COREWB & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020008 ",
+	.desc = "COREWB & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020008 ",
+	.desc = "COREWB & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020008 ",
+	.desc = "COREWB & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020008 ",
+	.desc = "COREWB & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020008 ",
+	.desc = "COREWB & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0008 ",
+	.desc = "Counts writebacks (modified to exclusive) that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0008 ",
+	.desc = "Counts writebacks (modified to exclusive) that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0008 ",
+	.desc = "Counts writebacks (modified to exclusive) that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0008 ",
+	.desc = "Counts writebacks (modified to exclusive) that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0008 ",
+	.desc = "COREWB & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0008 ",
+	.desc = "Counts writebacks (modified to exclusive) that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010010 ",
+	.desc = "Counts prefetch (that bring data to L2) data reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020010 ",
+	.desc = "PF_L2_DATA_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020010 ",
+	.desc = "PF_L2_DATA_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020010 ",
+	.desc = "PF_L2_DATA_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020010 ",
+	.desc = "PF_L2_DATA_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020010 ",
+	.desc = "PF_L2_DATA_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020010 ",
+	.desc = "PF_L2_DATA_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0010 ",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0010 ",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0010 ",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0010 ",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0010 ",
+	.desc = "PF_L2_DATA_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0010 ",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010020 ",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020020 ",
+	.desc = "PF_L2_RFO & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020020 ",
+	.desc = "PF_L2_RFO & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020020 ",
+	.desc = "PF_L2_RFO & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020020 ",
+	.desc = "PF_L2_RFO & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020020 ",
+	.desc = "PF_L2_RFO & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020020 ",
+	.desc = "PF_L2_RFO & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0020 ",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0020 ",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0020 ",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0020 ",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0020 ",
+	.desc = "PF_L2_RFO & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0020 ",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010040 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020040 ",
+	.desc = "PF_L2_CODE_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020040 ",
+	.desc = "PF_L2_CODE_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020040 ",
+	.desc = "PF_L2_CODE_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020040 ",
+	.desc = "PF_L2_CODE_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020040 ",
+	.desc = "PF_L2_CODE_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020040 ",
+	.desc = "PF_L2_CODE_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0040 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0040 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0040 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0040 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0040 ",
+	.desc = "PF_L2_CODE_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0040 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0100 ",
+	.desc = "PF_L3_RFO & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010200 ",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020200 ",
+	.desc = "PF_L3_CODE_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020200 ",
+	.desc = "PF_L3_CODE_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020200 ",
+	.desc = "PF_L3_CODE_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020200 ",
+	.desc = "PF_L3_CODE_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020200 ",
+	.desc = "PF_L3_CODE_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020200 ",
+	.desc = "PF_L3_CODE_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0200 ",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0200 ",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0200 ",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0200 ",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0200 ",
+	.desc = "PF_L3_CODE_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0200 ",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000018000 ",
+	.desc = "Counts any other requests that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c8000 ",
+	.desc = "Counts any other requests that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c8000 ",
+	.desc = "Counts any other requests that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c8000 ",
+	.desc = "Counts any other requests that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c8000 ",
+	.desc = "Counts any other requests that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c8000 ",
+	.desc = "OTHER & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c8000 ",
+	.desc = "Counts any other requests that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010090 ",
+	.desc = "Counts all prefetch data reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020090 ",
+	.desc = "ALL_PF_DATA_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020090 ",
+	.desc = "ALL_PF_DATA_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020090 ",
+	.desc = "ALL_PF_DATA_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020090 ",
+	.desc = "ALL_PF_DATA_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020090 ",
+	.desc = "ALL_PF_DATA_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020090 ",
+	.desc = "ALL_PF_DATA_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0090 ",
+	.desc = "Counts all prefetch data reads that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0090 ",
+	.desc = "Counts all prefetch data reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0090 ",
+	.desc = "Counts all prefetch data reads that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0090 ",
+	.desc = "Counts all prefetch data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0090 ",
+	.desc = "ALL_PF_DATA_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0090 ",
+	.desc = "Counts all prefetch data reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010120 ",
+	.desc = "Counts prefetch RFOs that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020120 ",
+	.desc = "ALL_PF_RFO & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020120 ",
+	.desc = "ALL_PF_RFO & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020120 ",
+	.desc = "ALL_PF_RFO & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020120 ",
+	.desc = "ALL_PF_RFO & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020120 ",
+	.desc = "ALL_PF_RFO & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020120 ",
+	.desc = "ALL_PF_RFO & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0120 ",
+	.desc = "Counts prefetch RFOs that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0120 ",
+	.desc = "Counts prefetch RFOs that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0120 ",
+	.desc = "Counts prefetch RFOs that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0120 ",
+	.desc = "Counts prefetch RFOs that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0120 ",
+	.desc = "ALL_PF_RFO & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0120 ",
+	.desc = "Counts prefetch RFOs that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010240 ",
+	.desc = "Counts all prefetch code reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020240 ",
+	.desc = "ALL_PF_CODE_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020240 ",
+	.desc = "ALL_PF_CODE_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020240 ",
+	.desc = "ALL_PF_CODE_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020240 ",
+	.desc = "ALL_PF_CODE_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020240 ",
+	.desc = "ALL_PF_CODE_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020240 ",
+	.desc = "ALL_PF_CODE_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0240 ",
+	.desc = "Counts all prefetch code reads that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0240 ",
+	.desc = "Counts all prefetch code reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0240 ",
+	.desc = "Counts all prefetch code reads that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0240 ",
+	.desc = "Counts all prefetch code reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0240 ",
+	.desc = "ALL_PF_CODE_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0240 ",
+	.desc = "Counts all prefetch code reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010091 ",
+	.desc = "Counts all demand & prefetch data reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020091 ",
+	.desc = "ALL_DATA_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020091 ",
+	.desc = "ALL_DATA_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020091 ",
+	.desc = "ALL_DATA_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020091 ",
+	.desc = "ALL_DATA_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020091 ",
+	.desc = "ALL_DATA_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020091 ",
+	.desc = "ALL_DATA_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0091 ",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0091 ",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0091 ",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0091 ",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0091 ",
+	.desc = "ALL_DATA_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0091 ",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010122 ",
+	.desc = "Counts all demand & prefetch RFOs that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020122 ",
+	.desc = "ALL_RFO & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020122 ",
+	.desc = "ALL_RFO & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020122 ",
+	.desc = "ALL_RFO & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020122 ",
+	.desc = "ALL_RFO & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020122 ",
+	.desc = "ALL_RFO & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f80020122 ",
+	.desc = "ALL_RFO & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00803c0122 ",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01003c0122 ",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02003c0122 ",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0122 ",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0122 ",
+	.desc = "ALL_RFO & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0122 ",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_haswell[] = {
+{
+	.name = "other_assists.avx_to_sse",
+	.event = "event=0xC1,umask=0x8,period=100003",
+	.desc = "Number of transitions from AVX-256 to legacy SSE when penalty applicable  Spec update: HSD56, HSM57",
+	.topic = "floating point",
+},
+{
+	.name = "other_assists.sse_to_avx",
+	.event = "event=0xC1,umask=0x10,period=100003",
+	.desc = "Number of transitions from SSE to AVX-256 when penalty applicable  Spec update: HSD56, HSM57",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.x87_output",
+	.event = "event=0xCA,umask=0x2,period=100003",
+	.desc = "Number of X87 assists due to output value",
+	.topic = "floating point",
+	.long_desc = "Number of X87 FP assists due to output values",
+},
+{
+	.name = "fp_assist.x87_input",
+	.event = "event=0xCA,umask=0x4,period=100003",
+	.desc = "Number of X87 assists due to input value",
+	.topic = "floating point",
+	.long_desc = "Number of X87 FP assists due to input values",
+},
+{
+	.name = "fp_assist.simd_output",
+	.event = "event=0xCA,umask=0x8,period=100003",
+	.desc = "Number of SIMD FP assists due to Output values",
+	.topic = "floating point",
+	.long_desc = "Number of SIMD FP assists due to output values",
+},
+{
+	.name = "fp_assist.simd_input",
+	.event = "event=0xCA,umask=0x10,period=100003",
+	.desc = "Number of SIMD FP assists due to input values",
+	.topic = "floating point",
+	.long_desc = "Number of SIMD FP assists due to input values",
+},
+{
+	.name = "fp_assist.any",
+	.event = "event=0xCA,umask=0x1e,period=100003,cmask=1",
+	.desc = "Cycles with any input/output SSE or FP assist",
+	.topic = "floating point",
+	.long_desc = "Cycles with any input/output SSE* or FP assists",
+},
+{
+	.name = "avx_insts.all",
+	.event = "event=0xC6,umask=0x7,period=2000003",
+	.desc = "Approximate counts of AVX & AVX2 256-bit instructions, including non-arithmetic instructions, loads, and stores.  May count non-AVX instructions that employ 256-bit operations, including (but not necessarily limited to) rep string instructions that use 256-bit loads and stores for optimized performance, XSAVE* and XRSTOR*, and operations that transition the x87 FPU data registers between x87 and MMX",
+	.topic = "floating point",
+	.long_desc = "Note that a whole rep string only counts AVX_INST.ALL once",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired from execution  Spec update: HSD140, HSD143",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions retired from execution. For instructions that consist of multiple micro-ops, this event counts the retirement of the last micro-op of the instruction. Counting continues during hardware interrupts, traps, and inside interrupt handlers. INST_RETIRED.ANY is counted by a designated fixed counter, leaving the programmable counters available for other events. Faulting executions of GETSEC/VM entry/VM Exit/MWait will not count as retired instructions  Spec update: HSD140, HSD143",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Core cycles when the thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of thread cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. The core frequency may change from time to time due to power or thermal throttling",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Reference cycles when the core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of reference cycles when the core is not in a halt state. The core enters the halt state when it is running the HLT instruction or the MWAIT instruction. This event is not affected by core frequency changes (for example, P states, TM2 transitions) but has the same incrementing frequency as the time stamp counter. This event can approximate elapsed time while the core was not in a halt state",
+},
+{
+	.name = "ld_blocks.store_forward",
+	.event = "event=0x03,umask=0x2,period=100003",
+	.desc = "loads blocked by overlapping with store buffer that cannot be forwarded",
+	.topic = "pipeline",
+	.long_desc = "This event counts loads that followed a store to the same address, where the data could not be forwarded inside the pipeline from the store to the load.  The most common reason why store forwarding would be blocked is when a load's address range overlaps with a preceding smaller uncompleted store. The penalty for blocked store forwarding is that the load must wait for the store to write its value to the cache before it can be issued",
+},
+{
+	.name = "ld_blocks.no_sr",
+	.event = "event=0x03,umask=0x8,period=100003",
+	.desc = "The number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+	.topic = "pipeline",
+	.long_desc = "The number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+},
+{
+	.name = "ld_blocks_partial.address_alias",
+	.event = "event=0x07,umask=0x1,period=100003",
+	.desc = "False dependencies in MOB due to partial compare on address",
+	.topic = "pipeline",
+	.long_desc = "Aliasing occurs when a load is issued after a store and their memory addresses are offset by 4K.  This event counts the number of loads that aliased with a preceding store, resulting in an extended address check in the pipeline which can have a performance impact",
+},
+{
+	.name = "int_misc.recovery_cycles",
+	.event = "event=0x0D,umask=0x3,period=2000003,cmask=1",
+	.desc = "Number of cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc...)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of cycles spent waiting for a recovery after an event such as a processor nuke, JEClear, assist, hle/rtm abort etc",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0x0E,umask=0x1,period=2000003",
+	.desc = "Uops that Resource Allocation Table (RAT) issues to Reservation Station (RS)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of uops issued by the Front-end of the pipeline to the Back-end. This event is counted at the allocation stage and will count both retired and non-retired uops",
+},
+{
+	.name = "uops_issued.flags_merge",
+	.event = "event=0x0E,umask=0x10,period=2000003",
+	.desc = "Number of flags-merge uops being allocated. Such uops considered perf sensitive; added by GSR u-arch",
+	.topic = "pipeline",
+	.long_desc = "Number of flags-merge uops allocated. Such uops add delay",
+},
+{
+	.name = "uops_issued.slow_lea",
+	.event = "event=0x0E,umask=0x20,period=2000003",
+	.desc = "Number of slow LEA uops being allocated. A uop is generally considered SlowLea if it has 3 sources (e.g. 2 sources + immediate) regardless if as a result of LEA instruction or not",
+	.topic = "pipeline",
+	.long_desc = "Number of slow LEA or similar uops allocated. Such uop has 3 sources (for example, 2 sources + immediate) regardless of whether it is a result of LEA instruction or not",
+},
+{
+	.name = "uops_issued.single_mul",
+	.event = "event=0x0E,umask=0x40,period=2000003",
+	.desc = "Number of Multiply packed/scalar single precision uops allocated",
+	.topic = "pipeline",
+	.long_desc = "Number of multiply packed/scalar single precision uops allocated",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.core_stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for all threads",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.divider_uops",
+	.event = "event=0x14,umask=0x2,period=2000003",
+	.desc = "Any uop executed by the Divider. (This includes all divide uops, sqrt, ...)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Increments at the frequency of XCLK (100 MHz) when not halted",
+},
+{
+	.name = "cpu_clk_thread_unhalted.one_thread_active",
+	.event = "event=0x3c,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.sw_pf",
+	.event = "event=0x4c,umask=0x1,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for software prefetch",
+	.topic = "pipeline",
+	.long_desc = "Non-SW-prefetch load dispatches that hit fill buffer allocated for S/W prefetch",
+},
+{
+	.name = "load_hit_pre.hw_pf",
+	.event = "event=0x4c,umask=0x2,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for hardware prefetch",
+	.topic = "pipeline",
+	.long_desc = "Non-SW-prefetch load dispatches that hit fill buffer allocated for H/W prefetch",
+},
+{
+	.name = "move_elimination.int_eliminated",
+	.event = "event=0x58,umask=0x1,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+	.long_desc = "Number of integer move elimination candidate uops that were eliminated",
+},
+{
+	.name = "move_elimination.simd_eliminated",
+	.event = "event=0x58,umask=0x2,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+	.long_desc = "Number of SIMD move elimination candidate uops that were eliminated",
+},
+{
+	.name = "move_elimination.int_not_eliminated",
+	.event = "event=0x58,umask=0x4,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+	.long_desc = "Number of integer move elimination candidate uops that were not eliminated",
+},
+{
+	.name = "move_elimination.simd_not_eliminated",
+	.event = "event=0x58,umask=0x8,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+	.long_desc = "Number of SIMD move elimination candidate uops that were not eliminated",
+},
+{
+	.name = "rs_events.empty_cycles",
+	.event = "event=0x5E,umask=0x1,period=2000003",
+	.desc = "Cycles when Reservation Station (RS) is empty for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles when the Reservation Station ( RS ) is empty for the thread. The RS is a structure that buffers allocated micro-ops from the Front-end. If there are many cycles when the RS is empty, it may represent an underflow of instructions delivered from the Front-end",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000003",
+	.desc = "Stalls caused by changing prefix length of the instruction",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles where the decoder is stalled on an instruction with a length changing prefix (LCP)",
+},
+{
+	.name = "ild_stall.iq_full",
+	.event = "event=0x87,umask=0x4,period=2000003",
+	.desc = "Stall cycles because IQ is full",
+	.topic = "pipeline",
+	.long_desc = "Stall cycles due to IQ is full",
+},
+{
+	.name = "br_inst_exec.nontaken_conditional",
+	.event = "event=0x88,umask=0x41,period=200003",
+	.desc = "Not taken macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_conditional",
+	.event = "event=0x88,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_direct_jump",
+	.event = "event=0x88,umask=0x82,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branch instructions excluding calls and indirects",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_return",
+	.event = "event=0x88,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired indirect branches with return mnemonic",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_direct_near_call",
+	.event = "event=0x88,umask=0x90,period=200003",
+	.desc = "Taken speculative and retired direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_call",
+	.event = "event=0x88,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_conditional",
+	.event = "event=0x88,umask=0xc1,period=200003",
+	.desc = "Speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_direct_jmp",
+	.event = "event=0x88,umask=0xc2,period=200003",
+	.desc = "Speculative and retired macro-unconditional branches excluding calls and indirects",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0xc4,period=200003",
+	.desc = "Speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_indirect_near_return",
+	.event = "event=0x88,umask=0xc8,period=200003",
+	.desc = "Speculative and retired indirect return branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_direct_near_call",
+	.event = "event=0x88,umask=0xd0,period=200003",
+	.desc = "Speculative and retired direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_branches",
+	.event = "event=0x88,umask=0xff,period=200003",
+	.desc = "Speculative and retired  branches",
+	.topic = "pipeline",
+	.long_desc = "Counts all near executed branches (not necessarily retired)",
+},
+{
+	.name = "br_misp_exec.nontaken_conditional",
+	.event = "event=0x89,umask=0x41,period=200003",
+	.desc = "Not taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_conditional",
+	.event = "event=0x89,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_return_near",
+	.event = "event=0x89,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches with return mnemonic",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_conditional",
+	.event = "event=0x89,umask=0xc1,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0xc4,period=200003",
+	.desc = "Mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_branches",
+	.event = "event=0x89,umask=0xff,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Counts all near executed branches (not necessarily retired)",
+},
+{
+	.name = "uops_executed_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 0",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 0 in this thread",
+},
+{
+	.name = "uops_executed_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 1",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 1 in this thread",
+},
+{
+	.name = "uops_executed_port.port_2",
+	.event = "event=0xA1,umask=0x4,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 2",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 2 in this thread",
+},
+{
+	.name = "uops_executed_port.port_3",
+	.event = "event=0xA1,umask=0x8,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 3",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 3 in this thread",
+},
+{
+	.name = "uops_executed_port.port_4",
+	.event = "event=0xA1,umask=0x10,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 4",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 4 in this thread",
+},
+{
+	.name = "uops_executed_port.port_5",
+	.event = "event=0xA1,umask=0x20,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 5",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 5 in this thread",
+},
+{
+	.name = "uops_executed_port.port_6",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 6",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 6 in this thread",
+},
+{
+	.name = "uops_executed_port.port_7",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 7",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 7 in this thread",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000003",
+	.desc = "Resource-related stall cycles  Spec update: HSD135",
+	.topic = "pipeline",
+	.long_desc = "Cycles allocation is stalled due to resource related reason  Spec update: HSD135",
+},
+{
+	.name = "resource_stalls.rs",
+	.event = "event=0xA2,umask=0x4,period=2000003",
+	.desc = "Cycles stalled due to no eligible RS entry available",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.sb",
+	.event = "event=0xA2,umask=0x8,period=2000003",
+	.desc = "Cycles stalled due to no store buffers available. (not including draining form sync)",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which no instructions were allocated because no Store Buffers (SB) were available",
+},
+{
+	.name = "resource_stalls.rob",
+	.event = "event=0xA2,umask=0x10,period=2000003",
+	.desc = "Cycles stalled due to re-order buffer full",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l2_pending",
+	.event = "event=0xA3,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with pending L2 cache miss loads  Spec update: HSD78",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending L2 miss loads. Set Cmask=2 to count cycle  Spec update: HSD78",
+},
+{
+	.name = "cycle_activity.cycles_l1d_pending",
+	.event = "event=0xA3,umask=0x8,period=2000003,cmask=8",
+	.desc = "Cycles with pending L1 cache miss loads",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending L1 data cache miss loads. Set Cmask=8 to count cycle",
+},
+{
+	.name = "cycle_activity.cycles_ldm_pending",
+	.event = "event=0xA3,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles with pending memory loads",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending memory loads. Set Cmask=2 to count cycle",
+},
+{
+	.name = "cycle_activity.cycles_no_execute",
+	.event = "event=0xA3,umask=0x4,period=2000003,cmask=4",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which no instructions were executed in the execution stage of the pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l2_pending",
+	.event = "event=0xA3,umask=0x5,period=2000003,cmask=5",
+	.desc = "Execution stalls due to L2 cache misses",
+	.topic = "pipeline",
+	.long_desc = "Number of loads missed L2",
+},
+{
+	.name = "cycle_activity.stalls_ldm_pending",
+	.event = "event=0xA3,umask=0x6,period=2000003,cmask=6",
+	.desc = "Execution stalls due to memory subsystem",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which no instructions were executed in the execution stage of the pipeline and there were memory instructions pending (waiting for data)",
+},
+{
+	.name = "cycle_activity.stalls_l1d_pending",
+	.event = "event=0xA3,umask=0xc,period=2000003,cmask=12",
+	.desc = "Execution stalls due to L1 data cache misses",
+	.topic = "pipeline",
+	.long_desc = "Execution stalls due to L1 data cache miss loads. Set Cmask=0CH",
+},
+{
+	.name = "lsd.uops",
+	.event = "event=0xa8,umask=0x1,period=2000003",
+	.desc = "Number of Uops delivered by the LSD",
+	.topic = "pipeline",
+	.long_desc = "Number of uops delivered by the LSD",
+},
+{
+	.name = "uops_executed.core",
+	.event = "event=0xB1,umask=0x2,period=2000003",
+	.desc = "Number of uops executed on the core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+	.long_desc = "Counts total number of uops to be executed per-core each cycle  Spec update: HSD30, HSM31",
+},
+{
+	.name = "uops_executed.stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts number of cycles no uops were dispatched to be executed on this thread  Spec update: HSD144, HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Number of instructions retired. General Counter   - architectural event  Spec update: HSD11, HSD140",
+	.topic = "pipeline",
+	.long_desc = "Number of instructions at retirement  Spec update: HSD11, HSD140",
+},
+{
+	.name = "inst_retired.x87",
+	.event = "event=0xC0,umask=0x2,period=2000003",
+	.desc = "FP operations retired. X87 FP operations that have no exceptions: Counts also flows that have several X87 or flows that use X87 uops in the exception handling",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts FP operations retired. For X87 FP operations that have no exceptions counting also includes flows that have several X87, or flows that use X87 uops in the exception handling",
+},
+{
+	.name = "inst_retired.prec_dist",
+	.event = "event=0xC0,umask=0x1,period=2000003",
+	.desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution  Spec update: HSD140 (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution  Spec update: HSD140 (Must be precise)",
+},
+{
+	.name = "other_assists.any_wb_assist",
+	.event = "event=0xC1,umask=0x40,period=100003",
+	.desc = "Number of times any microcode assist is invoked by HW upon uop writeback",
+	.topic = "pipeline",
+	.long_desc = "Number of microcode assists invoked by HW upon uop writeback",
+},
+{
+	.name = "uops_retired.all",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "Actually retired uops  Supports address when precise (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of micro-ops retired. Use Cmask=1 and invert to count active cycles or stalled cycles  Supports address when precise (Precise event)",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000003",
+	.desc = "Retirement slots used (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of retirement slots used each cycle.  There are potentially 4 slots that can be used each cycle - meaning, 4 uops or 4 instructions could retire each cycle (Precise event)",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=10",
+	.desc = "Cycles with less than 10 actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.core_stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.cycles",
+	.event = "event=0xC3,umask=0x1,period=2000003",
+	.desc = "Cycles there was a Nuke. Account for both thread-specific and All Thread Nukes",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=100003",
+	.desc = "Self-modifying code (SMC) detected",
+	.topic = "pipeline",
+	.long_desc = "This event is incremented when self-modifying code (SMC) is detected, which causes a machine clear.  Machine clears can have a significant performance impact if they are happening frequently",
+},
+{
+	.name = "machine_clears.maskmov",
+	.event = "event=0xC3,umask=0x20,period=100003",
+	.desc = "This event counts the number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=400009",
+	.desc = "Conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=100003",
+	.desc = "Direct and indirect near call instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=400009",
+	.desc = "All (macro) branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Branch instructions at retirement",
+},
+{
+	.name = "br_inst_retired.near_return",
+	.event = "event=0xC4,umask=0x8,period=100003",
+	.desc = "Return instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of near return instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.not_taken",
+	.event = "event=0xC4,umask=0x10,period=400009",
+	.desc = "Not taken branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of not taken branch instructions retired",
+},
+{
+	.name = "br_inst_retired.near_taken",
+	.event = "event=0xC4,umask=0x20,period=400009",
+	.desc = "Taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Number of near taken branches retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0x40,period=100003",
+	.desc = "Far branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Number of far branches retired",
+},
+{
+	.name = "br_inst_retired.all_branches_pebs",
+	.event = "event=0xC4,umask=0x4,period=400009",
+	.desc = "All (macro) branch instructions retired (Must be precise)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=400009",
+	.desc = "Mispredicted conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=400009",
+	.desc = "All mispredicted macro branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted branch instructions at retirement",
+},
+{
+	.name = "br_misp_retired.all_branches_pebs",
+	.event = "event=0xC5,umask=0x4,period=400009",
+	.desc = "Mispredicted macro branch instructions retired (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This event counts all mispredicted branch instructions retired. This is a precise event (Must be precise)",
+},
+{
+	.name = "rob_misc_events.lbr_inserts",
+	.event = "event=0xCC,umask=0x20,period=2000003",
+	.desc = "Count cases of saving new LBR",
+	.topic = "pipeline",
+	.long_desc = "Count cases of saving new LBR records by hardware",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Thread cycles when thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of thread cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. The core frequency may change from time to time due to power or thermal throttling",
+},
+{
+	.name = "br_misp_exec.taken_indirect_near_call",
+	.event = "event=0x89,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_0_core",
+	.event = "event=0xA1,umask=0x1,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_1_core",
+	.event = "event=0xA1,umask=0x2,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_2_core",
+	.event = "event=0xA1,umask=0x4,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 2",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_3_core",
+	.event = "event=0xA1,umask=0x8,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 3",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_4_core",
+	.event = "event=0xA1,umask=0x10,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_5_core",
+	.event = "event=0xA1,umask=0x20,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_6_core",
+	.event = "event=0xA1,umask=0x40,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 6",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_7_core",
+	.event = "event=0xA1,umask=0x80,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 7",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_taken",
+	.event = "event=0xC5,umask=0x20,period=400009",
+	.desc = "number of near branch instructions retired that were mispredicted and taken (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Number of near branch instructions retired that were taken but mispredicted (Precise event)",
+},
+{
+	.name = "uops_executed.cycles_ge_1_uop_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles where at least 1 uop was executed per-thread  Spec update: HSD144, HSD30, HSM31",
+	.topic = "pipeline",
+	.long_desc = "This events counts the cycles where at least one uop was executed. It is counted per thread  Spec update: HSD144, HSD30, HSM31",
+},
+{
+	.name = "uops_executed.cycles_ge_2_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles where at least 2 uops were executed per-thread  Spec update: HSD144, HSD30, HSM31",
+	.topic = "pipeline",
+	.long_desc = "This events counts the cycles where at least two uop were executed. It is counted per thread  Spec update: HSD144, HSD30, HSM31",
+},
+{
+	.name = "uops_executed.cycles_ge_3_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles where at least 3 uops were executed per-thread  Spec update: HSD144, HSD30, HSM31",
+	.topic = "pipeline",
+	.long_desc = "This events counts the cycles where at least three uop were executed. It is counted per thread  Spec update: HSD144, HSD30, HSM31",
+},
+{
+	.name = "uops_executed.cycles_ge_4_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles where at least 4 uops were executed per-thread  Spec update: HSD144, HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "baclears.any",
+	.event = "event=0xe6,umask=0x1f,period=100003",
+	.desc = "Counts the total number when the front end is resteered, mainly when the BPU cannot provide a correct prediction and this is corrected by other branch handling mechanisms at the front end",
+	.topic = "pipeline",
+	.long_desc = "Number of front end re-steers due to BPU misprediction",
+},
+{
+	.name = "machine_clears.count",
+	.event = "event=0xC3,umask=0x1,edge=1,period=100003,cmask=1",
+	.desc = "Number of machine clears (nukes) of any type",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_active",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_4_uops",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_end",
+	.event = "event=0x5E,inv=1,umask=0x1,edge=1,period=200003,cmask=1",
+	.desc = "Counts end of periods where the Reservation Station (RS) was empty. Could be useful to precisely locate Frontend Latency Bound issues",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_2",
+	.event = "event=0xA1,umask=0x4,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 2",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_3",
+	.event = "event=0xA1,umask=0x8,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 3",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_4",
+	.event = "event=0xA1,umask=0x10,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_5",
+	.event = "event=0xA1,umask=0x20,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_6",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 6",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_7",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 7",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_any",
+	.event = "event=0x3c,any=1",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p_any",
+	.event = "event=0x3C,umask=0x0,any=1,period=2000003",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+},
+{
+	.name = "int_misc.recovery_cycles_any",
+	.event = "event=0x0D,umask=0x3,any=1,period=2000003,cmask=1",
+	.desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+	.topic = "pipeline",
+	.long_desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+},
+{
+	.name = "uops_executed.core_cycles_ge_1",
+	.event = "event=0xb1,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles at least 1 micro-op is executed from any thread on physical core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_2",
+	.event = "event=0xb1,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles at least 2 micro-op is executed from any thread on physical core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_3",
+	.event = "event=0xb1,umask=0x2,period=2000003,cmask=3",
+	.desc = "Cycles at least 3 micro-op is executed from any thread on physical core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_4",
+	.event = "event=0xb1,umask=0x2,period=2000003,cmask=4",
+	.desc = "Cycles at least 4 micro-op is executed from any thread on physical core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_none",
+	.event = "event=0xb1,inv=1,umask=0x2,period=2000003",
+	.desc = "Cycles with no micro-ops executed from any thread on physical core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the thread is unhalted. (counts at 100 MHz rate)",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+},
+{
+	.name = "cpu_clk_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "idq.empty",
+	.event = "event=0x79,umask=0x2,period=2000003",
+	.desc = "Instruction Decode Queue (IDQ) empty cycles  Spec update: HSD135",
+	.topic = "frontend",
+	.long_desc = "Counts cycles the IDQ is empty  Spec update: HSD135",
+},
+{
+	.name = "idq.mite_uops",
+	.event = "event=0x79,umask=0x4,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ from MITE path. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.dsb_uops",
+	.event = "event=0x79,umask=0x8,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle. # of uops delivered to IDQ from DSB path. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.ms_dsb_uops",
+	.event = "event=0x79,umask=0x10,period=2000003",
+	.desc = "Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ when MS_busy by DSB. Set Cmask = 1 to count cycles. Add Edge=1 to count # of delivery",
+},
+{
+	.name = "idq.ms_mite_uops",
+	.event = "event=0x79,umask=0x20,period=2000003",
+	.desc = "Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ when MS_busy by MITE. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.ms_uops",
+	.event = "event=0x79,umask=0x30,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts uops delivered by the Front-end with the assistance of the microcode sequencer.  Microcode assists are used for complex instructions or scenarios that can't be handled by the standard decoder.  Using other instructions, if possible, will usually improve performance",
+},
+{
+	.name = "idq.ms_cycles",
+	.event = "event=0x79,umask=0x30,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which the microcode sequencer assisted the Front-end in delivering uops.  Microcode assists are used for complex instructions or scenarios that can't be handled by the standard decoder.  Using other instructions, if possible, will usually improve performance",
+},
+{
+	.name = "idq.mite_cycles",
+	.event = "event=0x79,umask=0x4,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.dsb_cycles",
+	.event = "event=0x79,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_dsb_cycles",
+	.event = "event=0x79,umask=0x10,period=2000003,cmask=1",
+	.desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_dsb_occur",
+	.event = "event=0x79,umask=0x10,edge=1,period=2000003,cmask=1",
+	.desc = "Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.all_dsb_cycles_4_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=4",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "Counts cycles DSB is delivered four uops. Set Cmask = 4",
+},
+{
+	.name = "idq.all_dsb_cycles_any_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=1",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "Counts cycles DSB is delivered at least one uops. Set Cmask = 1",
+},
+{
+	.name = "idq.all_mite_cycles_4_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=4",
+	.desc = "Cycles MITE is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "Counts cycles MITE is delivered four uops. Set Cmask = 4",
+},
+{
+	.name = "idq.all_mite_cycles_any_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=1",
+	.desc = "Cycles MITE is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "Counts cycles MITE is delivered at least one uop. Set Cmask = 1",
+},
+{
+	.name = "idq.mite_all_uops",
+	.event = "event=0x79,umask=0x3c,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "Number of uops delivered to IDQ from any path",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=2000003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Reads. both cacheable and noncacheable, including UC fetches",
+	.topic = "frontend",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Misses. Includes Uncacheable accesses",
+	.topic = "frontend",
+	.long_desc = "This event counts Instruction Cache (ICACHE) misses",
+},
+{
+	.name = "icache.ifetch_stall",
+	.event = "event=0x80,umask=0x4,period=2000003",
+	.desc = "Cycles where a code fetch is stalled due to L1 instruction-cache miss",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.core",
+	.event = "event=0x9C,umask=0x1,period=2000003",
+	.desc = "Uops not delivered to Resource Allocation Table (RAT) per thread when backend of the machine is not stalled  Spec update: HSD135",
+	.topic = "frontend",
+	.long_desc = "This event count the number of undelivered (unallocated) uops from the Front-end to the Resource Allocation Table (RAT) while the Back-end of the processor is not stalled. The Front-end can allocate up to 4 uops per cycle so this event can increment 0-4 times per cycle depending on the number of unallocated uops. This event is counted on a per-core basis  Spec update: HSD135",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_0_uops_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles per thread when 4 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled  Spec update: HSD135",
+	.topic = "frontend",
+	.long_desc = "This event counts the number cycles during which the Front-end allocated exactly zero uops to the Resource Allocation Table (RAT) while the Back-end of the processor is not stalled.  This event is counted on a per-core basis  Spec update: HSD135",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_1_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles per thread when 3 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled  Spec update: HSD135",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_2_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles with less than 2 uops delivered by the front end  Spec update: HSD135",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_3_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with less than 3 uops delivered by the front end  Spec update: HSD135",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_fe_was_ok",
+	.event = "event=0x9C,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts cycles FE delivered 4 uops or Resource Allocation Table (RAT) was stalling FE  Spec update: HSD135",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.penalty_cycles",
+	.event = "event=0xAB,umask=0x2,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_switches",
+	.event = "event=0x79,umask=0x30,edge=1,period=2000003,cmask=1",
+	.desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+	.topic = "frontend",
+},
+{
+	.name = "icache.ifdata_stall",
+	.event = "event=0x80,umask=0x4,period=2000003",
+	.desc = "Cycles where a code fetch is stalled due to L1 instruction-cache miss",
+	.topic = "frontend",
+},
+{
+	.name = "cpl_cycles.ring0",
+	.event = "event=0x5C,umask=0x1,period=2000003",
+	.desc = "Unhalted core cycles when the thread is in ring 0",
+	.topic = "other",
+	.long_desc = "Unhalted core cycles when the thread is in ring 0",
+},
+{
+	.name = "cpl_cycles.ring123",
+	.event = "event=0x5C,umask=0x2,period=2000003",
+	.desc = "Unhalted core cycles when thread is in rings 1, 2, or 3",
+	.topic = "other",
+	.long_desc = "Unhalted core cycles when the thread is not in ring 0",
+},
+{
+	.name = "cpl_cycles.ring0_trans",
+	.event = "event=0x5C,umask=0x1,edge=1,period=100003,cmask=1",
+	.desc = "Number of intervals between processor halts while thread is in ring 0",
+	.topic = "other",
+},
+{
+	.name = "lock_cycles.split_lock_uc_lock_duration",
+	.event = "event=0x63,umask=0x1,period=2000003",
+	.desc = "Cycles when L1 and L2 are locked due to UC or split lock",
+	.topic = "other",
+	.long_desc = "Cycles in which the L1D and L2 are locked, due to a UC lock or split lock",
+},
+{
+	.name = "misalign_mem_ref.loads",
+	.event = "event=0x05,umask=0x1,period=2000003",
+	.desc = "Speculative cache line split load uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "Speculative cache-line split load uops dispatched to L1D",
+},
+{
+	.name = "misalign_mem_ref.stores",
+	.event = "event=0x05,umask=0x2,period=2000003",
+	.desc = "Speculative cache line split STA uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "Speculative cache-line split store-address uops dispatched to L1D",
+},
+{
+	.name = "tx_mem.abort_conflict",
+	.event = "event=0x54,umask=0x1,period=2000003",
+	.desc = "Number of times a transactional abort was signaled due to a data conflict on a transactionally accessed address",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.abort_capacity_write",
+	.event = "event=0x54,umask=0x2,period=2000003",
+	.desc = "Number of times a transactional abort was signaled due to a data capacity limitation for transactional writes",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.abort_hle_store_to_elided_lock",
+	.event = "event=0x54,umask=0x4,period=2000003",
+	.desc = "Number of times a HLE transactional region aborted due to a non XRELEASE prefixed instruction writing to an elided lock in the elision buffer",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_not_empty",
+	.event = "event=0x54,umask=0x8,period=2000003",
+	.desc = "Number of times an HLE transactional execution aborted due to NoAllocatedElisionBuffer being non-zero",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_mismatch",
+	.event = "event=0x54,umask=0x10,period=2000003",
+	.desc = "Number of times an HLE transactional execution aborted due to XRELEASE lock not satisfying the address and value requirements in the elision buffer",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_unsupported_alignment",
+	.event = "event=0x54,umask=0x20,period=2000003",
+	.desc = "Number of times an HLE transactional execution aborted due to an unsupported read alignment from the elision buffer",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.hle_elision_buffer_full",
+	.event = "event=0x54,umask=0x40,period=2000003",
+	.desc = "Number of times HLE lock could not be elided due to ElisionBufferAvailable being zero",
+	.topic = "memory",
+},
+{
+	.name = "tx_exec.misc1",
+	.event = "event=0x5d,umask=0x1,period=2000003",
+	.desc = "Counts the number of times a class of instructions that may cause a transactional abort was executed. Since this is the count of execution, it may not always cause a transactional abort",
+	.topic = "memory",
+},
+{
+	.name = "tx_exec.misc2",
+	.event = "event=0x5d,umask=0x2,period=2000003",
+	.desc = "Counts the number of times a class of instructions (e.g., vzeroupper) that may cause a transactional abort was executed inside a transactional region",
+	.topic = "memory",
+},
+{
+	.name = "tx_exec.misc3",
+	.event = "event=0x5d,umask=0x4,period=2000003",
+	.desc = "Counts the number of times an instruction execution caused the transactional nest count supported to be exceeded",
+	.topic = "memory",
+},
+{
+	.name = "tx_exec.misc4",
+	.event = "event=0x5d,umask=0x8,period=2000003",
+	.desc = "Counts the number of times a XBEGIN instruction was executed inside an HLE transactional region",
+	.topic = "memory",
+},
+{
+	.name = "tx_exec.misc5",
+	.event = "event=0x5d,umask=0x10,period=2000003",
+	.desc = "Counts the number of times an HLE XACQUIRE instruction was executed inside an RTM transactional region",
+	.topic = "memory",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=100003",
+	.desc = "Counts the number of machine clears due to memory order conflicts",
+	.topic = "memory",
+	.long_desc = "This event counts the number of memory ordering machine clears detected. Memory ordering machine clears can result from memory address aliasing or snoops from another hardware thread or core to data inflight in the pipeline.  Machine clears can have a significant performance impact if they are happening frequently",
+},
+{
+	.name = "hle_retired.start",
+	.event = "event=0xC8,umask=0x1,period=2000003",
+	.desc = "Number of times an HLE execution started",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.commit",
+	.event = "event=0xc8,umask=0x2,period=2000003",
+	.desc = "Number of times an HLE execution successfully committed",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted",
+	.event = "event=0xc8,umask=0x4,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to any reasons (multiple categories may count as one) (Precise event)",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_misc1",
+	.event = "event=0xc8,umask=0x8,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to various memory events (e.g., read/write capacity and conflicts)",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_misc2",
+	.event = "event=0xc8,umask=0x10,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to uncommon conditions",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_misc3",
+	.event = "event=0xc8,umask=0x20,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to HLE-unfriendly instructions",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_misc4",
+	.event = "event=0xc8,umask=0x40,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to incompatible memory type  Spec update: HSD65",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_misc5",
+	.event = "event=0xc8,umask=0x80,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to none of the previous 4 categories (e.g. interrupts)",
+	.topic = "memory",
+	.long_desc = "Number of times an HLE execution aborted due to none of the previous 4 categories (e.g. interrupts)",
+},
+{
+	.name = "rtm_retired.start",
+	.event = "event=0xC9,umask=0x1,period=2000003",
+	.desc = "Number of times an RTM execution started",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.commit",
+	.event = "event=0xc9,umask=0x2,period=2000003",
+	.desc = "Number of times an RTM execution successfully committed",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.aborted",
+	.event = "event=0xc9,umask=0x4,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to any reasons (multiple categories may count as one) (Precise event)",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.aborted_misc1",
+	.event = "event=0xc9,umask=0x8,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to various memory events (e.g. read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times an RTM execution aborted due to various memory events (e.g. read/write capacity and conflicts)",
+},
+{
+	.name = "rtm_retired.aborted_misc2",
+	.event = "event=0xc9,umask=0x10,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to various memory events (e.g., read/write capacity and conflicts)",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.aborted_misc3",
+	.event = "event=0xc9,umask=0x20,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to HLE-unfriendly instructions",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.aborted_misc4",
+	.event = "event=0xc9,umask=0x40,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to incompatible memory type  Spec update: HSD65",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.aborted_misc5",
+	.event = "event=0xc9,umask=0x80,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to none of the previous 4 categories (e.g. interrupt)",
+	.topic = "memory",
+	.long_desc = "Number of times an RTM execution aborted due to none of the previous 4 categories (e.g. interrupt)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_4",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x4",
+	.desc = "Loads with latency value being above 4  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_8",
+	.event = "event=0xCD,umask=0x1,period=50021,ldlat=0x8",
+	.desc = "Loads with latency value being above 8  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_16",
+	.event = "event=0xCD,umask=0x1,period=20011,ldlat=0x10",
+	.desc = "Loads with latency value being above 16  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_32",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x20",
+	.desc = "Loads with latency value being above 32  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_64",
+	.event = "event=0xCD,umask=0x1,period=2003,ldlat=0x40",
+	.desc = "Loads with latency value being above 64  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_128",
+	.event = "event=0xCD,umask=0x1,period=1009,ldlat=0x80",
+	.desc = "Loads with latency value being above 128  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_256",
+	.event = "event=0xCD,umask=0x1,period=503,ldlat=0x100",
+	.desc = "Loads with latency value being above 256  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_512",
+	.event = "event=0xCD,umask=0x1,period=101,ldlat=0x200",
+	.desc = "Loads with latency value being above 512  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_requests.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc08fff",
+	.desc = "Counts all requests that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.l3_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01004007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_code_rd.l3_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400244",
+	.desc = "Counts all demand & prefetch code reads that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_code_rd.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00244",
+	.desc = "Counts all demand & prefetch code reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400122",
+	.desc = "Counts all demand & prefetch RFOs that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00122",
+	.desc = "Counts all demand & prefetch RFOs that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400091",
+	.desc = "Counts all demand & prefetch data reads that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00091",
+	.desc = "Counts all demand & prefetch data reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00200",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00100",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs  that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00080",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00040",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00020",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400004",
+	.desc = "Counts all demand code reads that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00004",
+	.desc = "Counts all demand code reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400002",
+	.desc = "Counts all demand data writes (RFOs) that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00002",
+	.desc = "Counts all demand data writes (RFOs) that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400001",
+	.desc = "Counts demand data reads that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00001",
+	.desc = "Counts demand data reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "dtlb_load_misses.miss_causes_a_walk",
+	.event = "event=0x08,umask=0x1,period=100003",
+	.desc = "Load misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Misses in all TLB levels that cause a page walk of any page size",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_4k",
+	.event = "event=0x08,umask=0x2,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (4K)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to demand load misses that caused 4K page walks in any TLB levels",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_2m_4m",
+	.event = "event=0x08,umask=0x4,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (2M/4M)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to demand load misses that caused 2M/4M page walks in any TLB levels",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_1g",
+	.event = "event=0x08,umask=0x8,period=2000003",
+	.desc = "Load miss in all TLB levels causes a page walk that completes. (1G)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_duration",
+	.event = "event=0x08,umask=0x10,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts cycles when the  page miss handler (PMH) is servicing page walks caused by DTLB load misses",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit_4k",
+	.event = "event=0x08,umask=0x20,period=2000003",
+	.desc = "Load misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load operations from a 4K page that miss the first DTLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit_2m",
+	.event = "event=0x08,umask=0x40,period=2000003",
+	.desc = "Load misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load operations from a 2M page that miss the first DTLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "dtlb_load_misses.pde_cache_miss",
+	.event = "event=0x08,umask=0x80,period=100003",
+	.desc = "DTLB demand load misses with low part of linear-to-physical address translation missed",
+	.topic = "virtual memory",
+	.long_desc = "DTLB demand load misses with low part of linear-to-physical address translation missed",
+},
+{
+	.name = "dtlb_store_misses.miss_causes_a_walk",
+	.event = "event=0x49,umask=0x1,period=100003",
+	.desc = "Store misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Miss in all TLB levels causes a page walk of any page size (4K/2M/4M/1G)",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_4k",
+	.event = "event=0x49,umask=0x2,period=100003",
+	.desc = "Store miss in all TLB levels causes a page walk that completes. (4K)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to store misses in one or more TLB levels of 4K page structure",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_2m_4m",
+	.event = "event=0x49,umask=0x4,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks (2M/4M)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to store misses in one or more TLB levels of 2M/4M page structure",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_1g",
+	.event = "event=0x49,umask=0x8,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks. (1G)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.walk_duration",
+	.event = "event=0x49,umask=0x10,period=100003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts cycles when the  page miss handler (PMH) is servicing page walks caused by DTLB store misses",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit_4k",
+	.event = "event=0x49,umask=0x20,period=100003",
+	.desc = "Store misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store operations from a 4K page that miss the first DTLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit_2m",
+	.event = "event=0x49,umask=0x40,period=100003",
+	.desc = "Store misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store operations from a 2M page that miss the first DTLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "dtlb_store_misses.pde_cache_miss",
+	.event = "event=0x49,umask=0x80,period=100003",
+	.desc = "DTLB store misses with low part of linear-to-physical address translation missed",
+	.topic = "virtual memory",
+	.long_desc = "DTLB store misses with low part of linear-to-physical address translation missed",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4f,umask=0x10,period=2000003",
+	.desc = "Cycle count for an Extended Page table walk",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.miss_causes_a_walk",
+	.event = "event=0x85,umask=0x1,period=100003",
+	.desc = "Misses at all ITLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Misses in ITLB that causes a page walk of any page size",
+},
+{
+	.name = "itlb_misses.walk_completed_4k",
+	.event = "event=0x85,umask=0x2,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (4K)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to misses in ITLB 4K page entries",
+},
+{
+	.name = "itlb_misses.walk_completed_2m_4m",
+	.event = "event=0x85,umask=0x4,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (2M/4M)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to misses in ITLB 2M/4M page entries",
+},
+{
+	.name = "itlb_misses.walk_completed_1g",
+	.event = "event=0x85,umask=0x8,period=100003",
+	.desc = "Store miss in all TLB levels causes a page walk that completes. (1G)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_duration",
+	.event = "event=0x85,umask=0x10,period=100003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts cycles when the  page miss handler (PMH) is servicing page walks caused by ITLB misses",
+},
+{
+	.name = "itlb_misses.stlb_hit_4k",
+	.event = "event=0x85,umask=0x20,period=100003",
+	.desc = "Core misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+	.long_desc = "ITLB misses that hit STLB (4K)",
+},
+{
+	.name = "itlb_misses.stlb_hit_2m",
+	.event = "event=0x85,umask=0x40,period=100003",
+	.desc = "Code misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+	.long_desc = "ITLB misses that hit STLB (2M)",
+},
+{
+	.name = "itlb.itlb_flush",
+	.event = "event=0xae,umask=0x1,period=100003",
+	.desc = "Flushing of the Instruction TLB (ITLB) pages, includes 4k/2M/4M pages",
+	.topic = "virtual memory",
+	.long_desc = "Counts the number of ITLB flushes, includes 4k/2M/4M pages",
+},
+{
+	.name = "page_walker_loads.dtlb_l1",
+	.event = "event=0xBC,umask=0x11,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L1+FB",
+	.topic = "virtual memory",
+	.long_desc = "Number of DTLB page walker loads that hit in the L1+FB",
+},
+{
+	.name = "page_walker_loads.itlb_l1",
+	.event = "event=0xBC,umask=0x21,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L1+FB",
+	.topic = "virtual memory",
+	.long_desc = "Number of ITLB page walker loads that hit in the L1+FB",
+},
+{
+	.name = "page_walker_loads.ept_dtlb_l1",
+	.event = "event=0xBC,umask=0x41,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the DTLB that hit in the L1 and FB",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.ept_itlb_l1",
+	.event = "event=0xBC,umask=0x81,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the ITLB that hit in the L1 and FB",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_l2",
+	.event = "event=0xBC,umask=0x12,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L2",
+	.topic = "virtual memory",
+	.long_desc = "Number of DTLB page walker loads that hit in the L2",
+},
+{
+	.name = "page_walker_loads.itlb_l2",
+	.event = "event=0xBC,umask=0x22,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L2",
+	.topic = "virtual memory",
+	.long_desc = "Number of ITLB page walker loads that hit in the L2",
+},
+{
+	.name = "page_walker_loads.ept_dtlb_l2",
+	.event = "event=0xBC,umask=0x42,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the DTLB that hit in the L2",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.ept_itlb_l2",
+	.event = "event=0xBC,umask=0x82,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the ITLB that hit in the L2",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_l3",
+	.event = "event=0xBC,umask=0x14,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L3 + XSNP  Spec update: HSD25",
+	.topic = "virtual memory",
+	.long_desc = "Number of DTLB page walker loads that hit in the L3  Spec update: HSD25",
+},
+{
+	.name = "page_walker_loads.itlb_l3",
+	.event = "event=0xBC,umask=0x24,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L3 + XSNP  Spec update: HSD25",
+	.topic = "virtual memory",
+	.long_desc = "Number of ITLB page walker loads that hit in the L3  Spec update: HSD25",
+},
+{
+	.name = "page_walker_loads.ept_dtlb_l3",
+	.event = "event=0xBC,umask=0x44,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the DTLB that hit in the L3",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.ept_itlb_l3",
+	.event = "event=0xBC,umask=0x84,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the ITLB that hit in the L2",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_memory",
+	.event = "event=0xBC,umask=0x18,period=2000003",
+	.desc = "Number of DTLB page walker hits in Memory  Spec update: HSD25",
+	.topic = "virtual memory",
+	.long_desc = "Number of DTLB page walker loads from memory  Spec update: HSD25",
+},
+{
+	.name = "page_walker_loads.itlb_memory",
+	.event = "event=0xBC,umask=0x28,period=2000003",
+	.desc = "Number of ITLB page walker hits in Memory  Spec update: HSD25",
+	.topic = "virtual memory",
+	.long_desc = "Number of ITLB page walker loads from memory  Spec update: HSD25",
+},
+{
+	.name = "page_walker_loads.ept_dtlb_memory",
+	.event = "event=0xBC,umask=0x48,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the DTLB that hit in memory",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.ept_itlb_memory",
+	.event = "event=0xBC,umask=0x88,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the ITLB that hit in memory",
+	.topic = "virtual memory",
+},
+{
+	.name = "tlb_flush.dtlb_thread",
+	.event = "event=0xBD,umask=0x1,period=100003",
+	.desc = "DTLB flush attempts of the thread-specific entries",
+	.topic = "virtual memory",
+	.long_desc = "DTLB flush attempts of the thread-specific entries",
+},
+{
+	.name = "tlb_flush.stlb_any",
+	.event = "event=0xBD,umask=0x20,period=100003",
+	.desc = "STLB flush attempts",
+	.topic = "virtual memory",
+	.long_desc = "Count number of STLB flush attempts",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x08,umask=0xe,period=100003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes of any page size",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks in any TLB of any page size due to demand load misses",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x08,umask=0x60,period=2000003",
+	.desc = "Load operations that miss the first DTLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Number of cache load STLB hits. No page walk",
+},
+{
+	.name = "dtlb_store_misses.walk_completed",
+	.event = "event=0x49,umask=0xe,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to store miss in any TLB levels of any page size (4K/2M/4M/1G)",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit",
+	.event = "event=0x49,umask=0x60,period=100003",
+	.desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0xe,period=100003",
+	.desc = "Misses in all ITLB levels that cause completed page walks",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks in ITLB of any page size",
+},
+{
+	.name = "itlb_misses.stlb_hit",
+	.event = "event=0x85,umask=0x60,period=100003",
+	.desc = "Operations that miss the first ITLB level but hit the second and do not cause any page walks",
+	.topic = "virtual memory",
+	.long_desc = "ITLB misses that hit STLB. No page walk",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_miss",
+	.event = "event=0x24,umask=0x21,period=200003",
+	.desc = "Demand Data Read miss L2, no rejects  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Demand data read requests that missed L2, no rejects  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_hit",
+	.event = "event=0x24,umask=0x41,period=200003",
+	.desc = "Demand Data Read requests that hit L2 cache  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Demand data read requests that hit L2 cache  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.l2_pf_miss",
+	.event = "event=0x24,umask=0x30,period=200003",
+	.desc = "L2 prefetch requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests that missed L2",
+},
+{
+	.name = "l2_rqsts.l2_pf_hit",
+	.event = "event=0x24,umask=0x50,period=200003",
+	.desc = "L2 prefetch requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests that hit L2",
+},
+{
+	.name = "l2_rqsts.all_demand_data_rd",
+	.event = "event=0x24,umask=0xe1,period=200003",
+	.desc = "Demand Data Read requests  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Counts any demand and L1 HW prefetch data load requests to L2  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.all_rfo",
+	.event = "event=0x24,umask=0xe2,period=200003",
+	.desc = "RFO requests to L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 store RFO requests",
+},
+{
+	.name = "l2_rqsts.all_code_rd",
+	.event = "event=0x24,umask=0xe4,period=200003",
+	.desc = "L2 code requests",
+	.topic = "cache",
+	.long_desc = "Counts all L2 code requests",
+},
+{
+	.name = "l2_rqsts.all_pf",
+	.event = "event=0x24,umask=0xf8,period=200003",
+	.desc = "Requests from L2 hardware prefetchers",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests",
+},
+{
+	.name = "l2_demand_rqsts.wb_hit",
+	.event = "event=0x27,umask=0x50,period=200003",
+	.desc = "Not rejected writebacks that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "Not rejected writebacks that hit L2 cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100003",
+	.desc = "Core-originated cacheable demand requests missed L3",
+	.topic = "cache",
+	.long_desc = "This event counts each cache miss condition for references to the last level cache",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=100003",
+	.desc = "Core-originated cacheable demand requests that refer to L3",
+	.topic = "cache",
+	.long_desc = "This event counts requests originating from the core that reference a cache line in the last level cache",
+},
+{
+	.name = "l1d_pend_miss.pending",
+	.event = "event=0x48,umask=0x1,period=2000003",
+	.desc = "L1D miss oustandings duration in cycles",
+	.topic = "cache",
+	.long_desc = "Increments the number of outstanding L1D misses every cycle. Set Cmask = 1 and Edge =1 to count occurrences",
+},
+{
+	.name = "l1d_pend_miss.request_fb_full",
+	.event = "event=0x48,umask=0x2,period=2000003",
+	.desc = "Number of times a request needed a FB entry but there was no entry available for it. That is the FB unavailability was dominant reason for blocking the request. A request includes cacheable/uncacheable demands that is load, store or SW prefetch. HWP are e",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles",
+	.event = "event=0x48,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding",
+	.topic = "cache",
+},
+{
+	.name = "l1d.replacement",
+	.event = "event=0x51,umask=0x1,period=2000003",
+	.desc = "L1D data line replacements",
+	.topic = "cache",
+	.long_desc = "This event counts when new data lines are brought into the L1 Data cache, which cause other lines to be evicted from the cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003",
+	.desc = "Offcore outstanding Demand Data Read transactions in uncore queue  Spec update: HSD78, HSD62, HSD61",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding demand data read transactions in SQ to uncore. Set Cmask=1 to count cycles  Spec update: HSD78, HSD62, HSD61",
+},
+{
+	.name = "offcore_requests_outstanding.demand_code_rd",
+	.event = "event=0x60,umask=0x2,period=2000003",
+	.desc = "Offcore outstanding code reads transactions in SuperQueue (SQ), queue to uncore, every cycle  Spec update: HSD62, HSD61",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding Demand code Read transactions in SQ to uncore. Set Cmask=1 to count cycles  Spec update: HSD62, HSD61",
+},
+{
+	.name = "offcore_requests_outstanding.demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003",
+	.desc = "Offcore outstanding RFO store transactions in SuperQueue (SQ), queue to uncore  Spec update: HSD62, HSD61",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding RFO store transactions in SQ to uncore. Set Cmask=1 to count cycles  Spec update: HSD62, HSD61",
+},
+{
+	.name = "offcore_requests_outstanding.all_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003",
+	.desc = "Offcore outstanding cacheable Core Data Read transactions in SuperQueue (SQ), queue to uncore  Spec update: HSD62, HSD61",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding cacheable data read transactions in SQ to uncore. Set Cmask=1 to count cycles  Spec update: HSD62, HSD61",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore  Spec update: HSD78, HSD62, HSD61",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore  Spec update: HSD62, HSD61",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003,cmask=1",
+	.desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle  Spec update: HSD62, HSD61",
+	.topic = "cache",
+},
+{
+	.name = "lock_cycles.cache_lock_duration",
+	.event = "event=0x63,umask=0x2,period=2000003",
+	.desc = "Cycles when L1D is locked",
+	.topic = "cache",
+	.long_desc = "Cycles in which the L1D is locked",
+},
+{
+	.name = "offcore_requests.demand_data_rd",
+	.event = "event=0xB0,umask=0x1,period=100003",
+	.desc = "Demand Data Read requests sent to uncore  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Demand data read requests sent to uncore  Spec update: HSD78",
+},
+{
+	.name = "offcore_requests.demand_code_rd",
+	.event = "event=0xB0,umask=0x2,period=100003",
+	.desc = "Cacheable and noncachaeble code read requests",
+	.topic = "cache",
+	.long_desc = "Demand code read requests sent to uncore",
+},
+{
+	.name = "offcore_requests.demand_rfo",
+	.event = "event=0xB0,umask=0x4,period=100003",
+	.desc = "Demand RFO requests including regular RFOs, locks, ItoM",
+	.topic = "cache",
+	.long_desc = "Demand RFO read requests sent to uncore, including regular RFOs, locks, ItoM",
+},
+{
+	.name = "offcore_requests.all_data_rd",
+	.event = "event=0xB0,umask=0x8,period=100003",
+	.desc = "Demand and prefetch data reads",
+	.topic = "cache",
+	.long_desc = "Data read requests sent to uncore (demand and prefetch)",
+},
+{
+	.name = "offcore_requests_buffer.sq_full",
+	.event = "event=0xb2,umask=0x1,period=2000003",
+	.desc = "Offcore requests buffer cannot take more entries for this thread core",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_loads",
+	.event = "event=0xD0,umask=0x11,period=100003",
+	.desc = "Retired load uops that miss the STLB  Spec update: HSD29, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_stores",
+	.event = "event=0xD0,umask=0x12,period=100003",
+	.desc = "Retired store uops that miss the STLB  Spec update: HSD29, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.lock_loads",
+	.event = "event=0xD0,umask=0x21,period=100003",
+	.desc = "Retired load uops with locked access  Spec update: HSD76, HSD29, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.split_loads",
+	.event = "event=0xD0,umask=0x41,period=100003",
+	.desc = "Retired load uops that split across a cacheline boundary  Spec update: HSD29, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.split_stores",
+	.event = "event=0xD0,umask=0x42,period=100003",
+	.desc = "Retired store uops that split across a cacheline boundary  Spec update: HSD29, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0xD0,umask=0x81,period=2000003",
+	.desc = "All retired load uops  Spec update: HSD29, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0xD0,umask=0x82,period=2000003",
+	.desc = "All retired store uops  Spec update: HSD29, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.l1_hit",
+	.event = "event=0xD1,umask=0x1,period=2000003",
+	.desc = "Retired load uops with L1 cache hits as data sources  Spec update: HSD29, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.l2_hit",
+	.event = "event=0xD1,umask=0x2,period=100003",
+	.desc = "Retired load uops with L2 cache hits as data sources  Spec update: HSD76, HSD29, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.l3_hit",
+	.event = "event=0xD1,umask=0x4,period=50021",
+	.desc = "Retired load uops which data sources were data hits in L3 without snoops required  Spec update: HSD74, HSD29, HSD25, HSM26, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops with L3 cache hits as data sources  Spec update: HSD74, HSD29, HSD25, HSM26, HSM30.  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_miss",
+	.event = "event=0xD1,umask=0x8,period=100003",
+	.desc = "Retired load uops misses in L1 cache as data sources  Spec update: HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops missed L1 cache as data sources  Spec update: HSM30.  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_miss",
+	.event = "event=0xD1,umask=0x10,period=50021",
+	.desc = "Miss in mid-level (L2) cache. Excludes Unknown data-source  Spec update: HSD29, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops missed L2. Unknown data source excluded  Spec update: HSD29, HSM30.  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l3_miss",
+	.event = "event=0xD1,umask=0x20,period=100003",
+	.desc = "Miss in last-level (L3) cache. Excludes Unknown data-source  Spec update: HSD74, HSD29, HSD25, HSM26, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops missed L3. Excludes unknown data source   Spec update: HSD74, HSD29, HSD25, HSM26, HSM30.  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.hit_lfb",
+	.event = "event=0xD1,umask=0x40,period=100003",
+	.desc = "Retired load uops which data sources were load uops missed L1 but hit FB due to preceding miss to the same cache line with data not ready  Spec update: HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_miss",
+	.event = "event=0xD2,umask=0x1,period=20011",
+	.desc = "Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache  Spec update: HSD29, HSD25, HSM26, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_hit",
+	.event = "event=0xD2,umask=0x2,period=20011",
+	.desc = "Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache  Spec update: HSD29, HSD25, HSM26, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_hitm",
+	.event = "event=0xD2,umask=0x4,period=20011",
+	.desc = "Retired load uops which data sources were HitM responses from shared L3  Spec update: HSD29, HSD25, HSM26, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_none",
+	.event = "event=0xD2,umask=0x8,period=100003",
+	.desc = "Retired load uops which data sources were hits in L3 without snoops required  Spec update: HSD74, HSD29, HSD25, HSM26, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_miss_retired.local_dram",
+	.event = "event=0xD3,umask=0x1,period=100003",
+	.desc = "Data from local DRAM either Snoop not needed or Snoop Miss (RspI)  Spec update: HSD74, HSD29, HSD25, HSM30.  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts retired load uops where the data came from local DRAM. This does not include hardware prefetches  Spec update: HSD74, HSD29, HSD25, HSM30.  Supports address when precise (Precise event)",
+},
+{
+	.name = "l2_trans.demand_data_rd",
+	.event = "event=0xf0,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "Demand data read requests that access L2 cache",
+},
+{
+	.name = "l2_trans.rfo",
+	.event = "event=0xf0,umask=0x2,period=200003",
+	.desc = "RFO requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "RFO requests that access L2 cache",
+},
+{
+	.name = "l2_trans.code_rd",
+	.event = "event=0xf0,umask=0x4,period=200003",
+	.desc = "L2 cache accesses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "L2 cache accesses when fetching instructions",
+},
+{
+	.name = "l2_trans.all_pf",
+	.event = "event=0xf0,umask=0x8,period=200003",
+	.desc = "L2 or L3 HW prefetches that access L2 cache",
+	.topic = "cache",
+	.long_desc = "Any MLC or L3 HW prefetch accessing L2, including rejects",
+},
+{
+	.name = "l2_trans.l1d_wb",
+	.event = "event=0xf0,umask=0x10,period=200003",
+	.desc = "L1D writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L1D writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_fill",
+	.event = "event=0xf0,umask=0x20,period=200003",
+	.desc = "L2 fill requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L2 fill requests that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_wb",
+	.event = "event=0xf0,umask=0x40,period=200003",
+	.desc = "L2 writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L2 writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.all_requests",
+	.event = "event=0xf0,umask=0x80,period=200003",
+	.desc = "Transactions accessing L2 pipe",
+	.topic = "cache",
+	.long_desc = "Transactions accessing L2 pipe",
+},
+{
+	.name = "l2_lines_in.i",
+	.event = "event=0xF1,umask=0x1,period=100003",
+	.desc = "L2 cache lines in I state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in I state filling L2",
+},
+{
+	.name = "l2_lines_in.s",
+	.event = "event=0xF1,umask=0x2,period=100003",
+	.desc = "L2 cache lines in S state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in S state filling L2",
+},
+{
+	.name = "l2_lines_in.e",
+	.event = "event=0xF1,umask=0x4,period=100003",
+	.desc = "L2 cache lines in E state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in E state filling L2",
+},
+{
+	.name = "l2_lines_in.all",
+	.event = "event=0xF1,umask=0x7,period=100003",
+	.desc = "L2 cache lines filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines brought into the L2 cache.  Lines are filled into the L2 cache when there was an L2 miss",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x5,period=100003",
+	.desc = "Clean L2 cache lines evicted by demand",
+	.topic = "cache",
+	.long_desc = "Clean L2 cache lines evicted by demand",
+},
+{
+	.name = "l2_lines_out.demand_dirty",
+	.event = "event=0xF2,umask=0x6,period=100003",
+	.desc = "Dirty L2 cache lines evicted by demand",
+	.topic = "cache",
+	.long_desc = "Dirty L2 cache lines evicted by demand",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xf4,umask=0x10,period=100003",
+	.desc = "Split locks in SQ",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x42,period=200003",
+	.desc = "RFO requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts the number of store RFO requests that hit the L2 cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x22,period=200003",
+	.desc = "RFO requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts the number of store RFO requests that miss the L2 cache",
+},
+{
+	.name = "l2_rqsts.code_rd_hit",
+	.event = "event=0x24,umask=0x44,period=200003",
+	.desc = "L2 cache hits when fetching instructions, code reads",
+	.topic = "cache",
+	.long_desc = "Number of instruction fetches that hit the L2 cache",
+},
+{
+	.name = "l2_rqsts.code_rd_miss",
+	.event = "event=0x24,umask=0x24,period=200003",
+	.desc = "L2 cache misses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "Number of instruction fetches that missed the L2 cache",
+},
+{
+	.name = "l2_rqsts.all_demand_miss",
+	.event = "event=0x24,umask=0x27,period=200003",
+	.desc = "Demand requests that miss L2 cache  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Demand requests that miss L2 cache  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.all_demand_references",
+	.event = "event=0x24,umask=0xe7,period=200003",
+	.desc = "Demand requests to L2 cache  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Demand requests to L2 cache  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.miss",
+	.event = "event=0x24,umask=0x3f,period=200003",
+	.desc = "All requests that miss L2 cache  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "All requests that missed L2  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.references",
+	.event = "event=0x24,umask=0xff,period=200003",
+	.desc = "All L2 requests  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "All requests to L2 cache  Spec update: HSD78",
+},
+{
+	.name = "offcore_response",
+	.event = "event=0xB7,umask=0x1,period=100003",
+	.desc = "Offcore response can be programmed only with a specific pair of event select and counter MSR, and with specific event codes and predefine mask bit value in a dedicated MSR to specify attributes of the offcore transaction",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd_ge_6",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=6",
+	.desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue  Spec update: HSD78, HSD62, HSD61",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles_any",
+	.event = "event=0x48,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.fb_full",
+	.event = "event=0x48,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles a demand request was blocked due to Fill Buffers inavailability",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_requests.l3_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c8fff",
+	.desc = "Counts all requests that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.l3_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c07f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.l3_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c07f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_code_rd.l3_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0244",
+	.desc = "Counts all demand & prefetch code reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.l3_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0122",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.l3_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0122",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0091",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.l3_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0091",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_code_rd.l3_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0200",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0100",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs  that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0080",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l3_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0040",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l3_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0020",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l3_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0004",
+	.desc = "Counts all demand code reads that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0004",
+	.desc = "Counts all demand code reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0002",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0002",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0001",
+	.desc = "Counts demand data reads that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0001",
+	.desc = "Counts demand data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_westmereex[] = {
+{
+	.name = "fp_assist.all",
+	.event = "event=0xF7,umask=0x1,period=20000",
+	.desc = "X87 Floating point assists (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.input",
+	.event = "event=0xF7,umask=0x4,period=20000",
+	.desc = "X87 Floating poiint assists for invalid input value (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.output",
+	.event = "event=0xF7,umask=0x2,period=20000",
+	.desc = "X87 Floating point assists for invalid output value (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.mmx",
+	.event = "event=0x10,umask=0x2,period=2000000",
+	.desc = "MMX Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_double_precision",
+	.event = "event=0x10,umask=0x80,period=2000000",
+	.desc = "SSE* FP double precision Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp",
+	.event = "event=0x10,umask=0x4,period=2000000",
+	.desc = "SSE and SSE2 FP Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp_packed",
+	.event = "event=0x10,umask=0x10,period=2000000",
+	.desc = "SSE FP packed Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp_scalar",
+	.event = "event=0x10,umask=0x20,period=2000000",
+	.desc = "SSE FP scalar Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_single_precision",
+	.event = "event=0x10,umask=0x40,period=2000000",
+	.desc = "SSE* FP single precision Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse2_integer",
+	.event = "event=0x10,umask=0x8,period=2000000",
+	.desc = "SSE2 integer Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.x87",
+	.event = "event=0x10,umask=0x1,period=2000000",
+	.desc = "Computational floating-point operations executed",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.any",
+	.event = "event=0xCC,umask=0x3,period=2000000",
+	.desc = "All Floating Point to and from MMX transitions",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.to_fp",
+	.event = "event=0xCC,umask=0x1,period=2000000",
+	.desc = "Transitions from MMX to Floating Point instructions",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.to_mmx",
+	.event = "event=0xCC,umask=0x2,period=2000000",
+	.desc = "Transitions from Floating Point to MMX instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.pack",
+	.event = "event=0x12,umask=0x4,period=200000",
+	.desc = "128 bit SIMD integer pack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_arith",
+	.event = "event=0x12,umask=0x20,period=200000",
+	.desc = "128 bit SIMD integer arithmetic operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_logical",
+	.event = "event=0x12,umask=0x10,period=200000",
+	.desc = "128 bit SIMD integer logical operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_mpy",
+	.event = "event=0x12,umask=0x1,period=200000",
+	.desc = "128 bit SIMD integer multiply operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_shift",
+	.event = "event=0x12,umask=0x2,period=200000",
+	.desc = "128 bit SIMD integer shift operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.shuffle_move",
+	.event = "event=0x12,umask=0x40,period=200000",
+	.desc = "128 bit SIMD integer shuffle/move operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.unpack",
+	.event = "event=0x12,umask=0x8,period=200000",
+	.desc = "128 bit SIMD integer unpack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.pack",
+	.event = "event=0xFD,umask=0x4,period=200000",
+	.desc = "SIMD integer 64 bit pack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_arith",
+	.event = "event=0xFD,umask=0x20,period=200000",
+	.desc = "SIMD integer 64 bit arithmetic operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_logical",
+	.event = "event=0xFD,umask=0x10,period=200000",
+	.desc = "SIMD integer 64 bit logical operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_mpy",
+	.event = "event=0xFD,umask=0x1,period=200000",
+	.desc = "SIMD integer 64 bit packed multiply operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_shift",
+	.event = "event=0xFD,umask=0x2,period=200000",
+	.desc = "SIMD integer 64 bit shift operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.shuffle_move",
+	.event = "event=0xFD,umask=0x40,period=200000",
+	.desc = "SIMD integer 64 bit shuffle/move operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.unpack",
+	.event = "event=0xFD,umask=0x8,period=200000",
+	.desc = "SIMD integer 64 bit unpack operations",
+	.topic = "floating point",
+},
+{
+	.name = "arith.cycles_div_busy",
+	.event = "event=0x14,umask=0x1,period=2000000",
+	.desc = "Cycles the divider is busy",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.div",
+	.event = "event=0x14,inv=1,umask=0x1,period=2000000,cmask=1,edge=1",
+	.desc = "Divide Operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.mul",
+	.event = "event=0x14,umask=0x2,period=2000000",
+	.desc = "Multiply operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear.bad_target",
+	.event = "event=0xE6,umask=0x2,period=2000000",
+	.desc = "BACLEAR asserted with bad target address",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear.clear",
+	.event = "event=0xE6,umask=0x1,period=2000000",
+	.desc = "BACLEAR asserted, regardless of cause ",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear_force_iq",
+	.event = "event=0xA7,umask=0x1,period=2000000",
+	.desc = "Instruction queue forced BACLEAR",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_decoded",
+	.event = "event=0xE0,umask=0x1,period=2000000",
+	.desc = "Branch instructions decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.any",
+	.event = "event=0x88,umask=0x7f,period=200000",
+	.desc = "Branch instructions executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.cond",
+	.event = "event=0x88,umask=0x1,period=200000",
+	.desc = "Conditional branch instructions executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.direct",
+	.event = "event=0x88,umask=0x2,period=200000",
+	.desc = "Unconditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.direct_near_call",
+	.event = "event=0x88,umask=0x10,period=20000",
+	.desc = "Unconditional call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.indirect_near_call",
+	.event = "event=0x88,umask=0x20,period=20000",
+	.desc = "Indirect call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.indirect_non_call",
+	.event = "event=0x88,umask=0x4,period=20000",
+	.desc = "Indirect non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.near_calls",
+	.event = "event=0x88,umask=0x30,period=20000",
+	.desc = "Call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.non_calls",
+	.event = "event=0x88,umask=0x7,period=200000",
+	.desc = "All non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.return_near",
+	.event = "event=0x88,umask=0x8,period=20000",
+	.desc = "Indirect return branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken",
+	.event = "event=0x88,umask=0x40,period=200000",
+	.desc = "Taken branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x4,period=200000",
+	.desc = "Retired branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=200000",
+	.desc = "Retired conditional branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=20000",
+	.desc = "Retired near call instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.any",
+	.event = "event=0x89,umask=0x7f,period=20000",
+	.desc = "Mispredicted branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.cond",
+	.event = "event=0x89,umask=0x1,period=20000",
+	.desc = "Mispredicted conditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.direct",
+	.event = "event=0x89,umask=0x2,period=20000",
+	.desc = "Mispredicted unconditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.direct_near_call",
+	.event = "event=0x89,umask=0x10,period=2000",
+	.desc = "Mispredicted non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.indirect_near_call",
+	.event = "event=0x89,umask=0x20,period=2000",
+	.desc = "Mispredicted indirect call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.indirect_non_call",
+	.event = "event=0x89,umask=0x4,period=2000",
+	.desc = "Mispredicted indirect non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.near_calls",
+	.event = "event=0x89,umask=0x30,period=2000",
+	.desc = "Mispredicted call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.non_calls",
+	.event = "event=0x89,umask=0x7,period=20000",
+	.desc = "Mispredicted non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.return_near",
+	.event = "event=0x89,umask=0x8,period=2000",
+	.desc = "Mispredicted return branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken",
+	.event = "event=0x89,umask=0x40,period=20000",
+	.desc = "Mispredicted taken branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x4,period=20000",
+	.desc = "Mispredicted retired branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=20000",
+	.desc = "Mispredicted conditional retired branches (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_call",
+	.event = "event=0xC5,umask=0x2,period=2000",
+	.desc = "Mispredicted near retired calls (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref",
+	.event = "event=0x0,umask=0x03",
+	.desc = "Reference cycles when thread is not halted (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_p",
+	.event = "event=0x3C,umask=0x1,period=100000",
+	.desc = "Reference base clock (133 Mhz) cycles when thread is not halted (programmable counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Cycles when thread is not halted (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000000",
+	.desc = "Cycles when thread is not halted (programmable counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.total_cycles",
+	.event = "event=0x3C,inv=1,umask=0x0,period=2000000,cmask=2",
+	.desc = "Total CPU cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.any",
+	.event = "event=0x87,umask=0xf,period=2000000",
+	.desc = "Any Instruction Length Decoder stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.iq_full",
+	.event = "event=0x87,umask=0x4,period=2000000",
+	.desc = "Instruction Queue full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000000",
+	.desc = "Length Change Prefix stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.mru",
+	.event = "event=0x87,umask=0x2,period=2000000",
+	.desc = "Stall cycles due to BPU MRU bypass",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.regen",
+	.event = "event=0x87,umask=0x8,period=2000000",
+	.desc = "Regen stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_decoded.dec0",
+	.event = "event=0x18,umask=0x1,period=2000000",
+	.desc = "Instructions that must be decoded by decoder 0",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_queue_write_cycles",
+	.event = "event=0x1E,umask=0x1,period=2000000",
+	.desc = "Cycles instructions are written to the instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_queue_writes",
+	.event = "event=0x17,umask=0x1,period=2000000",
+	.desc = "Instructions written to instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (Programmable counter and Precise Event) (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.mmx",
+	.event = "event=0xC0,umask=0x4,period=2000000",
+	.desc = "Retired MMX instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.total_cycles",
+	.event = "event=0xC0,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.x87",
+	.event = "event=0xC0,umask=0x2,period=2000000",
+	.desc = "Retired floating-point operations (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre",
+	.event = "event=0x4C,umask=0x1,period=200000",
+	.desc = "Load operations conflicting with software prefetches",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.active",
+	.event = "event=0xA8,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles when uops were delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.inactive",
+	.event = "event=0xA8,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no uops were delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd_overflow",
+	.event = "event=0x20,umask=0x1,period=2000000",
+	.desc = "Loops that can't stream from the instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.cycles",
+	.event = "event=0xC3,umask=0x1,period=20000",
+	.desc = "Cycles machine clear asserted",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.mem_order",
+	.event = "event=0xC3,umask=0x2,period=20000",
+	.desc = "Execution pipeline restart due to Memory ordering conflicts",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=20000",
+	.desc = "Self-Modifying Code detected",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000000",
+	.desc = "Resource related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.fpcw",
+	.event = "event=0xA2,umask=0x20,period=2000000",
+	.desc = "FPU control word write stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.load",
+	.event = "event=0xA2,umask=0x2,period=2000000",
+	.desc = "Load buffer stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.mxcsr",
+	.event = "event=0xA2,umask=0x40,period=2000000",
+	.desc = "MXCSR rename stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.other",
+	.event = "event=0xA2,umask=0x80,period=2000000",
+	.desc = "Other Resource related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rob_full",
+	.event = "event=0xA2,umask=0x10,period=2000000",
+	.desc = "ROB full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rs_full",
+	.event = "event=0xA2,umask=0x4,period=2000000",
+	.desc = "Reservation Station full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.store",
+	.event = "event=0xA2,umask=0x8,period=2000000",
+	.desc = "Store buffer stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.packed_double",
+	.event = "event=0xC7,umask=0x4,period=200000",
+	.desc = "SIMD Packed-Double Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.packed_single",
+	.event = "event=0xC7,umask=0x1,period=200000",
+	.desc = "SIMD Packed-Single Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.scalar_double",
+	.event = "event=0xC7,umask=0x8,period=200000",
+	.desc = "SIMD Scalar-Double Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.scalar_single",
+	.event = "event=0xC7,umask=0x2,period=200000",
+	.desc = "SIMD Scalar-Single Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.vector_integer",
+	.event = "event=0xC7,umask=0x10,period=200000",
+	.desc = "SIMD Vector Integer Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000000",
+	.desc = "Cycles thread is active",
+	.topic = "pipeline",
+},
+{
+	.name = "uop_unfusion",
+	.event = "event=0xDB,umask=0x1,period=2000000",
+	.desc = "Uop unfusions due to FP exceptions",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.esp_folding",
+	.event = "event=0xD1,umask=0x4,period=2000000",
+	.desc = "Stack pointer instructions decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.esp_sync",
+	.event = "event=0xD1,umask=0x8,period=2000000",
+	.desc = "Stack pointer sync operations",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.ms_cycles_active",
+	.event = "event=0xD1,umask=0x2,period=2000000,cmask=1",
+	.desc = "Uops decoded by Microcode Sequencer",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.stall_cycles",
+	.event = "event=0xD1,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops are decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_active_cycles",
+	.event = "event=0xB1,umask=0x3f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops executed on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_active_cycles_no_port5",
+	.event = "event=0xB1,umask=0x1f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops executed on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_count",
+	.event = "event=0xB1,inv=1,umask=0x3f,period=2000000,cmask=1,edge=1",
+	.desc = "Uops executed on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_count_no_port5",
+	.event = "event=0xB1,inv=1,umask=0x1f,period=2000000,cmask=1,edge=1",
+	.desc = "Uops executed on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x3f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_cycles_no_port5",
+	.event = "event=0xB1,inv=1,umask=0x1f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port0",
+	.event = "event=0xB1,umask=0x1,period=2000000",
+	.desc = "Uops executed on port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port015",
+	.event = "event=0xB1,umask=0x40,period=2000000",
+	.desc = "Uops issued on ports 0, 1 or 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port015_stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x40,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on ports 0, 1 or 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port1",
+	.event = "event=0xB1,umask=0x2,period=2000000",
+	.desc = "Uops executed on port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port2_core",
+	.event = "event=0xB1,umask=0x4,any=1,period=2000000",
+	.desc = "Uops executed on port 2 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port234_core",
+	.event = "event=0xB1,umask=0x80,any=1,period=2000000",
+	.desc = "Uops issued on ports 2, 3 or 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port3_core",
+	.event = "event=0xB1,umask=0x8,any=1,period=2000000",
+	.desc = "Uops executed on port 3 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port4_core",
+	.event = "event=0xB1,umask=0x10,any=1,period=2000000",
+	.desc = "Uops executed on port 4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port5",
+	.event = "event=0xB1,umask=0x20,period=2000000",
+	.desc = "Uops executed on port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0xE,umask=0x1,period=2000000",
+	.desc = "Uops issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.core_stall_cycles",
+	.event = "event=0xE,inv=1,umask=0x1,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops were issued on any thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.cycles_all_threads",
+	.event = "event=0xE,umask=0x1,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops were issued on either thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.fused",
+	.event = "event=0xE,umask=0x2,period=2000000",
+	.desc = "Fused Uops issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0xE,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops were issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.active_cycles",
+	.event = "event=0xC2,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles Uops are being retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.any",
+	.event = "event=0xC2,umask=0x1,period=2000000",
+	.desc = "Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.macro_fused",
+	.event = "event=0xC2,umask=0x4,period=2000000",
+	.desc = "Macro-fused Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000000",
+	.desc = "Retirement slots used (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles Uops are not retiring (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles using precise uop retired event (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.total_cycles_ps",
+	.event = "event=0xC0,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "macro_insts.decoded",
+	.event = "event=0xD0,umask=0x1,period=2000000",
+	.desc = "Instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "macro_insts.fusions_decoded",
+	.event = "event=0xA6,umask=0x1,period=2000000",
+	.desc = "Macro-fused instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "two_uop_insts_decoded",
+	.event = "event=0x19,umask=0x1,period=2000000",
+	.desc = "Two Uop instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "bpu_clears.early",
+	.event = "event=0xE8,umask=0x1,period=2000000",
+	.desc = "Early Branch Prediciton Unit clears",
+	.topic = "other",
+},
+{
+	.name = "bpu_clears.late",
+	.event = "event=0xE8,umask=0x2,period=2000000",
+	.desc = "Late Branch Prediction Unit clears",
+	.topic = "other",
+},
+{
+	.name = "bpu_missed_call_ret",
+	.event = "event=0xE5,umask=0x1,period=2000000",
+	.desc = "Branch prediction unit missed call or return",
+	.topic = "other",
+},
+{
+	.name = "es_reg_renames",
+	.event = "event=0xD5,umask=0x1,period=2000000",
+	.desc = "ES segment renames",
+	.topic = "other",
+},
+{
+	.name = "io_transactions",
+	.event = "event=0x6C,umask=0x1,period=2000000",
+	.desc = "I/O transactions",
+	.topic = "other",
+},
+{
+	.name = "l1i.cycles_stalled",
+	.event = "event=0x80,umask=0x4,period=2000000",
+	.desc = "L1I instruction fetch stall cycles",
+	.topic = "other",
+},
+{
+	.name = "l1i.hits",
+	.event = "event=0x80,umask=0x1,period=2000000",
+	.desc = "L1I instruction fetch hits",
+	.topic = "other",
+},
+{
+	.name = "l1i.misses",
+	.event = "event=0x80,umask=0x2,period=2000000",
+	.desc = "L1I instruction fetch misses",
+	.topic = "other",
+},
+{
+	.name = "l1i.reads",
+	.event = "event=0x80,umask=0x3,period=2000000",
+	.desc = "L1I Instruction fetches",
+	.topic = "other",
+},
+{
+	.name = "large_itlb.hit",
+	.event = "event=0x82,umask=0x1,period=200000",
+	.desc = "Large ITLB hit",
+	.topic = "other",
+},
+{
+	.name = "load_block.overlap_store",
+	.event = "event=0x3,umask=0x2,period=200000",
+	.desc = "Loads that partially overlap an earlier store",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.any",
+	.event = "event=0x13,umask=0x7,period=2000000",
+	.desc = "All loads dispatched",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.mob",
+	.event = "event=0x13,umask=0x4,period=2000000",
+	.desc = "Loads dispatched from the MOB",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.rs",
+	.event = "event=0x13,umask=0x1,period=2000000",
+	.desc = "Loads dispatched that bypass the MOB",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.rs_delayed",
+	.event = "event=0x13,umask=0x2,period=2000000",
+	.desc = "Loads dispatched from stage 305",
+	.topic = "other",
+},
+{
+	.name = "partial_address_alias",
+	.event = "event=0x7,umask=0x1,period=200000",
+	.desc = "False dependencies due to partial address aliasing",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.any",
+	.event = "event=0xD2,umask=0xf,period=2000000",
+	.desc = "All RAT stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.flags",
+	.event = "event=0xD2,umask=0x1,period=2000000",
+	.desc = "Flag stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.registers",
+	.event = "event=0xD2,umask=0x2,period=2000000",
+	.desc = "Partial register stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.rob_read_port",
+	.event = "event=0xD2,umask=0x4,period=2000000",
+	.desc = "ROB read port stalls cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.scoreboard",
+	.event = "event=0xD2,umask=0x8,period=2000000",
+	.desc = "Scoreboard stall cycles",
+	.topic = "other",
+},
+{
+	.name = "sb_drain.any",
+	.event = "event=0x4,umask=0x7,period=200000",
+	.desc = "All Store buffer stall cycles",
+	.topic = "other",
+},
+{
+	.name = "seg_rename_stalls",
+	.event = "event=0xD4,umask=0x1,period=2000000",
+	.desc = "Segment rename stall cycles",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hit",
+	.event = "event=0xB8,umask=0x1,period=100000",
+	.desc = "Thread responded HIT to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hite",
+	.event = "event=0xB8,umask=0x2,period=100000",
+	.desc = "Thread responded HITE to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hitm",
+	.event = "event=0xB8,umask=0x4,period=100000",
+	.desc = "Thread responded HITM to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests.code",
+	.event = "event=0xB4,umask=0x4,period=100000",
+	.desc = "Snoop code requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests.data",
+	.event = "event=0xB4,umask=0x1,period=100000",
+	.desc = "Snoop data requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests.invalidate",
+	.event = "event=0xB4,umask=0x2,period=100000",
+	.desc = "Snoop invalidate requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.code",
+	.event = "event=0xB3,umask=0x4,period=2000000",
+	.desc = "Outstanding snoop code requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.code_not_empty",
+	.event = "event=0xB3,umask=0x4,period=2000000,cmask=1",
+	.desc = "Cycles snoop code requests queued",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.data",
+	.event = "event=0xB3,umask=0x1,period=2000000",
+	.desc = "Outstanding snoop data requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.data_not_empty",
+	.event = "event=0xB3,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles snoop data requests queued",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.invalidate",
+	.event = "event=0xB3,umask=0x2,period=2000000",
+	.desc = "Outstanding snoop invalidate requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.invalidate_not_empty",
+	.event = "event=0xB3,umask=0x2,period=2000000,cmask=1",
+	.desc = "Cycles snoop invalidate requests queued",
+	.topic = "other",
+},
+{
+	.name = "sq_full_stall_cycles",
+	.event = "event=0xF6,umask=0x1,period=2000000",
+	.desc = "Super Queue full stall cycles",
+	.topic = "other",
+},
+{
+	.name = "misalign_mem_ref.store",
+	.event = "event=0x5,umask=0x2,period=200000",
+	.desc = "Misaligned store references",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6011",
+	.desc = "Offcore data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF811",
+	.desc = "Offcore data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4011",
+	.desc = "Offcore data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2011",
+	.desc = "Offcore data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6044",
+	.desc = "Offcore code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF844",
+	.desc = "Offcore code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4044",
+	.desc = "Offcore code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2044",
+	.desc = "Offcore code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x60FF",
+	.desc = "Offcore requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF8FF",
+	.desc = "Offcore requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x40FF",
+	.desc = "Offcore requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x20FF",
+	.desc = "Offcore requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6022",
+	.desc = "Offcore RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF822",
+	.desc = "Offcore RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4022",
+	.desc = "Offcore RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2022",
+	.desc = "Offcore RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6008",
+	.desc = "Offcore writebacks to any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF808",
+	.desc = "Offcore writebacks that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4008",
+	.desc = "Offcore writebacks to the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2008",
+	.desc = "Offcore writebacks to a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6077",
+	.desc = "Offcore code or data read requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF877",
+	.desc = "Offcore code or data read requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4077",
+	.desc = "Offcore code or data read requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2077",
+	.desc = "Offcore code or data read requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6033",
+	.desc = "Offcore request = all data, response = any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF833",
+	.desc = "Offcore request = all data, response = any LLC miss",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4033",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2033",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6003",
+	.desc = "Offcore demand data requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF803",
+	.desc = "Offcore demand data requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4003",
+	.desc = "Offcore demand data requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2003",
+	.desc = "Offcore demand data requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6001",
+	.desc = "Offcore demand data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF801",
+	.desc = "Offcore demand data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4001",
+	.desc = "Offcore demand data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2001",
+	.desc = "Offcore demand data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6004",
+	.desc = "Offcore demand code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF804",
+	.desc = "Offcore demand code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4004",
+	.desc = "Offcore demand code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2004",
+	.desc = "Offcore demand code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6002",
+	.desc = "Offcore demand RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF802",
+	.desc = "Offcore demand RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4002",
+	.desc = "Offcore demand RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2002",
+	.desc = "Offcore demand RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6080",
+	.desc = "Offcore other requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF880",
+	.desc = "Offcore other requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2080",
+	.desc = "Offcore other requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6030",
+	.desc = "Offcore prefetch data requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF830",
+	.desc = "Offcore prefetch data requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4030",
+	.desc = "Offcore prefetch data requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2030",
+	.desc = "Offcore prefetch data requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6010",
+	.desc = "Offcore prefetch data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF810",
+	.desc = "Offcore prefetch data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4010",
+	.desc = "Offcore prefetch data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2010",
+	.desc = "Offcore prefetch data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6040",
+	.desc = "Offcore prefetch code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF840",
+	.desc = "Offcore prefetch code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4040",
+	.desc = "Offcore prefetch code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2040",
+	.desc = "Offcore prefetch code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6020",
+	.desc = "Offcore prefetch RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF820",
+	.desc = "Offcore prefetch RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4020",
+	.desc = "Offcore prefetch RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2020",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6070",
+	.desc = "Offcore prefetch requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF870",
+	.desc = "Offcore prefetch requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4070",
+	.desc = "Offcore prefetch requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2070",
+	.desc = "Offcore prefetch requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "dtlb_load_misses.any",
+	.event = "event=0x8,umask=0x1,period=200000",
+	.desc = "DTLB load misses",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.large_walk_completed",
+	.event = "event=0x8,umask=0x80,period=200000",
+	.desc = "DTLB load miss large page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.pde_miss",
+	.event = "event=0x8,umask=0x20,period=200000",
+	.desc = "DTLB load miss caused by low part of address",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x8,umask=0x10,period=2000000",
+	.desc = "DTLB second level hit",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x8,umask=0x2,period=200000",
+	.desc = "DTLB load miss page walks complete",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_cycles",
+	.event = "event=0x8,umask=0x4,period=200000",
+	.desc = "DTLB load miss page walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.any",
+	.event = "event=0x49,umask=0x1,period=200000",
+	.desc = "DTLB misses",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.large_walk_completed",
+	.event = "event=0x49,umask=0x80,period=200000",
+	.desc = "DTLB miss large page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.pde_miss",
+	.event = "event=0x49,umask=0x20,period=200000",
+	.desc = "DTLB misses caused by low part of address. Count also includes 2M page references because 2M pages do not use the PDE",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.stlb_hit",
+	.event = "event=0x49,umask=0x10,period=200000",
+	.desc = "DTLB first level misses but second level hit",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.walk_completed",
+	.event = "event=0x49,umask=0x2,period=200000",
+	.desc = "DTLB miss page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.walk_cycles",
+	.event = "event=0x49,umask=0x4,period=2000000",
+	.desc = "DTLB miss page walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4F,umask=0x10,period=2000000",
+	.desc = "Extended Page Table walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=2000000",
+	.desc = "ITLB flushes",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_miss_retired",
+	.event = "event=0xC8,umask=0x20,period=200000",
+	.desc = "Retired instructions that missed the ITLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.any",
+	.event = "event=0x85,umask=0x1,period=200000",
+	.desc = "ITLB miss",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.large_walk_completed",
+	.event = "event=0x85,umask=0x80,period=200000",
+	.desc = "ITLB miss large page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0x2,period=200000",
+	.desc = "ITLB miss page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_cycles",
+	.event = "event=0x85,umask=0x4,period=2000000",
+	.desc = "ITLB miss page walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_load_retired.dtlb_miss",
+	.event = "event=0xCB,umask=0x80,period=200000",
+	.desc = "Retired loads that miss the DTLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_store_retired.dtlb_miss",
+	.event = "event=0xC,umask=0x1,period=200000",
+	.desc = "Retired stores that miss the DTLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "cache_lock_cycles.l1d",
+	.event = "event=0x63,umask=0x2,period=2000000",
+	.desc = "Cycles L1D locked",
+	.topic = "cache",
+},
+{
+	.name = "cache_lock_cycles.l1d_l2",
+	.event = "event=0x63,umask=0x1,period=2000000",
+	.desc = "Cycles L1D and L2 locked",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_evict",
+	.event = "event=0x51,umask=0x4,period=2000000",
+	.desc = "L1D cache lines replaced in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_repl",
+	.event = "event=0x51,umask=0x2,period=2000000",
+	.desc = "L1D cache lines allocated in the M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_snoop_evict",
+	.event = "event=0x51,umask=0x8,period=2000000",
+	.desc = "L1D snoop eviction of cache lines in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.repl",
+	.event = "event=0x51,umask=0x1,period=2000000",
+	.desc = "L1 data cache lines allocated",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_prefetch_lock_fb_hit",
+	.event = "event=0x52,umask=0x1,period=2000000",
+	.desc = "L1D prefetch load lock accepted in fill buffer",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.miss",
+	.event = "event=0x4E,umask=0x2,period=200000",
+	.desc = "L1D hardware prefetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.requests",
+	.event = "event=0x4E,umask=0x1,period=200000",
+	.desc = "L1D hardware prefetch requests",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.triggers",
+	.event = "event=0x4E,umask=0x4,period=200000",
+	.desc = "L1D hardware prefetch requests triggered",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.e_state",
+	.event = "event=0x28,umask=0x4,period=100000",
+	.desc = "L1 writebacks to L2 in E state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.i_state",
+	.event = "event=0x28,umask=0x1,period=100000",
+	.desc = "L1 writebacks to L2 in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.m_state",
+	.event = "event=0x28,umask=0x8,period=100000",
+	.desc = "L1 writebacks to L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.mesi",
+	.event = "event=0x28,umask=0xf,period=100000",
+	.desc = "All L1 writebacks to L2",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.s_state",
+	.event = "event=0x28,umask=0x2,period=100000",
+	.desc = "L1 writebacks to L2 in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.any",
+	.event = "event=0x26,umask=0xff,period=200000",
+	.desc = "All L2 data requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.e_state",
+	.event = "event=0x26,umask=0x4,period=200000",
+	.desc = "L2 data demand loads in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.i_state",
+	.event = "event=0x26,umask=0x1,period=200000",
+	.desc = "L2 data demand loads in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.m_state",
+	.event = "event=0x26,umask=0x8,period=200000",
+	.desc = "L2 data demand loads in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.mesi",
+	.event = "event=0x26,umask=0xf,period=200000",
+	.desc = "L2 data demand requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.s_state",
+	.event = "event=0x26,umask=0x2,period=200000",
+	.desc = "L2 data demand loads in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.e_state",
+	.event = "event=0x26,umask=0x40,period=200000",
+	.desc = "L2 data prefetches in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.i_state",
+	.event = "event=0x26,umask=0x10,period=200000",
+	.desc = "L2 data prefetches in the I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.m_state",
+	.event = "event=0x26,umask=0x80,period=200000",
+	.desc = "L2 data prefetches in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.mesi",
+	.event = "event=0x26,umask=0xf0,period=200000",
+	.desc = "All L2 data prefetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.s_state",
+	.event = "event=0x26,umask=0x20,period=200000",
+	.desc = "L2 data prefetches in the S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.any",
+	.event = "event=0xF1,umask=0x7,period=100000",
+	.desc = "L2 lines alloacated",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.e_state",
+	.event = "event=0xF1,umask=0x4,period=100000",
+	.desc = "L2 lines allocated in the E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.s_state",
+	.event = "event=0xF1,umask=0x2,period=100000",
+	.desc = "L2 lines allocated in the S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.any",
+	.event = "event=0xF2,umask=0xf,period=100000",
+	.desc = "L2 lines evicted",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x1,period=100000",
+	.desc = "L2 lines evicted by a demand request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_dirty",
+	.event = "event=0xF2,umask=0x2,period=100000",
+	.desc = "L2 modified lines evicted by a demand request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.prefetch_clean",
+	.event = "event=0xF2,umask=0x4,period=100000",
+	.desc = "L2 lines evicted by a prefetch request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.prefetch_dirty",
+	.event = "event=0xF2,umask=0x8,period=100000",
+	.desc = "L2 modified lines evicted by a prefetch request",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetch_hit",
+	.event = "event=0x24,umask=0x10,period=200000",
+	.desc = "L2 instruction fetch hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetch_miss",
+	.event = "event=0x24,umask=0x20,period=200000",
+	.desc = "L2 instruction fetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetches",
+	.event = "event=0x24,umask=0x30,period=200000",
+	.desc = "L2 instruction fetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ld_hit",
+	.event = "event=0x24,umask=0x1,period=200000",
+	.desc = "L2 load hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ld_miss",
+	.event = "event=0x24,umask=0x2,period=200000",
+	.desc = "L2 load misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.loads",
+	.event = "event=0x24,umask=0x3,period=200000",
+	.desc = "L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.miss",
+	.event = "event=0x24,umask=0xaa,period=200000",
+	.desc = "All L2 misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetch_hit",
+	.event = "event=0x24,umask=0x40,period=200000",
+	.desc = "L2 prefetch hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetch_miss",
+	.event = "event=0x24,umask=0x80,period=200000",
+	.desc = "L2 prefetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetches",
+	.event = "event=0x24,umask=0xc0,period=200000",
+	.desc = "All L2 prefetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.references",
+	.event = "event=0x24,umask=0xff,period=200000",
+	.desc = "All L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x4,period=200000",
+	.desc = "L2 RFO hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x8,period=200000",
+	.desc = "L2 RFO misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfos",
+	.event = "event=0x24,umask=0xc,period=200000",
+	.desc = "L2 RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.any",
+	.event = "event=0xF0,umask=0x80,period=200000",
+	.desc = "All L2 transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.fill",
+	.event = "event=0xF0,umask=0x20,period=200000",
+	.desc = "L2 fill transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.ifetch",
+	.event = "event=0xF0,umask=0x4,period=200000",
+	.desc = "L2 instruction fetch transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200000",
+	.desc = "L1D writeback to L2 transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.load",
+	.event = "event=0xF0,umask=0x1,period=200000",
+	.desc = "L2 Load transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.prefetch",
+	.event = "event=0xF0,umask=0x8,period=200000",
+	.desc = "L2 prefetch transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.rfo",
+	.event = "event=0xF0,umask=0x2,period=200000",
+	.desc = "L2 RFO transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.wb",
+	.event = "event=0xF0,umask=0x40,period=200000",
+	.desc = "L2 writeback to LLC transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.e_state",
+	.event = "event=0x27,umask=0x40,period=100000",
+	.desc = "L2 demand lock RFOs in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.hit",
+	.event = "event=0x27,umask=0xe0,period=100000",
+	.desc = "All demand L2 lock RFOs that hit the cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.i_state",
+	.event = "event=0x27,umask=0x10,period=100000",
+	.desc = "L2 demand lock RFOs in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.m_state",
+	.event = "event=0x27,umask=0x80,period=100000",
+	.desc = "L2 demand lock RFOs in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.mesi",
+	.event = "event=0x27,umask=0xf0,period=100000",
+	.desc = "All demand L2 lock RFOs",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.s_state",
+	.event = "event=0x27,umask=0x20,period=100000",
+	.desc = "L2 demand lock RFOs in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.hit",
+	.event = "event=0x27,umask=0xe,period=100000",
+	.desc = "All L2 demand store RFOs that hit the cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.i_state",
+	.event = "event=0x27,umask=0x1,period=100000",
+	.desc = "L2 demand store RFOs in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.m_state",
+	.event = "event=0x27,umask=0x8,period=100000",
+	.desc = "L2 demand store RFOs in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.mesi",
+	.event = "event=0x27,umask=0xf,period=100000",
+	.desc = "All L2 demand store RFOs",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.s_state",
+	.event = "event=0x27,umask=0x2,period=100000",
+	.desc = "L2 demand store RFOs in S state",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100000",
+	.desc = "Longest latency cache miss",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=200000",
+	.desc = "Longest latency cache reference",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.loads",
+	.event = "event=0xB,umask=0x1,period=2000000",
+	.desc = "Instructions retired which contains a load (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.stores",
+	.event = "event=0xB,umask=0x2,period=2000000",
+	.desc = "Instructions retired which contains a store (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.hit_lfb",
+	.event = "event=0xCB,umask=0x40,period=200000",
+	.desc = "Retired loads that miss L1D and hit an previously allocated LFB (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l1d_hit",
+	.event = "event=0xCB,umask=0x1,period=2000000",
+	.desc = "Retired loads that hit the L1 data cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l2_hit",
+	.event = "event=0xCB,umask=0x2,period=200000",
+	.desc = "Retired loads that hit the L2 cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.llc_miss",
+	.event = "event=0xCB,umask=0x10,period=10000",
+	.desc = "Retired loads that miss the LLC cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.llc_unshared_hit",
+	.event = "event=0xCB,umask=0x4,period=40000",
+	.desc = "Retired loads that hit valid versions in the LLC cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.other_core_l2_hit_hitm",
+	.event = "event=0xCB,umask=0x8,period=40000",
+	.desc = "Retired loads that hit sibling core's L2 in modified or unmodified states (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.local_hitm",
+	.event = "event=0xF,umask=0x2,period=40000",
+	.desc = "Load instructions retired that HIT modified data in sibling core (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.local_dram_and_remote_cache_hit",
+	.event = "event=0xF,umask=0x8,period=20000",
+	.desc = "Load instructions retired local dram and remote cache HIT data sources (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.remote_dram",
+	.event = "event=0xF,umask=0x20,period=10000",
+	.desc = "Load instructions retired remote DRAM and remote home-remote cache HITM (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.uncacheable",
+	.event = "event=0xF,umask=0x80,period=4000",
+	.desc = "Load instructions retired IO (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.remote_hitm",
+	.event = "event=0xF,umask=0x4,period=40000",
+	.desc = "Retired loads that hit remote socket in modified state (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.any",
+	.event = "event=0xB0,umask=0x80,period=100000",
+	.desc = "All offcore requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.any.read",
+	.event = "event=0xB0,umask=0x8,period=100000",
+	.desc = "Offcore read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.any.rfo",
+	.event = "event=0xB0,umask=0x10,period=100000",
+	.desc = "Offcore RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand.read_code",
+	.event = "event=0xB0,umask=0x2,period=100000",
+	.desc = "Offcore demand code read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand.read_data",
+	.event = "event=0xB0,umask=0x1,period=100000",
+	.desc = "Offcore demand data read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand.rfo",
+	.event = "event=0xB0,umask=0x4,period=100000",
+	.desc = "Offcore demand RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.l1d_writeback",
+	.event = "event=0xB0,umask=0x40,period=100000",
+	.desc = "Offcore L1 data cache writebacks",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.any.read",
+	.event = "event=0x60,umask=0x8,period=2000000",
+	.desc = "Outstanding offcore reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.any.read_not_empty",
+	.event = "event=0x60,umask=0x8,period=2000000,cmask=1",
+	.desc = "Cycles offcore reads busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_code",
+	.event = "event=0x60,umask=0x2,period=2000000",
+	.desc = "Outstanding offcore demand code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_code_not_empty",
+	.event = "event=0x60,umask=0x2,period=2000000,cmask=1",
+	.desc = "Cycles offcore demand code read busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_data",
+	.event = "event=0x60,umask=0x1,period=2000000",
+	.desc = "Outstanding offcore demand data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_data_not_empty",
+	.event = "event=0x60,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles offcore demand data read busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.rfo",
+	.event = "event=0x60,umask=0x4,period=2000000",
+	.desc = "Outstanding offcore demand RFOs",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.rfo_not_empty",
+	.event = "event=0x60,umask=0x4,period=2000000,cmask=1",
+	.desc = "Cycles offcore demand RFOs busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_sq_full",
+	.event = "event=0xB2,umask=0x1,period=100000",
+	.desc = "Offcore requests blocked due to Super Queue full",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.lru_hints",
+	.event = "event=0xF4,umask=0x4,period=2000000",
+	.desc = "Super Queue LRU hints sent to LLC",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xF4,umask=0x10,period=2000000",
+	.desc = "Super Queue lock splits across a cache line",
+	.topic = "cache",
+},
+{
+	.name = "store_blocks.at_ret",
+	.event = "event=0x6,umask=0x4,period=200000",
+	.desc = "Loads delayed with at-Retirement block code",
+	.topic = "cache",
+},
+{
+	.name = "store_blocks.l1d_block",
+	.event = "event=0x6,umask=0x8,period=200000",
+	.desc = "Cacheable loads delayed with L1D block code",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_0",
+	.event = "event=0xB,umask=0x10,period=2000000,ldlat=0x0",
+	.desc = "Memory instructions retired above 0 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_1024",
+	.event = "event=0xB,umask=0x10,period=100,ldlat=0x400",
+	.desc = "Memory instructions retired above 1024 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_128",
+	.event = "event=0xB,umask=0x10,period=1000,ldlat=0x80",
+	.desc = "Memory instructions retired above 128 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_16",
+	.event = "event=0xB,umask=0x10,period=10000,ldlat=0x10",
+	.desc = "Memory instructions retired above 16 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_16384",
+	.event = "event=0xB,umask=0x10,period=5,ldlat=0x4000",
+	.desc = "Memory instructions retired above 16384 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_2048",
+	.event = "event=0xB,umask=0x10,period=50,ldlat=0x800",
+	.desc = "Memory instructions retired above 2048 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_256",
+	.event = "event=0xB,umask=0x10,period=500,ldlat=0x100",
+	.desc = "Memory instructions retired above 256 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_32",
+	.event = "event=0xB,umask=0x10,period=5000,ldlat=0x20",
+	.desc = "Memory instructions retired above 32 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_32768",
+	.event = "event=0xB,umask=0x10,period=3,ldlat=0x8000",
+	.desc = "Memory instructions retired above 32768 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_4",
+	.event = "event=0xB,umask=0x10,period=50000,ldlat=0x4",
+	.desc = "Memory instructions retired above 4 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_4096",
+	.event = "event=0xB,umask=0x10,period=20,ldlat=0x1000",
+	.desc = "Memory instructions retired above 4096 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_512",
+	.event = "event=0xB,umask=0x10,period=200,ldlat=0x200",
+	.desc = "Memory instructions retired above 512 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_64",
+	.event = "event=0xB,umask=0x10,period=2000,ldlat=0x40",
+	.desc = "Memory instructions retired above 64 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_8",
+	.event = "event=0xB,umask=0x10,period=20000,ldlat=0x8",
+	.desc = "Memory instructions retired above 8 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_8192",
+	.event = "event=0xB,umask=0x10,period=10,ldlat=0x2000",
+	.desc = "Memory instructions retired above 8192 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F11",
+	.desc = "Offcore data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF11",
+	.desc = "All offcore data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8011",
+	.desc = "Offcore data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x111",
+	.desc = "Offcore data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x211",
+	.desc = "Offcore data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x411",
+	.desc = "Offcore data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x711",
+	.desc = "Offcore data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4711",
+	.desc = "Offcore data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1811",
+	.desc = "Offcore data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3811",
+	.desc = "Offcore data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1011",
+	.desc = "Offcore data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x811",
+	.desc = "Offcore data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F44",
+	.desc = "Offcore code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF44",
+	.desc = "All offcore code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8044",
+	.desc = "Offcore code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x144",
+	.desc = "Offcore code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x244",
+	.desc = "Offcore code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x444",
+	.desc = "Offcore code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x744",
+	.desc = "Offcore code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4744",
+	.desc = "Offcore code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1844",
+	.desc = "Offcore code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3844",
+	.desc = "Offcore code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1044",
+	.desc = "Offcore code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x844",
+	.desc = "Offcore code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7FFF",
+	.desc = "Offcore requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFFFF",
+	.desc = "All offcore requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x80FF",
+	.desc = "Offcore requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1FF",
+	.desc = "Offcore requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2FF",
+	.desc = "Offcore requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4FF",
+	.desc = "Offcore requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7FF",
+	.desc = "Offcore requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x47FF",
+	.desc = "Offcore requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x18FF",
+	.desc = "Offcore requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x38FF",
+	.desc = "Offcore requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x10FF",
+	.desc = "Offcore requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8FF",
+	.desc = "Offcore requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F22",
+	.desc = "Offcore RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF22",
+	.desc = "All offcore RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8022",
+	.desc = "Offcore RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x122",
+	.desc = "Offcore RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x222",
+	.desc = "Offcore RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x422",
+	.desc = "Offcore RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x722",
+	.desc = "Offcore RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4722",
+	.desc = "Offcore RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1822",
+	.desc = "Offcore RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3822",
+	.desc = "Offcore RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1022",
+	.desc = "Offcore RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x822",
+	.desc = "Offcore RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F08",
+	.desc = "Offcore writebacks to any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF08",
+	.desc = "All offcore writebacks",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8008",
+	.desc = "Offcore writebacks to the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x108",
+	.desc = "Offcore writebacks to the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x408",
+	.desc = "Offcore writebacks to the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x708",
+	.desc = "Offcore writebacks to the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4708",
+	.desc = "Offcore writebacks to the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1808",
+	.desc = "Offcore writebacks to a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3808",
+	.desc = "Offcore writebacks to a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1008",
+	.desc = "Offcore writebacks that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x808",
+	.desc = "Offcore writebacks that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F77",
+	.desc = "Offcore code or data read requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF77",
+	.desc = "All offcore code or data read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8077",
+	.desc = "Offcore code or data read requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x177",
+	.desc = "Offcore code or data read requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x277",
+	.desc = "Offcore code or data read requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x477",
+	.desc = "Offcore code or data read requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x777",
+	.desc = "Offcore code or data read requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4777",
+	.desc = "Offcore code or data read requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1877",
+	.desc = "Offcore code or data read requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3877",
+	.desc = "Offcore code or data read requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1077",
+	.desc = "Offcore code or data read requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x877",
+	.desc = "Offcore code or data read requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F33",
+	.desc = "Offcore request = all data, response = any cache_dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF33",
+	.desc = "Offcore request = all data, response = any location",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8033",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x133",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x233",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x433",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x733",
+	.desc = "Offcore request = all data, response = local cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4733",
+	.desc = "Offcore request = all data, response = local cache or dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1833",
+	.desc = "Offcore request = all data, response = remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3833",
+	.desc = "Offcore request = all data, response = remote cache or dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1033",
+	.desc = "Offcore data reads, RFO's and prefetches that HIT in a remote cache ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x833",
+	.desc = "Offcore data reads, RFO's and prefetches that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F03",
+	.desc = "Offcore demand data requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF03",
+	.desc = "All offcore demand data requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8003",
+	.desc = "Offcore demand data requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x103",
+	.desc = "Offcore demand data requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x203",
+	.desc = "Offcore demand data requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x403",
+	.desc = "Offcore demand data requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x703",
+	.desc = "Offcore demand data requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4703",
+	.desc = "Offcore demand data requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1803",
+	.desc = "Offcore demand data requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3803",
+	.desc = "Offcore demand data requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1003",
+	.desc = "Offcore demand data requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x803",
+	.desc = "Offcore demand data requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F01",
+	.desc = "Offcore demand data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF01",
+	.desc = "All offcore demand data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8001",
+	.desc = "Offcore demand data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x101",
+	.desc = "Offcore demand data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x201",
+	.desc = "Offcore demand data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x401",
+	.desc = "Offcore demand data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x701",
+	.desc = "Offcore demand data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4701",
+	.desc = "Offcore demand data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1801",
+	.desc = "Offcore demand data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3801",
+	.desc = "Offcore demand data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1001",
+	.desc = "Offcore demand data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x801",
+	.desc = "Offcore demand data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F04",
+	.desc = "Offcore demand code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF04",
+	.desc = "All offcore demand code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8004",
+	.desc = "Offcore demand code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x104",
+	.desc = "Offcore demand code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x204",
+	.desc = "Offcore demand code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x404",
+	.desc = "Offcore demand code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x704",
+	.desc = "Offcore demand code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4704",
+	.desc = "Offcore demand code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1804",
+	.desc = "Offcore demand code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3804",
+	.desc = "Offcore demand code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1004",
+	.desc = "Offcore demand code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x804",
+	.desc = "Offcore demand code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F02",
+	.desc = "Offcore demand RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF02",
+	.desc = "All offcore demand RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8002",
+	.desc = "Offcore demand RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x102",
+	.desc = "Offcore demand RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x202",
+	.desc = "Offcore demand RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x402",
+	.desc = "Offcore demand RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x702",
+	.desc = "Offcore demand RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4702",
+	.desc = "Offcore demand RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1802",
+	.desc = "Offcore demand RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3802",
+	.desc = "Offcore demand RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1002",
+	.desc = "Offcore demand RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x802",
+	.desc = "Offcore demand RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F80",
+	.desc = "Offcore other requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF80",
+	.desc = "All offcore other requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8080",
+	.desc = "Offcore other requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x180",
+	.desc = "Offcore other requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x280",
+	.desc = "Offcore other requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x480",
+	.desc = "Offcore other requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x780",
+	.desc = "Offcore other requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4780",
+	.desc = "Offcore other requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1880",
+	.desc = "Offcore other requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3880",
+	.desc = "Offcore other requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1080",
+	.desc = "Offcore other requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x880",
+	.desc = "Offcore other requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F30",
+	.desc = "Offcore prefetch data requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF30",
+	.desc = "All offcore prefetch data requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8030",
+	.desc = "Offcore prefetch data requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x130",
+	.desc = "Offcore prefetch data requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x230",
+	.desc = "Offcore prefetch data requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x430",
+	.desc = "Offcore prefetch data requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x730",
+	.desc = "Offcore prefetch data requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4730",
+	.desc = "Offcore prefetch data requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1830",
+	.desc = "Offcore prefetch data requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3830",
+	.desc = "Offcore prefetch data requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1030",
+	.desc = "Offcore prefetch data requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x830",
+	.desc = "Offcore prefetch data requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F10",
+	.desc = "Offcore prefetch data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF10",
+	.desc = "All offcore prefetch data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8010",
+	.desc = "Offcore prefetch data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x110",
+	.desc = "Offcore prefetch data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x210",
+	.desc = "Offcore prefetch data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x410",
+	.desc = "Offcore prefetch data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x710",
+	.desc = "Offcore prefetch data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4710",
+	.desc = "Offcore prefetch data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1810",
+	.desc = "Offcore prefetch data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3810",
+	.desc = "Offcore prefetch data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1010",
+	.desc = "Offcore prefetch data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x810",
+	.desc = "Offcore prefetch data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F40",
+	.desc = "Offcore prefetch code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF40",
+	.desc = "All offcore prefetch code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8040",
+	.desc = "Offcore prefetch code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x140",
+	.desc = "Offcore prefetch code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x240",
+	.desc = "Offcore prefetch code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x440",
+	.desc = "Offcore prefetch code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x740",
+	.desc = "Offcore prefetch code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4740",
+	.desc = "Offcore prefetch code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1840",
+	.desc = "Offcore prefetch code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3840",
+	.desc = "Offcore prefetch code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1040",
+	.desc = "Offcore prefetch code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x840",
+	.desc = "Offcore prefetch code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F20",
+	.desc = "Offcore prefetch RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF20",
+	.desc = "All offcore prefetch RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8020",
+	.desc = "Offcore prefetch RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x120",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x220",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x420",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x720",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4720",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1820",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3820",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1020",
+	.desc = "Offcore prefetch RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x820",
+	.desc = "Offcore prefetch RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F70",
+	.desc = "Offcore prefetch requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF70",
+	.desc = "All offcore prefetch requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8070",
+	.desc = "Offcore prefetch requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x170",
+	.desc = "Offcore prefetch requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x270",
+	.desc = "Offcore prefetch requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x470",
+	.desc = "Offcore prefetch requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x770",
+	.desc = "Offcore prefetch requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4770",
+	.desc = "Offcore prefetch requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1870",
+	.desc = "Offcore prefetch requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3870",
+	.desc = "Offcore prefetch requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1070",
+	.desc = "Offcore prefetch requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x870",
+	.desc = "Offcore prefetch requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_nehalemep[] = {
+{
+	.name = "fp_assist.all",
+	.event = "event=0xF7,umask=0x1,period=20000",
+	.desc = "X87 Floating point assists (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.input",
+	.event = "event=0xF7,umask=0x4,period=20000",
+	.desc = "X87 Floating poiint assists for invalid input value (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.output",
+	.event = "event=0xF7,umask=0x2,period=20000",
+	.desc = "X87 Floating point assists for invalid output value (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.mmx",
+	.event = "event=0x10,umask=0x2,period=2000000",
+	.desc = "MMX Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_double_precision",
+	.event = "event=0x10,umask=0x80,period=2000000",
+	.desc = "SSE* FP double precision Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp",
+	.event = "event=0x10,umask=0x4,period=2000000",
+	.desc = "SSE and SSE2 FP Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp_packed",
+	.event = "event=0x10,umask=0x10,period=2000000",
+	.desc = "SSE FP packed Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp_scalar",
+	.event = "event=0x10,umask=0x20,period=2000000",
+	.desc = "SSE FP scalar Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_single_precision",
+	.event = "event=0x10,umask=0x40,period=2000000",
+	.desc = "SSE* FP single precision Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse2_integer",
+	.event = "event=0x10,umask=0x8,period=2000000",
+	.desc = "SSE2 integer Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.x87",
+	.event = "event=0x10,umask=0x1,period=2000000",
+	.desc = "Computational floating-point operations executed",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.any",
+	.event = "event=0xCC,umask=0x3,period=2000000",
+	.desc = "All Floating Point to and from MMX transitions",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.to_fp",
+	.event = "event=0xCC,umask=0x1,period=2000000",
+	.desc = "Transitions from MMX to Floating Point instructions",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.to_mmx",
+	.event = "event=0xCC,umask=0x2,period=2000000",
+	.desc = "Transitions from Floating Point to MMX instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.pack",
+	.event = "event=0x12,umask=0x4,period=200000",
+	.desc = "128 bit SIMD integer pack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_arith",
+	.event = "event=0x12,umask=0x20,period=200000",
+	.desc = "128 bit SIMD integer arithmetic operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_logical",
+	.event = "event=0x12,umask=0x10,period=200000",
+	.desc = "128 bit SIMD integer logical operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_mpy",
+	.event = "event=0x12,umask=0x1,period=200000",
+	.desc = "128 bit SIMD integer multiply operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_shift",
+	.event = "event=0x12,umask=0x2,period=200000",
+	.desc = "128 bit SIMD integer shift operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.shuffle_move",
+	.event = "event=0x12,umask=0x40,period=200000",
+	.desc = "128 bit SIMD integer shuffle/move operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.unpack",
+	.event = "event=0x12,umask=0x8,period=200000",
+	.desc = "128 bit SIMD integer unpack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.pack",
+	.event = "event=0xFD,umask=0x4,period=200000",
+	.desc = "SIMD integer 64 bit pack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_arith",
+	.event = "event=0xFD,umask=0x20,period=200000",
+	.desc = "SIMD integer 64 bit arithmetic operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_logical",
+	.event = "event=0xFD,umask=0x10,period=200000",
+	.desc = "SIMD integer 64 bit logical operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_mpy",
+	.event = "event=0xFD,umask=0x1,period=200000",
+	.desc = "SIMD integer 64 bit packed multiply operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_shift",
+	.event = "event=0xFD,umask=0x2,period=200000",
+	.desc = "SIMD integer 64 bit shift operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.shuffle_move",
+	.event = "event=0xFD,umask=0x40,period=200000",
+	.desc = "SIMD integer 64 bit shuffle/move operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.unpack",
+	.event = "event=0xFD,umask=0x8,period=200000",
+	.desc = "SIMD integer 64 bit unpack operations",
+	.topic = "floating point",
+},
+{
+	.name = "arith.cycles_div_busy",
+	.event = "event=0x14,umask=0x1,period=2000000",
+	.desc = "Cycles the divider is busy",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.div",
+	.event = "event=0x14,inv=1,umask=0x1,period=2000000,cmask=1,edge=1",
+	.desc = "Divide Operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.mul",
+	.event = "event=0x14,umask=0x2,period=2000000",
+	.desc = "Multiply operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear.bad_target",
+	.event = "event=0xE6,umask=0x2,period=2000000",
+	.desc = "BACLEAR asserted with bad target address",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear.clear",
+	.event = "event=0xE6,umask=0x1,period=2000000",
+	.desc = "BACLEAR asserted, regardless of cause ",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear_force_iq",
+	.event = "event=0xA7,umask=0x1,period=2000000",
+	.desc = "Instruction queue forced BACLEAR",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_decoded",
+	.event = "event=0xE0,umask=0x1,period=2000000",
+	.desc = "Branch instructions decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.any",
+	.event = "event=0x88,umask=0x7f,period=200000",
+	.desc = "Branch instructions executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.cond",
+	.event = "event=0x88,umask=0x1,period=200000",
+	.desc = "Conditional branch instructions executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.direct",
+	.event = "event=0x88,umask=0x2,period=200000",
+	.desc = "Unconditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.direct_near_call",
+	.event = "event=0x88,umask=0x10,period=20000",
+	.desc = "Unconditional call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.indirect_near_call",
+	.event = "event=0x88,umask=0x20,period=20000",
+	.desc = "Indirect call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.indirect_non_call",
+	.event = "event=0x88,umask=0x4,period=20000",
+	.desc = "Indirect non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.near_calls",
+	.event = "event=0x88,umask=0x30,period=20000",
+	.desc = "Call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.non_calls",
+	.event = "event=0x88,umask=0x7,period=200000",
+	.desc = "All non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.return_near",
+	.event = "event=0x88,umask=0x8,period=20000",
+	.desc = "Indirect return branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken",
+	.event = "event=0x88,umask=0x40,period=200000",
+	.desc = "Taken branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x4,period=200000",
+	.desc = "Retired branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=200000",
+	.desc = "Retired conditional branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=20000",
+	.desc = "Retired near call instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.any",
+	.event = "event=0x89,umask=0x7f,period=20000",
+	.desc = "Mispredicted branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.cond",
+	.event = "event=0x89,umask=0x1,period=20000",
+	.desc = "Mispredicted conditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.direct",
+	.event = "event=0x89,umask=0x2,period=20000",
+	.desc = "Mispredicted unconditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.direct_near_call",
+	.event = "event=0x89,umask=0x10,period=2000",
+	.desc = "Mispredicted non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.indirect_near_call",
+	.event = "event=0x89,umask=0x20,period=2000",
+	.desc = "Mispredicted indirect call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.indirect_non_call",
+	.event = "event=0x89,umask=0x4,period=2000",
+	.desc = "Mispredicted indirect non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.near_calls",
+	.event = "event=0x89,umask=0x30,period=2000",
+	.desc = "Mispredicted call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.non_calls",
+	.event = "event=0x89,umask=0x7,period=20000",
+	.desc = "Mispredicted non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.return_near",
+	.event = "event=0x89,umask=0x8,period=2000",
+	.desc = "Mispredicted return branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken",
+	.event = "event=0x89,umask=0x40,period=20000",
+	.desc = "Mispredicted taken branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_call",
+	.event = "event=0xC5,umask=0x2,period=2000",
+	.desc = "Mispredicted near retired calls (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref",
+	.event = "event=0x0,umask=0x03",
+	.desc = "Reference cycles when thread is not halted (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_p",
+	.event = "event=0x3C,umask=0x1,period=100000",
+	.desc = "Reference base clock (133 Mhz) cycles when thread is not halted (programmable counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Cycles when thread is not halted (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000000",
+	.desc = "Cycles when thread is not halted (programmable counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.total_cycles",
+	.event = "event=0x3C,inv=1,umask=0x0,period=2000000,cmask=2",
+	.desc = "Total CPU cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.any",
+	.event = "event=0x87,umask=0xf,period=2000000",
+	.desc = "Any Instruction Length Decoder stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.iq_full",
+	.event = "event=0x87,umask=0x4,period=2000000",
+	.desc = "Instruction Queue full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000000",
+	.desc = "Length Change Prefix stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.mru",
+	.event = "event=0x87,umask=0x2,period=2000000",
+	.desc = "Stall cycles due to BPU MRU bypass",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.regen",
+	.event = "event=0x87,umask=0x8,period=2000000",
+	.desc = "Regen stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_decoded.dec0",
+	.event = "event=0x18,umask=0x1,period=2000000",
+	.desc = "Instructions that must be decoded by decoder 0",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_queue_write_cycles",
+	.event = "event=0x1E,umask=0x1,period=2000000",
+	.desc = "Cycles instructions are written to the instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_queue_writes",
+	.event = "event=0x17,umask=0x1,period=2000000",
+	.desc = "Instructions written to instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (Programmable counter and Precise Event) (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.mmx",
+	.event = "event=0xC0,umask=0x4,period=2000000",
+	.desc = "Retired MMX instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.total_cycles",
+	.event = "event=0xC0,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.x87",
+	.event = "event=0xC0,umask=0x2,period=2000000",
+	.desc = "Retired floating-point operations (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre",
+	.event = "event=0x4C,umask=0x1,period=200000",
+	.desc = "Load operations conflicting with software prefetches",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.active",
+	.event = "event=0xA8,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles when uops were delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.inactive",
+	.event = "event=0xA8,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no uops were delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd_overflow",
+	.event = "event=0x20,umask=0x1,period=2000000",
+	.desc = "Loops that can't stream from the instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.cycles",
+	.event = "event=0xC3,umask=0x1,period=20000",
+	.desc = "Cycles machine clear asserted",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.mem_order",
+	.event = "event=0xC3,umask=0x2,period=20000",
+	.desc = "Execution pipeline restart due to Memory ordering conflicts",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=20000",
+	.desc = "Self-Modifying Code detected",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000000",
+	.desc = "Resource related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.fpcw",
+	.event = "event=0xA2,umask=0x20,period=2000000",
+	.desc = "FPU control word write stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.load",
+	.event = "event=0xA2,umask=0x2,period=2000000",
+	.desc = "Load buffer stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.mxcsr",
+	.event = "event=0xA2,umask=0x40,period=2000000",
+	.desc = "MXCSR rename stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.other",
+	.event = "event=0xA2,umask=0x80,period=2000000",
+	.desc = "Other Resource related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rob_full",
+	.event = "event=0xA2,umask=0x10,period=2000000",
+	.desc = "ROB full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rs_full",
+	.event = "event=0xA2,umask=0x4,period=2000000",
+	.desc = "Reservation Station full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.store",
+	.event = "event=0xA2,umask=0x8,period=2000000",
+	.desc = "Store buffer stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.packed_double",
+	.event = "event=0xC7,umask=0x4,period=200000",
+	.desc = "SIMD Packed-Double Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.packed_single",
+	.event = "event=0xC7,umask=0x1,period=200000",
+	.desc = "SIMD Packed-Single Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.scalar_double",
+	.event = "event=0xC7,umask=0x8,period=200000",
+	.desc = "SIMD Scalar-Double Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.scalar_single",
+	.event = "event=0xC7,umask=0x2,period=200000",
+	.desc = "SIMD Scalar-Single Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.vector_integer",
+	.event = "event=0xC7,umask=0x10,period=200000",
+	.desc = "SIMD Vector Integer Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uop_unfusion",
+	.event = "event=0xDB,umask=0x1,period=2000000",
+	.desc = "Uop unfusions due to FP exceptions",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.esp_folding",
+	.event = "event=0xD1,umask=0x4,period=2000000",
+	.desc = "Stack pointer instructions decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.esp_sync",
+	.event = "event=0xD1,umask=0x8,period=2000000",
+	.desc = "Stack pointer sync operations",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.ms_cycles_active",
+	.event = "event=0xD1,umask=0x2,period=2000000,cmask=1",
+	.desc = "Uops decoded by Microcode Sequencer",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.stall_cycles",
+	.event = "event=0xD1,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops are decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_active_cycles",
+	.event = "event=0xB1,umask=0x3f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops executed on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_active_cycles_no_port5",
+	.event = "event=0xB1,umask=0x1f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops executed on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_count",
+	.event = "event=0xB1,inv=1,umask=0x3f,any=1,period=2000000,cmask=1,edge=1",
+	.desc = "Uops executed on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_count_no_port5",
+	.event = "event=0xB1,inv=1,umask=0x1f,any=1,period=2000000,cmask=1,edge=1",
+	.desc = "Uops executed on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x3f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_cycles_no_port5",
+	.event = "event=0xB1,inv=1,umask=0x1f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port0",
+	.event = "event=0xB1,umask=0x1,period=2000000",
+	.desc = "Uops executed on port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port015",
+	.event = "event=0xB1,umask=0x40,period=2000000",
+	.desc = "Uops issued on ports 0, 1 or 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port015_stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x40,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on ports 0, 1 or 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port1",
+	.event = "event=0xB1,umask=0x2,period=2000000",
+	.desc = "Uops executed on port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port2_core",
+	.event = "event=0xB1,umask=0x4,any=1,period=2000000",
+	.desc = "Uops executed on port 2 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port234_core",
+	.event = "event=0xB1,umask=0x80,any=1,period=2000000",
+	.desc = "Uops issued on ports 2, 3 or 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port3_core",
+	.event = "event=0xB1,umask=0x8,any=1,period=2000000",
+	.desc = "Uops executed on port 3 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port4_core",
+	.event = "event=0xB1,umask=0x10,any=1,period=2000000",
+	.desc = "Uops executed on port 4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port5",
+	.event = "event=0xB1,umask=0x20,period=2000000",
+	.desc = "Uops executed on port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0xE,umask=0x1,period=2000000",
+	.desc = "Uops issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.core_stall_cycles",
+	.event = "event=0xE,inv=1,umask=0x1,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops were issued on any thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.cycles_all_threads",
+	.event = "event=0xE,umask=0x1,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops were issued on either thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.fused",
+	.event = "event=0xE,umask=0x2,period=2000000",
+	.desc = "Fused Uops issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0xE,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops were issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.active_cycles",
+	.event = "event=0xC2,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles Uops are being retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.any",
+	.event = "event=0xC2,umask=0x1,period=2000000",
+	.desc = "Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.macro_fused",
+	.event = "event=0xC2,umask=0x4,period=2000000",
+	.desc = "Macro-fused Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000000",
+	.desc = "Retirement slots used (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles Uops are not retiring (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles using precise uop retired event (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.total_cycles_ps",
+	.event = "event=0xC0,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "macro_insts.decoded",
+	.event = "event=0xD0,umask=0x1,period=2000000",
+	.desc = "Instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "macro_insts.fusions_decoded",
+	.event = "event=0xA6,umask=0x1,period=2000000",
+	.desc = "Macro-fused instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "two_uop_insts_decoded",
+	.event = "event=0x19,umask=0x1,period=2000000",
+	.desc = "Two Uop instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "bpu_clears.early",
+	.event = "event=0xE8,umask=0x1,period=2000000",
+	.desc = "Early Branch Prediciton Unit clears",
+	.topic = "other",
+},
+{
+	.name = "bpu_clears.late",
+	.event = "event=0xE8,umask=0x2,period=2000000",
+	.desc = "Late Branch Prediction Unit clears",
+	.topic = "other",
+},
+{
+	.name = "bpu_missed_call_ret",
+	.event = "event=0xE5,umask=0x1,period=2000000",
+	.desc = "Branch prediction unit missed call or return",
+	.topic = "other",
+},
+{
+	.name = "es_reg_renames",
+	.event = "event=0xD5,umask=0x1,period=2000000",
+	.desc = "ES segment renames",
+	.topic = "other",
+},
+{
+	.name = "io_transactions",
+	.event = "event=0x6C,umask=0x1,period=2000000",
+	.desc = "I/O transactions",
+	.topic = "other",
+},
+{
+	.name = "l1i.cycles_stalled",
+	.event = "event=0x80,umask=0x4,period=2000000",
+	.desc = "L1I instruction fetch stall cycles",
+	.topic = "other",
+},
+{
+	.name = "l1i.hits",
+	.event = "event=0x80,umask=0x1,period=2000000",
+	.desc = "L1I instruction fetch hits",
+	.topic = "other",
+},
+{
+	.name = "l1i.misses",
+	.event = "event=0x80,umask=0x2,period=2000000",
+	.desc = "L1I instruction fetch misses",
+	.topic = "other",
+},
+{
+	.name = "l1i.reads",
+	.event = "event=0x80,umask=0x3,period=2000000",
+	.desc = "L1I Instruction fetches",
+	.topic = "other",
+},
+{
+	.name = "large_itlb.hit",
+	.event = "event=0x82,umask=0x1,period=200000",
+	.desc = "Large ITLB hit",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.any",
+	.event = "event=0x13,umask=0x7,period=2000000",
+	.desc = "All loads dispatched",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.mob",
+	.event = "event=0x13,umask=0x4,period=2000000",
+	.desc = "Loads dispatched from the MOB",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.rs",
+	.event = "event=0x13,umask=0x1,period=2000000",
+	.desc = "Loads dispatched that bypass the MOB",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.rs_delayed",
+	.event = "event=0x13,umask=0x2,period=2000000",
+	.desc = "Loads dispatched from stage 305",
+	.topic = "other",
+},
+{
+	.name = "partial_address_alias",
+	.event = "event=0x7,umask=0x1,period=200000",
+	.desc = "False dependencies due to partial address aliasing",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.any",
+	.event = "event=0xD2,umask=0xf,period=2000000",
+	.desc = "All RAT stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.flags",
+	.event = "event=0xD2,umask=0x1,period=2000000",
+	.desc = "Flag stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.registers",
+	.event = "event=0xD2,umask=0x2,period=2000000",
+	.desc = "Partial register stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.rob_read_port",
+	.event = "event=0xD2,umask=0x4,period=2000000",
+	.desc = "ROB read port stalls cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.scoreboard",
+	.event = "event=0xD2,umask=0x8,period=2000000",
+	.desc = "Scoreboard stall cycles",
+	.topic = "other",
+},
+{
+	.name = "sb_drain.any",
+	.event = "event=0x4,umask=0x7,period=200000",
+	.desc = "All Store buffer stall cycles",
+	.topic = "other",
+},
+{
+	.name = "seg_rename_stalls",
+	.event = "event=0xD4,umask=0x1,period=2000000",
+	.desc = "Segment rename stall cycles",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hit",
+	.event = "event=0xB8,umask=0x1,period=100000",
+	.desc = "Thread responded HIT to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hite",
+	.event = "event=0xB8,umask=0x2,period=100000",
+	.desc = "Thread responded HITE to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hitm",
+	.event = "event=0xB8,umask=0x4,period=100000",
+	.desc = "Thread responded HITM to snoop",
+	.topic = "other",
+},
+{
+	.name = "sq_full_stall_cycles",
+	.event = "event=0xF6,umask=0x1,period=2000000",
+	.desc = "Super Queue full stall cycles",
+	.topic = "other",
+},
+{
+	.name = "offcore_response.any_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6011",
+	.desc = "Offcore data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF811",
+	.desc = "Offcore data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4011",
+	.desc = "Offcore data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2011",
+	.desc = "Offcore data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6044",
+	.desc = "Offcore code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF844",
+	.desc = "Offcore code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4044",
+	.desc = "Offcore code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2044",
+	.desc = "Offcore code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x60FF",
+	.desc = "Offcore requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF8FF",
+	.desc = "Offcore requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x40FF",
+	.desc = "Offcore requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x20FF",
+	.desc = "Offcore requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6022",
+	.desc = "Offcore RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF822",
+	.desc = "Offcore RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4022",
+	.desc = "Offcore RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2022",
+	.desc = "Offcore RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6008",
+	.desc = "Offcore writebacks to any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF808",
+	.desc = "Offcore writebacks that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4008",
+	.desc = "Offcore writebacks to the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2008",
+	.desc = "Offcore writebacks to a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6077",
+	.desc = "Offcore code or data read requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF877",
+	.desc = "Offcore code or data read requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4077",
+	.desc = "Offcore code or data read requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2077",
+	.desc = "Offcore code or data read requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6033",
+	.desc = "Offcore request = all data, response = any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF833",
+	.desc = "Offcore request = all data, response = any LLC miss",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4033",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2033",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6003",
+	.desc = "Offcore demand data requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF803",
+	.desc = "Offcore demand data requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4003",
+	.desc = "Offcore demand data requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2003",
+	.desc = "Offcore demand data requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6001",
+	.desc = "Offcore demand data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF801",
+	.desc = "Offcore demand data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4001",
+	.desc = "Offcore demand data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2001",
+	.desc = "Offcore demand data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6004",
+	.desc = "Offcore demand code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF804",
+	.desc = "Offcore demand code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4004",
+	.desc = "Offcore demand code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2004",
+	.desc = "Offcore demand code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6002",
+	.desc = "Offcore demand RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF802",
+	.desc = "Offcore demand RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4002",
+	.desc = "Offcore demand RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2002",
+	.desc = "Offcore demand RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6080",
+	.desc = "Offcore other requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF880",
+	.desc = "Offcore other requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2080",
+	.desc = "Offcore other requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6030",
+	.desc = "Offcore prefetch data requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF830",
+	.desc = "Offcore prefetch data requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4030",
+	.desc = "Offcore prefetch data requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2030",
+	.desc = "Offcore prefetch data requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6010",
+	.desc = "Offcore prefetch data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF810",
+	.desc = "Offcore prefetch data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4010",
+	.desc = "Offcore prefetch data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2010",
+	.desc = "Offcore prefetch data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6040",
+	.desc = "Offcore prefetch code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF840",
+	.desc = "Offcore prefetch code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4040",
+	.desc = "Offcore prefetch code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2040",
+	.desc = "Offcore prefetch code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6020",
+	.desc = "Offcore prefetch RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF820",
+	.desc = "Offcore prefetch RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4020",
+	.desc = "Offcore prefetch RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2020",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6070",
+	.desc = "Offcore prefetch requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF870",
+	.desc = "Offcore prefetch requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4070",
+	.desc = "Offcore prefetch requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2070",
+	.desc = "Offcore prefetch requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "dtlb_load_misses.any",
+	.event = "event=0x8,umask=0x1,period=200000",
+	.desc = "DTLB load misses",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.pde_miss",
+	.event = "event=0x8,umask=0x20,period=200000",
+	.desc = "DTLB load miss caused by low part of address",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x8,umask=0x10,period=2000000",
+	.desc = "DTLB second level hit",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x8,umask=0x2,period=200000",
+	.desc = "DTLB load miss page walks complete",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.any",
+	.event = "event=0x49,umask=0x1,period=200000",
+	.desc = "DTLB misses",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.stlb_hit",
+	.event = "event=0x49,umask=0x10,period=200000",
+	.desc = "DTLB first level misses but second level hit",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.walk_completed",
+	.event = "event=0x49,umask=0x2,period=200000",
+	.desc = "DTLB miss page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=2000000",
+	.desc = "ITLB flushes",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_miss_retired",
+	.event = "event=0xC8,umask=0x20,period=200000",
+	.desc = "Retired instructions that missed the ITLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.any",
+	.event = "event=0x85,umask=0x1,period=200000",
+	.desc = "ITLB miss",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0x2,period=200000",
+	.desc = "ITLB miss page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_load_retired.dtlb_miss",
+	.event = "event=0xCB,umask=0x80,period=200000",
+	.desc = "Retired loads that miss the DTLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_store_retired.dtlb_miss",
+	.event = "event=0xC,umask=0x1,period=200000",
+	.desc = "Retired stores that miss the DTLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "cache_lock_cycles.l1d",
+	.event = "event=0x63,umask=0x2,period=2000000",
+	.desc = "Cycles L1D locked",
+	.topic = "cache",
+},
+{
+	.name = "cache_lock_cycles.l1d_l2",
+	.event = "event=0x63,umask=0x1,period=2000000",
+	.desc = "Cycles L1D and L2 locked",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_evict",
+	.event = "event=0x51,umask=0x4,period=2000000",
+	.desc = "L1D cache lines replaced in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_repl",
+	.event = "event=0x51,umask=0x2,period=2000000",
+	.desc = "L1D cache lines allocated in the M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_snoop_evict",
+	.event = "event=0x51,umask=0x8,period=2000000",
+	.desc = "L1D snoop eviction of cache lines in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.repl",
+	.event = "event=0x51,umask=0x1,period=2000000",
+	.desc = "L1 data cache lines allocated",
+	.topic = "cache",
+},
+{
+	.name = "l1d_all_ref.any",
+	.event = "event=0x43,umask=0x1,period=2000000",
+	.desc = "All references to the L1 data cache",
+	.topic = "cache",
+},
+{
+	.name = "l1d_all_ref.cacheable",
+	.event = "event=0x43,umask=0x2,period=2000000",
+	.desc = "L1 data cacheable reads and writes",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_ld.e_state",
+	.event = "event=0x40,umask=0x4,period=2000000",
+	.desc = "L1 data cache read in E state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_ld.i_state",
+	.event = "event=0x40,umask=0x1,period=2000000",
+	.desc = "L1 data cache read in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_ld.m_state",
+	.event = "event=0x40,umask=0x8,period=2000000",
+	.desc = "L1 data cache read in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_ld.mesi",
+	.event = "event=0x40,umask=0xf,period=2000000",
+	.desc = "L1 data cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_ld.s_state",
+	.event = "event=0x40,umask=0x2,period=2000000",
+	.desc = "L1 data cache read in S state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_lock.e_state",
+	.event = "event=0x42,umask=0x4,period=2000000",
+	.desc = "L1 data cache load locks in E state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_lock.hit",
+	.event = "event=0x42,umask=0x1,period=2000000",
+	.desc = "L1 data cache load lock hits",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_lock.m_state",
+	.event = "event=0x42,umask=0x8,period=2000000",
+	.desc = "L1 data cache load locks in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_lock.s_state",
+	.event = "event=0x42,umask=0x2,period=2000000",
+	.desc = "L1 data cache load locks in S state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_lock_fb_hit",
+	.event = "event=0x53,umask=0x1,period=2000000",
+	.desc = "L1D load lock accepted in fill buffer",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_prefetch_lock_fb_hit",
+	.event = "event=0x52,umask=0x1,period=2000000",
+	.desc = "L1D prefetch load lock accepted in fill buffer",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_st.e_state",
+	.event = "event=0x41,umask=0x4,period=2000000",
+	.desc = "L1 data cache stores in E state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_st.m_state",
+	.event = "event=0x41,umask=0x8,period=2000000",
+	.desc = "L1 data cache stores in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_st.s_state",
+	.event = "event=0x41,umask=0x2,period=2000000",
+	.desc = "L1 data cache stores in S state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.miss",
+	.event = "event=0x4E,umask=0x2,period=200000",
+	.desc = "L1D hardware prefetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.requests",
+	.event = "event=0x4E,umask=0x1,period=200000",
+	.desc = "L1D hardware prefetch requests",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.triggers",
+	.event = "event=0x4E,umask=0x4,period=200000",
+	.desc = "L1D hardware prefetch requests triggered",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.e_state",
+	.event = "event=0x28,umask=0x4,period=100000",
+	.desc = "L1 writebacks to L2 in E state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.i_state",
+	.event = "event=0x28,umask=0x1,period=100000",
+	.desc = "L1 writebacks to L2 in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.m_state",
+	.event = "event=0x28,umask=0x8,period=100000",
+	.desc = "L1 writebacks to L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.mesi",
+	.event = "event=0x28,umask=0xf,period=100000",
+	.desc = "All L1 writebacks to L2",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.s_state",
+	.event = "event=0x28,umask=0x2,period=100000",
+	.desc = "L1 writebacks to L2 in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.any",
+	.event = "event=0x26,umask=0xff,period=200000",
+	.desc = "All L2 data requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.e_state",
+	.event = "event=0x26,umask=0x4,period=200000",
+	.desc = "L2 data demand loads in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.i_state",
+	.event = "event=0x26,umask=0x1,period=200000",
+	.desc = "L2 data demand loads in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.m_state",
+	.event = "event=0x26,umask=0x8,period=200000",
+	.desc = "L2 data demand loads in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.mesi",
+	.event = "event=0x26,umask=0xf,period=200000",
+	.desc = "L2 data demand requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.s_state",
+	.event = "event=0x26,umask=0x2,period=200000",
+	.desc = "L2 data demand loads in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.e_state",
+	.event = "event=0x26,umask=0x40,period=200000",
+	.desc = "L2 data prefetches in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.i_state",
+	.event = "event=0x26,umask=0x10,period=200000",
+	.desc = "L2 data prefetches in the I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.m_state",
+	.event = "event=0x26,umask=0x80,period=200000",
+	.desc = "L2 data prefetches in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.mesi",
+	.event = "event=0x26,umask=0xf0,period=200000",
+	.desc = "All L2 data prefetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.s_state",
+	.event = "event=0x26,umask=0x20,period=200000",
+	.desc = "L2 data prefetches in the S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.any",
+	.event = "event=0xF1,umask=0x7,period=100000",
+	.desc = "L2 lines alloacated",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.e_state",
+	.event = "event=0xF1,umask=0x4,period=100000",
+	.desc = "L2 lines allocated in the E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.s_state",
+	.event = "event=0xF1,umask=0x2,period=100000",
+	.desc = "L2 lines allocated in the S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.any",
+	.event = "event=0xF2,umask=0xf,period=100000",
+	.desc = "L2 lines evicted",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x1,period=100000",
+	.desc = "L2 lines evicted by a demand request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_dirty",
+	.event = "event=0xF2,umask=0x2,period=100000",
+	.desc = "L2 modified lines evicted by a demand request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.prefetch_clean",
+	.event = "event=0xF2,umask=0x4,period=100000",
+	.desc = "L2 lines evicted by a prefetch request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.prefetch_dirty",
+	.event = "event=0xF2,umask=0x8,period=100000",
+	.desc = "L2 modified lines evicted by a prefetch request",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetch_hit",
+	.event = "event=0x24,umask=0x10,period=200000",
+	.desc = "L2 instruction fetch hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetch_miss",
+	.event = "event=0x24,umask=0x20,period=200000",
+	.desc = "L2 instruction fetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetches",
+	.event = "event=0x24,umask=0x30,period=200000",
+	.desc = "L2 instruction fetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ld_hit",
+	.event = "event=0x24,umask=0x1,period=200000",
+	.desc = "L2 load hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ld_miss",
+	.event = "event=0x24,umask=0x2,period=200000",
+	.desc = "L2 load misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.loads",
+	.event = "event=0x24,umask=0x3,period=200000",
+	.desc = "L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.miss",
+	.event = "event=0x24,umask=0xaa,period=200000",
+	.desc = "All L2 misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetch_hit",
+	.event = "event=0x24,umask=0x40,period=200000",
+	.desc = "L2 prefetch hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetch_miss",
+	.event = "event=0x24,umask=0x80,period=200000",
+	.desc = "L2 prefetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetches",
+	.event = "event=0x24,umask=0xc0,period=200000",
+	.desc = "All L2 prefetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.references",
+	.event = "event=0x24,umask=0xff,period=200000",
+	.desc = "All L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x4,period=200000",
+	.desc = "L2 RFO hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x8,period=200000",
+	.desc = "L2 RFO misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfos",
+	.event = "event=0x24,umask=0xc,period=200000",
+	.desc = "L2 RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.any",
+	.event = "event=0xF0,umask=0x80,period=200000",
+	.desc = "All L2 transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.fill",
+	.event = "event=0xF0,umask=0x20,period=200000",
+	.desc = "L2 fill transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.ifetch",
+	.event = "event=0xF0,umask=0x4,period=200000",
+	.desc = "L2 instruction fetch transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200000",
+	.desc = "L1D writeback to L2 transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.load",
+	.event = "event=0xF0,umask=0x1,period=200000",
+	.desc = "L2 Load transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.prefetch",
+	.event = "event=0xF0,umask=0x8,period=200000",
+	.desc = "L2 prefetch transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.rfo",
+	.event = "event=0xF0,umask=0x2,period=200000",
+	.desc = "L2 RFO transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.wb",
+	.event = "event=0xF0,umask=0x40,period=200000",
+	.desc = "L2 writeback to LLC transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.e_state",
+	.event = "event=0x27,umask=0x40,period=100000",
+	.desc = "L2 demand lock RFOs in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.hit",
+	.event = "event=0x27,umask=0xe0,period=100000",
+	.desc = "All demand L2 lock RFOs that hit the cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.i_state",
+	.event = "event=0x27,umask=0x10,period=100000",
+	.desc = "L2 demand lock RFOs in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.m_state",
+	.event = "event=0x27,umask=0x80,period=100000",
+	.desc = "L2 demand lock RFOs in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.mesi",
+	.event = "event=0x27,umask=0xf0,period=100000",
+	.desc = "All demand L2 lock RFOs",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.s_state",
+	.event = "event=0x27,umask=0x20,period=100000",
+	.desc = "L2 demand lock RFOs in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.hit",
+	.event = "event=0x27,umask=0xe,period=100000",
+	.desc = "All L2 demand store RFOs that hit the cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.i_state",
+	.event = "event=0x27,umask=0x1,period=100000",
+	.desc = "L2 demand store RFOs in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.m_state",
+	.event = "event=0x27,umask=0x8,period=100000",
+	.desc = "L2 demand store RFOs in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.mesi",
+	.event = "event=0x27,umask=0xf,period=100000",
+	.desc = "All L2 demand store RFOs",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.s_state",
+	.event = "event=0x27,umask=0x2,period=100000",
+	.desc = "L2 demand store RFOs in S state",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100000",
+	.desc = "Longest latency cache miss",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=200000",
+	.desc = "Longest latency cache reference",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.loads",
+	.event = "event=0xB,umask=0x1,period=2000000",
+	.desc = "Instructions retired which contains a load (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.stores",
+	.event = "event=0xB,umask=0x2,period=2000000",
+	.desc = "Instructions retired which contains a store (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.hit_lfb",
+	.event = "event=0xCB,umask=0x40,period=200000",
+	.desc = "Retired loads that miss L1D and hit an previously allocated LFB (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l1d_hit",
+	.event = "event=0xCB,umask=0x1,period=2000000",
+	.desc = "Retired loads that hit the L1 data cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l2_hit",
+	.event = "event=0xCB,umask=0x2,period=200000",
+	.desc = "Retired loads that hit the L2 cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.llc_miss",
+	.event = "event=0xCB,umask=0x10,period=10000",
+	.desc = "Retired loads that miss the LLC cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.llc_unshared_hit",
+	.event = "event=0xCB,umask=0x4,period=40000",
+	.desc = "Retired loads that hit valid versions in the LLC cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.other_core_l2_hit_hitm",
+	.event = "event=0xCB,umask=0x8,period=40000",
+	.desc = "Retired loads that hit sibling core's L2 in modified or unmodified states (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.local_dram",
+	.event = "event=0xF,umask=0x20,period=10000",
+	.desc = "Load instructions retired with a data source of local DRAM or locally homed remote hitm (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.other_core_l2_hitm",
+	.event = "event=0xF,umask=0x2,period=40000",
+	.desc = "Load instructions retired that HIT modified data in sibling core (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.remote_cache_local_home_hit",
+	.event = "event=0xF,umask=0x8,period=20000",
+	.desc = "Load instructions retired remote cache HIT data source (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.remote_dram",
+	.event = "event=0xF,umask=0x10,period=10000",
+	.desc = "Load instructions retired remote DRAM and remote home-remote cache HITM (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.uncacheable",
+	.event = "event=0xF,umask=0x80,period=4000",
+	.desc = "Load instructions retired IO (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.l1d_writeback",
+	.event = "event=0xB0,umask=0x40,period=100000",
+	.desc = "Offcore L1 data cache writebacks",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_sq_full",
+	.event = "event=0xB2,umask=0x1,period=100000",
+	.desc = "Offcore requests blocked due to Super Queue full",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xF4,umask=0x10,period=2000000",
+	.desc = "Super Queue lock splits across a cache line",
+	.topic = "cache",
+},
+{
+	.name = "store_blocks.at_ret",
+	.event = "event=0x6,umask=0x4,period=200000",
+	.desc = "Loads delayed with at-Retirement block code",
+	.topic = "cache",
+},
+{
+	.name = "store_blocks.l1d_block",
+	.event = "event=0x6,umask=0x8,period=200000",
+	.desc = "Cacheable loads delayed with L1D block code",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_0",
+	.event = "event=0xB,umask=0x10,period=2000000,ldlat=0x0",
+	.desc = "Memory instructions retired above 0 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_1024",
+	.event = "event=0xB,umask=0x10,period=100,ldlat=0x400",
+	.desc = "Memory instructions retired above 1024 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_128",
+	.event = "event=0xB,umask=0x10,period=1000,ldlat=0x80",
+	.desc = "Memory instructions retired above 128 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_16",
+	.event = "event=0xB,umask=0x10,period=10000,ldlat=0x10",
+	.desc = "Memory instructions retired above 16 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_16384",
+	.event = "event=0xB,umask=0x10,period=5,ldlat=0x4000",
+	.desc = "Memory instructions retired above 16384 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_2048",
+	.event = "event=0xB,umask=0x10,period=50,ldlat=0x800",
+	.desc = "Memory instructions retired above 2048 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_256",
+	.event = "event=0xB,umask=0x10,period=500,ldlat=0x100",
+	.desc = "Memory instructions retired above 256 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_32",
+	.event = "event=0xB,umask=0x10,period=5000,ldlat=0x20",
+	.desc = "Memory instructions retired above 32 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_32768",
+	.event = "event=0xB,umask=0x10,period=3,ldlat=0x8000",
+	.desc = "Memory instructions retired above 32768 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_4",
+	.event = "event=0xB,umask=0x10,period=50000,ldlat=0x4",
+	.desc = "Memory instructions retired above 4 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_4096",
+	.event = "event=0xB,umask=0x10,period=20,ldlat=0x1000",
+	.desc = "Memory instructions retired above 4096 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_512",
+	.event = "event=0xB,umask=0x10,period=200,ldlat=0x200",
+	.desc = "Memory instructions retired above 512 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_64",
+	.event = "event=0xB,umask=0x10,period=2000,ldlat=0x40",
+	.desc = "Memory instructions retired above 64 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_8",
+	.event = "event=0xB,umask=0x10,period=20000,ldlat=0x8",
+	.desc = "Memory instructions retired above 8 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_8192",
+	.event = "event=0xB,umask=0x10,period=10,ldlat=0x2000",
+	.desc = "Memory instructions retired above 8192 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F11",
+	.desc = "Offcore data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF11",
+	.desc = "All offcore data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8011",
+	.desc = "Offcore data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x111",
+	.desc = "Offcore data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x211",
+	.desc = "Offcore data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x411",
+	.desc = "Offcore data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x711",
+	.desc = "Offcore data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4711",
+	.desc = "Offcore data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1811",
+	.desc = "Offcore data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3811",
+	.desc = "Offcore data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1011",
+	.desc = "Offcore data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x811",
+	.desc = "Offcore data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F44",
+	.desc = "Offcore code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF44",
+	.desc = "All offcore code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8044",
+	.desc = "Offcore code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x144",
+	.desc = "Offcore code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x244",
+	.desc = "Offcore code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x444",
+	.desc = "Offcore code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x744",
+	.desc = "Offcore code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4744",
+	.desc = "Offcore code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1844",
+	.desc = "Offcore code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3844",
+	.desc = "Offcore code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1044",
+	.desc = "Offcore code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x844",
+	.desc = "Offcore code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7FFF",
+	.desc = "Offcore requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFFFF",
+	.desc = "All offcore requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x80FF",
+	.desc = "Offcore requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1FF",
+	.desc = "Offcore requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2FF",
+	.desc = "Offcore requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4FF",
+	.desc = "Offcore requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7FF",
+	.desc = "Offcore requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x47FF",
+	.desc = "Offcore requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x18FF",
+	.desc = "Offcore requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x38FF",
+	.desc = "Offcore requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x10FF",
+	.desc = "Offcore requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8FF",
+	.desc = "Offcore requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F22",
+	.desc = "Offcore RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF22",
+	.desc = "All offcore RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8022",
+	.desc = "Offcore RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x122",
+	.desc = "Offcore RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x222",
+	.desc = "Offcore RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x422",
+	.desc = "Offcore RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x722",
+	.desc = "Offcore RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4722",
+	.desc = "Offcore RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1822",
+	.desc = "Offcore RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3822",
+	.desc = "Offcore RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1022",
+	.desc = "Offcore RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x822",
+	.desc = "Offcore RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F08",
+	.desc = "Offcore writebacks to any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF08",
+	.desc = "All offcore writebacks",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8008",
+	.desc = "Offcore writebacks to the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x108",
+	.desc = "Offcore writebacks to the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x408",
+	.desc = "Offcore writebacks to the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x708",
+	.desc = "Offcore writebacks to the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4708",
+	.desc = "Offcore writebacks to the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1808",
+	.desc = "Offcore writebacks to a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3808",
+	.desc = "Offcore writebacks to a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1008",
+	.desc = "Offcore writebacks that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x808",
+	.desc = "Offcore writebacks that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F77",
+	.desc = "Offcore code or data read requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF77",
+	.desc = "All offcore code or data read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8077",
+	.desc = "Offcore code or data read requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x177",
+	.desc = "Offcore code or data read requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x277",
+	.desc = "Offcore code or data read requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x477",
+	.desc = "Offcore code or data read requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x777",
+	.desc = "Offcore code or data read requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4777",
+	.desc = "Offcore code or data read requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1877",
+	.desc = "Offcore code or data read requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3877",
+	.desc = "Offcore code or data read requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1077",
+	.desc = "Offcore code or data read requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x877",
+	.desc = "Offcore code or data read requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F33",
+	.desc = "Offcore request = all data, response = any cache_dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF33",
+	.desc = "Offcore request = all data, response = any location",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8033",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x133",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x233",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x433",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x733",
+	.desc = "Offcore request = all data, response = local cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4733",
+	.desc = "Offcore request = all data, response = local cache or dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1833",
+	.desc = "Offcore request = all data, response = remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3833",
+	.desc = "Offcore request = all data, response = remote cache or dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1033",
+	.desc = "Offcore data reads, RFO's and prefetches that HIT in a remote cache ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x833",
+	.desc = "Offcore data reads, RFO's and prefetches that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F03",
+	.desc = "Offcore demand data requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF03",
+	.desc = "All offcore demand data requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8003",
+	.desc = "Offcore demand data requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x103",
+	.desc = "Offcore demand data requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x203",
+	.desc = "Offcore demand data requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x403",
+	.desc = "Offcore demand data requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x703",
+	.desc = "Offcore demand data requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4703",
+	.desc = "Offcore demand data requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1803",
+	.desc = "Offcore demand data requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3803",
+	.desc = "Offcore demand data requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1003",
+	.desc = "Offcore demand data requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x803",
+	.desc = "Offcore demand data requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F01",
+	.desc = "Offcore demand data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF01",
+	.desc = "All offcore demand data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8001",
+	.desc = "Offcore demand data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x101",
+	.desc = "Offcore demand data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x201",
+	.desc = "Offcore demand data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x401",
+	.desc = "Offcore demand data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x701",
+	.desc = "Offcore demand data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4701",
+	.desc = "Offcore demand data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1801",
+	.desc = "Offcore demand data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3801",
+	.desc = "Offcore demand data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1001",
+	.desc = "Offcore demand data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x801",
+	.desc = "Offcore demand data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F04",
+	.desc = "Offcore demand code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF04",
+	.desc = "All offcore demand code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8004",
+	.desc = "Offcore demand code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x104",
+	.desc = "Offcore demand code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x204",
+	.desc = "Offcore demand code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x404",
+	.desc = "Offcore demand code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x704",
+	.desc = "Offcore demand code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4704",
+	.desc = "Offcore demand code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1804",
+	.desc = "Offcore demand code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3804",
+	.desc = "Offcore demand code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1004",
+	.desc = "Offcore demand code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x804",
+	.desc = "Offcore demand code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F02",
+	.desc = "Offcore demand RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF02",
+	.desc = "All offcore demand RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8002",
+	.desc = "Offcore demand RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x102",
+	.desc = "Offcore demand RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x202",
+	.desc = "Offcore demand RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x402",
+	.desc = "Offcore demand RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x702",
+	.desc = "Offcore demand RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4702",
+	.desc = "Offcore demand RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1802",
+	.desc = "Offcore demand RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3802",
+	.desc = "Offcore demand RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1002",
+	.desc = "Offcore demand RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x802",
+	.desc = "Offcore demand RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F80",
+	.desc = "Offcore other requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF80",
+	.desc = "All offcore other requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8080",
+	.desc = "Offcore other requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x180",
+	.desc = "Offcore other requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x280",
+	.desc = "Offcore other requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x480",
+	.desc = "Offcore other requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x780",
+	.desc = "Offcore other requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4780",
+	.desc = "Offcore other requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1880",
+	.desc = "Offcore other requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3880",
+	.desc = "Offcore other requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1080",
+	.desc = "Offcore other requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x880",
+	.desc = "Offcore other requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F30",
+	.desc = "Offcore prefetch data requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF30",
+	.desc = "All offcore prefetch data requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8030",
+	.desc = "Offcore prefetch data requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x130",
+	.desc = "Offcore prefetch data requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x230",
+	.desc = "Offcore prefetch data requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x430",
+	.desc = "Offcore prefetch data requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x730",
+	.desc = "Offcore prefetch data requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4730",
+	.desc = "Offcore prefetch data requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1830",
+	.desc = "Offcore prefetch data requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3830",
+	.desc = "Offcore prefetch data requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1030",
+	.desc = "Offcore prefetch data requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x830",
+	.desc = "Offcore prefetch data requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F10",
+	.desc = "Offcore prefetch data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF10",
+	.desc = "All offcore prefetch data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8010",
+	.desc = "Offcore prefetch data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x110",
+	.desc = "Offcore prefetch data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x210",
+	.desc = "Offcore prefetch data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x410",
+	.desc = "Offcore prefetch data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x710",
+	.desc = "Offcore prefetch data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4710",
+	.desc = "Offcore prefetch data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1810",
+	.desc = "Offcore prefetch data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3810",
+	.desc = "Offcore prefetch data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1010",
+	.desc = "Offcore prefetch data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x810",
+	.desc = "Offcore prefetch data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F40",
+	.desc = "Offcore prefetch code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF40",
+	.desc = "All offcore prefetch code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8040",
+	.desc = "Offcore prefetch code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x140",
+	.desc = "Offcore prefetch code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x240",
+	.desc = "Offcore prefetch code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x440",
+	.desc = "Offcore prefetch code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x740",
+	.desc = "Offcore prefetch code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4740",
+	.desc = "Offcore prefetch code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1840",
+	.desc = "Offcore prefetch code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3840",
+	.desc = "Offcore prefetch code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1040",
+	.desc = "Offcore prefetch code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x840",
+	.desc = "Offcore prefetch code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F20",
+	.desc = "Offcore prefetch RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF20",
+	.desc = "All offcore prefetch RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8020",
+	.desc = "Offcore prefetch RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x120",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x220",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x420",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x720",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4720",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1820",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3820",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1020",
+	.desc = "Offcore prefetch RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x820",
+	.desc = "Offcore prefetch RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F70",
+	.desc = "Offcore prefetch requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF70",
+	.desc = "All offcore prefetch requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8070",
+	.desc = "Offcore prefetch requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x170",
+	.desc = "Offcore prefetch requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x270",
+	.desc = "Offcore prefetch requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x470",
+	.desc = "Offcore prefetch requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x770",
+	.desc = "Offcore prefetch requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4770",
+	.desc = "Offcore prefetch requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1870",
+	.desc = "Offcore prefetch requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3870",
+	.desc = "Offcore prefetch requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1070",
+	.desc = "Offcore prefetch requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x870",
+	.desc = "Offcore prefetch requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_goldmont[] = {
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=200003",
+	.desc = "Retired branch instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts branch instructions retired for all branch types.  This is an architectural performance event (Must be precise)",
+},
+{
+	.name = "br_inst_retired.jcc",
+	.event = "event=0xC4,umask=0x7e,period=200003",
+	.desc = "Retired conditional branch instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts retired Jcc (Jump on Conditional Code/Jump if Condition is Met) branch instructions retired, including both when the branch was taken and when it was not taken (Must be precise)",
+},
+{
+	.name = "br_inst_retired.taken_jcc",
+	.event = "event=0xC4,umask=0xfe,period=200003",
+	.desc = "Retired conditional branch instructions that were taken (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts Jcc (Jump on Conditional Code/Jump if Condition is Met) branch instructions retired that were taken and does not count when the Jcc branch instruction were not taken (Must be precise)",
+},
+{
+	.name = "br_inst_retired.call",
+	.event = "event=0xC4,umask=0xf9,period=200003",
+	.desc = "Retired near call instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts near CALL branch instructions retired (Must be precise)",
+},
+{
+	.name = "br_inst_retired.rel_call",
+	.event = "event=0xC4,umask=0xfd,period=200003",
+	.desc = "Retired near relative call instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts near relative CALL branch instructions retired (Must be precise)",
+},
+{
+	.name = "br_inst_retired.ind_call",
+	.event = "event=0xC4,umask=0xfb,period=200003",
+	.desc = "Retired near indirect call instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts near indirect CALL branch instructions retired (Must be precise)",
+},
+{
+	.name = "br_inst_retired.return",
+	.event = "event=0xC4,umask=0xf7,period=200003",
+	.desc = "Retired near return instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts near return branch instructions retired (Must be precise)",
+},
+{
+	.name = "br_inst_retired.non_return_ind",
+	.event = "event=0xC4,umask=0xeb,period=200003",
+	.desc = "Retired instructions of near indirect Jmp or call (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts near indirect call or near indirect jmp branch instructions retired (Must be precise)",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0xbf,period=200003",
+	.desc = "Retired far branch instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts far branch instructions retired.  This includes far jump, far call and return, and Interrupt call and return (Must be precise)",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=200003",
+	.desc = "Retired mispredicted branch instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts mispredicted branch instructions retired including all branch types (Must be precise)",
+},
+{
+	.name = "br_misp_retired.jcc",
+	.event = "event=0xC5,umask=0x7e,period=200003",
+	.desc = "Retired mispredicted conditional branch instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts mispredicted retired Jcc (Jump on Conditional Code/Jump if Condition is Met) branch instructions retired, including both when the branch was supposed to be taken and when it was not supposed to be taken (but the processor predicted the opposite condition) (Must be precise)",
+},
+{
+	.name = "br_misp_retired.taken_jcc",
+	.event = "event=0xC5,umask=0xfe,period=200003",
+	.desc = "Retired mispredicted conditional branch instructions that were taken (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts mispredicted retired Jcc (Jump on Conditional Code/Jump if Condition is Met) branch instructions retired that were supposed to be taken but the processor predicted that it would not be taken (Must be precise)",
+},
+{
+	.name = "br_misp_retired.ind_call",
+	.event = "event=0xC5,umask=0xfb,period=200003",
+	.desc = "Retired mispredicted near indirect call instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts mispredicted near indirect CALL branch instructions retired, where the target address taken was not what the processor predicted (Must be precise)",
+},
+{
+	.name = "br_misp_retired.return",
+	.event = "event=0xC5,umask=0xf7,period=200003",
+	.desc = "Retired mispredicted near return instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts mispredicted near RET branch instructions retired, where the return address taken was not what the processor predicted (Must be precise)",
+},
+{
+	.name = "br_misp_retired.non_return_ind",
+	.event = "event=0xC5,umask=0xeb,period=200003",
+	.desc = "Retired mispredicted instructions of near indirect Jmp or near indirect call. (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts mispredicted branch instructions retired that were near indirect call or near indirect jmp, where the target address taken was not what the processor predicted (Must be precise)",
+},
+{
+	.name = "uops_retired.any",
+	.event = "event=0xC2,umask=0x0,period=2000003",
+	.desc = "Uops retired (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts uops which retired (Must be precise)",
+},
+{
+	.name = "uops_retired.ms",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "MS uops retired (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts uops retired that are from the complex flows issued by the micro-sequencer (MS).  Counts both the uops from a micro-coded instruction, and the uops that might be generated from a micro-coded assist (Must be precise)",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x1,period=200003",
+	.desc = "Self-Modifying Code detected",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of times that the processor detects that a program is writing to a code section and has to perform a machine clear because of that modification.  Self-modifying code (SMC) causes a severe penalty in all Intel? architecture processors",
+},
+{
+	.name = "machine_clears.fp_assist",
+	.event = "event=0xC3,umask=0x4,period=200003",
+	.desc = "Machine clears due to FP assists",
+	.topic = "pipeline",
+	.long_desc = "Counts machine clears due to floating point (FP) operations needing assists.  For instance, if the result was a floating point denormal, the hardware clears the pipeline and reissues uops to produce the correct IEEE compliant denormal result",
+},
+{
+	.name = "machine_clears.disambiguation",
+	.event = "event=0xC3,umask=0x8,period=200003",
+	.desc = "Machine clears due to memory disambiguation",
+	.topic = "pipeline",
+	.long_desc = "Counts machine clears due to memory disambiguation.  Memory disambiguation happens when a load which has been issued conflicts with a previous unretired store in the pipeline whose address was not known at issue time, but is later resolved to be the same as the load address",
+},
+{
+	.name = "machine_clears.all",
+	.event = "event=0xC3,umask=0x0,period=200003",
+	.desc = "All machine clears",
+	.topic = "pipeline",
+	.long_desc = "Counts machine clears for any reason",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of instructions that retire execution. For instructions that consist of multiple uops, this event counts the retirement of the last uop of the instruction. The event continues counting during hardware interrupts, traps, and inside interrupt handlers.  This is an architectural performance event.  This event uses a (_P)rogrammable general purpose performance counter. *This event is Precise Event capable:  The EventingRIP field in the PEBS record is precise to the address of the instruction which caused the event.  Note: Because PEBS records can be collected only on IA32_PMC0, only one event can use the PEBS facility at a time (Must be precise)",
+},
+{
+	.name = "uops_not_delivered.any",
+	.event = "event=0x9C,umask=0x0,period=200003",
+	.desc = "Uops requested but not-delivered to the back-end per cycle",
+	.topic = "pipeline",
+	.long_desc = "This event used to measure front-end inefficiencies. I.e. when front-end of the machine is not delivering uops to the back-end and the back-end has is not stalled. This event can be used to identify if the machine is truly front-end bound.  When this event occurs, it is an indication that the front-end of the machine is operating at less than its theoretical peak performance. Background: We can think of the processor pipeline as being divided into 2 broader parts: Front-end and Back-end. Front-end is responsible for fetching the instruction, decoding into uops in machine understandable format and putting them into a uop queue to be consumed by back end. The back-end then takes these uops, allocates the required resources.  When all resources are ready, uops are executed. If the back-end is not ready to accept uops from the front-end, then we do not want to count these as front-end bottlenecks.  However, whenever we have bottlenecks in the back-end, we will have allocation unit stalls and eventually forcing the front-end to wait until the back-end is ready to receive more uops. This event counts only when back-end is requesting more uops and front-end is not able to provide them. When 3 uops are requested and no uops are delivered, the event counts 3. When 3 are requested, and only 1 is delivered, the event counts 2. When only 2 are delivered, the event counts 1. Alternatively stated, the event will not count if 3 uops are delivered, or if the back end is stalled and not requesting any uops at all.  Counts indicate missed opportunities for the front-end to deliver a uop to the back end. Some examples of conditions that cause front-end efficiencies are: ICache misses, ITLB misses, and decoder restrictions that limit the front-end bandwidth. Known Issues: Some uops require multiple allocation slots.  These uops will not be charged as a front end 'not delivered' opportunity, and will be regarded as a back end problem. For example, the INC instruction has one uop that requires 2 issue slots.  A stream of INC instructions will not count as UOPS_NOT_DELIVERED, even though only one instruction can be issued per clock.  The low uop issue rate for a stream of INC instructions is considered to be a back end issue",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0x0E,umask=0x0,period=200003",
+	.desc = "Uops issued to the back end per cycle",
+	.topic = "pipeline",
+	.long_desc = "Counts uops issued by the front end and allocated into the back end of the machine.  This event counts uops that retire as well as uops that were speculatively executed but didn't retire. The sort of speculative uops that might be counted includes, but is not limited to those uops issued in the shadow of a miss-predicted branch, those uops that are inserted during an assist (such as for a denormal floating point result), and (previously allocated) uops that might be canceled during a machine clear",
+},
+{
+	.name = "cycles_div_busy.all",
+	.event = "event=0xCD,umask=0x0,period=2000003",
+	.desc = "Cycles a divider is busy",
+	.topic = "pipeline",
+	.long_desc = "Counts core cycles if either divide unit is busy",
+},
+{
+	.name = "cycles_div_busy.idiv",
+	.event = "event=0xCD,umask=0x1,period=200003",
+	.desc = "Cycles the integer divide unit is busy",
+	.topic = "pipeline",
+	.long_desc = "Counts core cycles the integer divide unit is busy",
+},
+{
+	.name = "cycles_div_busy.fpdiv",
+	.event = "event=0xCD,umask=0x2,period=200003",
+	.desc = "Cycles the FP divide unit is busy",
+	.topic = "pipeline",
+	.long_desc = "Counts core cycles the floating point divide unit is busy",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (Fixed event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of instructions that retire execution. For instructions that consist of multiple uops, this event counts the retirement of the last uop of the instruction. The counter continues counting during hardware interrupts, traps, and inside interrupt handlers.  This event uses fixed counter 0.  You cannot collect a PEBs record for this event",
+},
+{
+	.name = "cpu_clk_unhalted.core",
+	.event = "event=0x00,umask=0x2,period=2000003",
+	.desc = "Core cycles when core is not halted  (Fixed event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of core cycles while the core is not in a halt state.  The core enters the halt state when it is running the HLT instruction. In mobile systems the core frequency may change from time to time. For this reason this event may have a changing ratio with regards to time.  This event uses fixed counter 1.  You cannot collect a PEBs record for this event",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Reference cycles when core is not halted  (Fixed event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of reference cycles that the core is not in a halt state. The core enters the halt state when it is running the HLT instruction.  In mobile systems the core frequency may change from time.  This event is not affected by core frequency changes but counts as if the core is running at the maximum frequency all the time.  This event uses fixed counter 2.  You cannot collect a PEBs record for this event",
+},
+{
+	.name = "cpu_clk_unhalted.core_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Core cycles when core is not halted",
+	.topic = "pipeline",
+	.long_desc = "Core cycles when core is not halted.  This event uses a (_P)rogrammable general purpose performance counter",
+},
+{
+	.name = "cpu_clk_unhalted.ref",
+	.event = "event=0x0,umask=0x03",
+	.desc = "Reference cycles when core is not halted",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when core is not halted.  This event uses a (_P)rogrammable general purpose performance counter",
+},
+{
+	.name = "baclears.all",
+	.event = "event=0xE6,umask=0x1,period=200003",
+	.desc = "BACLEARs asserted for any branch type",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of times a BACLEAR is signaled for any reason, including, but not limited to indirect branch/call,  Jcc (Jump on Conditional Code/Jump if Condition is Met) branch, unconditional branch/call, and returns",
+},
+{
+	.name = "baclears.return",
+	.event = "event=0xE6,umask=0x8,period=200003",
+	.desc = "BACLEARs asserted for return branch",
+	.topic = "pipeline",
+	.long_desc = "Counts BACLEARS on return instructions",
+},
+{
+	.name = "baclears.cond",
+	.event = "event=0xE6,umask=0x10,period=200003",
+	.desc = "BACLEARs asserted for conditional branch",
+	.topic = "pipeline",
+	.long_desc = "Counts BACLEARS on Jcc (Jump on Conditional Code/Jump if Condition is Met) branches",
+},
+{
+	.name = "ld_blocks.all_block",
+	.event = "event=0x03,umask=0x10,period=200003",
+	.desc = "Loads blocked (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts anytime a load that retires is blocked for any reason (Must be precise)",
+},
+{
+	.name = "ld_blocks.utlb_miss",
+	.event = "event=0x03,umask=0x8,period=200003",
+	.desc = "Loads blocked because address in not in the UTLB (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts loads blocked because they are unable to find their physical address in the micro TLB (UTLB) (Must be precise)",
+},
+{
+	.name = "ld_blocks.store_forward",
+	.event = "event=0x03,umask=0x2,period=200003",
+	.desc = "Loads blocked due to store forward restriction (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts a load blocked from using a store forward because of an address/size mismatch, only one of the loads blocked from each store will be counted (Must be precise)",
+},
+{
+	.name = "ld_blocks.data_unknown",
+	.event = "event=0x03,umask=0x1,period=200003",
+	.desc = "Loads blocked due to store data not ready (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts a load blocked from using a store forward, but did not occur because the store data was not available at the right time.  The forward might occur subsequently when the data is available (Must be precise)",
+},
+{
+	.name = "ld_blocks.4k_alias",
+	.event = "event=0x03,umask=0x4,period=200003",
+	.desc = "Loads blocked because address has 4k partial address false dependence (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts loads that block because their address modulo 4K matches a pending store (Must be precise)",
+},
+{
+	.name = "br_inst_retired.all_taken_branches",
+	.event = "event=0xC4,umask=0x80,period=200003",
+	.desc = "Retired taken branch instructions (Precise event capable) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of taken branch instructions retired (Must be precise)",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=200003",
+	.desc = "References per ICache line that are available in the ICache (hit). This event counts differently than Intel processors based on Silvermont microarchitecture",
+	.topic = "frontend",
+	.long_desc = "Counts requests to the Instruction Cache (ICache) for one or more bytes in an ICache Line and that cache line is in the ICache (hit).  The event strives to count on a cache line basis, so that multiple accesses which hit in a single cache line count as one ICACHE.HIT.  Specifically, the event counts when straight line code crosses the cache line boundary, or when a branch target is to a new line, and that cache line is in the ICache. This event counts differently than Intel processors based on Silvermont microarchitecture",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "References per ICache line that are not available in the ICache (miss). This event counts differently than Intel processors based on Silvermont microarchitecture",
+	.topic = "frontend",
+	.long_desc = "Counts requests to the Instruction Cache (ICache)  for one or more bytes in an ICache Line and that cache line is not in the ICache (miss).  The event strives to count on a cache line basis, so that multiple accesses which miss in a single cache line count as one ICACHE.MISS.  Specifically, the event counts when straight line code crosses the cache line boundary, or when a branch target is to a new line, and that cache line is not in the ICache. This event counts differently than Intel processors based on Silvermont microarchitecture",
+},
+{
+	.name = "icache.accesses",
+	.event = "event=0x80,umask=0x3,period=200003",
+	.desc = "References per ICache line. This event counts differently than Intel processors based on Silvermont microarchitecture",
+	.topic = "frontend",
+	.long_desc = "Counts requests to the Instruction Cache (ICache) for one or more bytes in an ICache Line.  The event strives to count on a cache line basis, so that multiple fetches to a single cache line count as one ICACHE.ACCESS.  Specifically, the event counts when accesses from straight line code crosses the cache line boundary, or when a branch target is to a new line.\r\nThis event counts differently than Intel processors based on Silvermont microarchitecture",
+},
+{
+	.name = "ms_decoded.ms_entry",
+	.event = "event=0xE7,umask=0x1,period=200003",
+	.desc = "MS decode starts",
+	.topic = "frontend",
+	.long_desc = "Counts the number of times the Microcode Sequencer (MS) starts a flow of uops from the MSROM. It does not count every time a uop is read from the MSROM.  The most common case that this counts is when a micro-coded instruction is encountered by the front end of the machine.  Other cases include when an instruction encounters a fault, trap, or microcode assist of any sort that initiates a flow of uops.  The event will count MS startups for uops that are speculative, and subsequently cleared by branch mispredict or a machine clear",
+},
+{
+	.name = "decode_restriction.predecode_wrong",
+	.event = "event=0xE9,umask=0x1,period=200003",
+	.desc = "Decode restrictions due to predicting wrong instruction length",
+	.topic = "frontend",
+	.long_desc = "Counts the number of times the prediction (from the predecode cache) for instruction length is incorrect",
+},
+{
+	.name = "issue_slots_not_consumed.resource_full",
+	.event = "event=0xCA,umask=0x1,period=200003",
+	.desc = "Unfilled issue slots per cycle because of a full resource in the backend",
+	.topic = "other",
+	.long_desc = "Counts the number of issue slots per core cycle that were not consumed because of a full resource in the backend.  Including but not limited to resources such as the Re-order Buffer (ROB), reservation stations (RS), load/store buffers, physical registers, or any other needed machine resource that is currently unavailable.   Note that uops must be available for consumption in order for this event to fire.  If a uop is not available (Instruction Queue is empty), this event will not count",
+},
+{
+	.name = "issue_slots_not_consumed.recovery",
+	.event = "event=0xCA,umask=0x2,period=200003",
+	.desc = "Unfilled issue slots per cycle to recover",
+	.topic = "other",
+	.long_desc = "Counts the number of issue slots per core cycle that were not consumed by the backend because allocation is stalled waiting for a mispredicted jump to retire or other branch-like conditions (e.g. the event is relevant during certain microcode flows).   Counts all issue slots blocked while within this window including slots where uops were not available in the Instruction Queue",
+},
+{
+	.name = "issue_slots_not_consumed.any",
+	.event = "event=0xCA,umask=0x0,period=200003",
+	.desc = "Unfilled issue slots per cycle",
+	.topic = "other",
+	.long_desc = "Counts the number of issue slots per core cycle that were not consumed by the backend due to either a full resource  in the backend (RESOURCE_FULL) or due to the processor recovering from some event (RECOVERY)",
+},
+{
+	.name = "hw_interrupts.received",
+	.event = "event=0xCB,umask=0x1,period=200003",
+	.desc = "Hardware interrupts received (Precise event capable)",
+	.topic = "other",
+	.long_desc = "Counts hardware interrupts received by the processor",
+},
+{
+	.name = "hw_interrupts.pending_and_masked",
+	.event = "event=0xCB,umask=0x4,period=200003",
+	.desc = "Cycles pending interrupts are masked (Precise event capable)",
+	.topic = "other",
+	.long_desc = "Counts core cycles during which there are pending interrupts, but interrupts are masked (EFLAGS.IF = 0)",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=200003",
+	.desc = "Machine clears due to memory ordering issue",
+	.topic = "memory",
+	.long_desc = "Counts machine clears due to memory ordering issues.  This occurs when a snoop request happens and the machine is uncertain if memory ordering will be preserved - as another core is in the process of modifying the data",
+},
+{
+	.name = "misalign_mem_ref.load_page_split",
+	.event = "event=0x13,umask=0x2,period=200003",
+	.desc = "Load uops that split a page (Precise event capable) (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Counts when a memory load of a uop spans a page boundary (a split) is retired (Must be precise)",
+},
+{
+	.name = "misalign_mem_ref.store_page_split",
+	.event = "event=0x13,umask=0x4,period=200003",
+	.desc = "Store uops that split a page (Precise event capable) (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Counts when a memory store of a uop spans a page boundary (a split) is retired (Must be precise)",
+},
+{
+	.name = "itlb.miss",
+	.event = "event=0x81,umask=0x4,period=200003",
+	.desc = "ITLB misses",
+	.topic = "virtual memory",
+	.long_desc = "Counts the number of times the machine was unable to find a translation in the Instruction Translation Lookaside Buffer (ITLB) for a linear address of an instruction fetch.  It counts when new translation are filled into the ITLB.  The event is speculative in nature, but will not count translations (page walks) that are begun and not finished, or translations that are finished but not filled into the ITLB",
+},
+{
+	.name = "mem_uops_retired.dtlb_miss_loads",
+	.event = "event=0xD0,umask=0x11,period=200003",
+	.desc = "Load uops retired that missed the DTLB (Precise event capable) (Must be precise)",
+	.topic = "virtual memory",
+	.long_desc = "Counts load uops retired that caused a DTLB miss (Must be precise)",
+},
+{
+	.name = "mem_uops_retired.dtlb_miss_stores",
+	.event = "event=0xD0,umask=0x12,period=200003",
+	.desc = "Store uops retired that missed the DTLB (Precise event capable) (Must be precise)",
+	.topic = "virtual memory",
+	.long_desc = "Counts store uops retired that caused a DTLB miss (Must be precise)",
+},
+{
+	.name = "mem_uops_retired.dtlb_miss",
+	.event = "event=0xD0,umask=0x13,period=200003",
+	.desc = "Memory uops retired that missed the DTLB (Precise event capable) (Must be precise)",
+	.topic = "virtual memory",
+	.long_desc = "Counts uops retired that had a DTLB miss on load, store or either.  Note that when two distinct memory operations to the same page miss the DTLB, only one of them will be recorded as a DTLB miss (Must be precise)",
+},
+{
+	.name = "page_walks.d_side_cycles",
+	.event = "event=0x05,umask=0x1,period=200003",
+	.desc = "Duration of D-side page-walks in cycles",
+	.topic = "virtual memory",
+	.long_desc = "Counts every core cycle when a Data-side (walks due to a data operation) page walk is in progress",
+},
+{
+	.name = "page_walks.i_side_cycles",
+	.event = "event=0x05,umask=0x2,period=200003",
+	.desc = "Duration of I-side pagewalks in cycles",
+	.topic = "virtual memory",
+	.long_desc = "Counts every core cycle when a Instruction-side (walks due to an instruction fetch) page walk is in progress",
+},
+{
+	.name = "page_walks.cycles",
+	.event = "event=0x05,umask=0x3,period=200003",
+	.desc = "Duration of page-walks in cycles",
+	.topic = "virtual memory",
+	.long_desc = "Counts every core cycle a page-walk is in progress due to either a data memory operation or an instruction fetch",
+},
+{
+	.name = "l2_reject_xq.all",
+	.event = "event=0x30,umask=0x0,period=200003",
+	.desc = "Requests rejected by the XQ",
+	.topic = "cache",
+	.long_desc = "Counts the number of demand and prefetch transactions that the L2 XQ rejects due to a full or near full condition which likely indicates back pressure from the intra-die interconnect (IDI) fabric. The XQ may reject transactions from the L2Q (non-cacheable requests), L2 misses and L2 write-back victims",
+},
+{
+	.name = "core_reject_l2q.all",
+	.event = "event=0x31,umask=0x0,period=200003",
+	.desc = "Requests rejected by the L2Q ",
+	.topic = "cache",
+	.long_desc = "Counts the number of demand and L1 prefetcher requests rejected by the L2Q due to a full or nearly full condition which likely indicates back pressure from L2Q. It also counts requests that would have gone directly to the XQ, but are rejected due to a full or nearly full condition, indicating back pressure from the IDI link. The L2Q may also reject transactions from a core to insure fairness between cores, or to delay a core's dirty eviction when the address conflicts with incoming external snoops",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=200003",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+	.long_desc = "Counts memory requests originating from the core that reference a cache line in the L2 cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=200003",
+	.desc = "L2 cache request misses",
+	.topic = "cache",
+	.long_desc = "Counts memory requests originating from the core that miss in the L2 cache",
+},
+{
+	.name = "fetch_stall.icache_fill_pending_cycles",
+	.event = "event=0x86,umask=0x2,period=200003",
+	.desc = "Cycles where code-fetch is stalled and an ICache miss is outstanding.  This is not the same as an ICache Miss",
+	.topic = "cache",
+	.long_desc = "Counts cycles that an ICache miss is outstanding, and instruction fetch is stalled.  That is, the decoder queue is able to accept bytes, but the fetch unit is unable to provide bytes, while an Icache miss outstanding.  Note this event is not the same as cycles to retrieve an instruction due to an Icache miss.  Rather, it is the part of the Instruction Cache (ICache) miss time where no bytes are available for the decoder",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0xD0,umask=0x81,period=200003",
+	.desc = "Load uops retired (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts the number of load uops retired (Must be precise)",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0xD0,umask=0x82,period=200003",
+	.desc = "Store uops retired (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts the number of store uops retired (Must be precise)",
+},
+{
+	.name = "mem_uops_retired.all",
+	.event = "event=0xD0,umask=0x83,period=200003",
+	.desc = "Memory uops retired (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts the number of memory uops retired that is either a loads or a store or both (Must be precise)",
+},
+{
+	.name = "mem_uops_retired.lock_loads",
+	.event = "event=0xD0,umask=0x21,period=200003",
+	.desc = "Locked load uops retired (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts locked memory uops retired.  This includes \"regular\" locks and bus locks. (To specifically count bus locks only, see the Offcore response event.)  A locked access is one with a lock prefix, or an exchange to memory.  See the SDM for a complete description of which memory load accesses are locks (Must be precise)",
+},
+{
+	.name = "mem_uops_retired.split_loads",
+	.event = "event=0xD0,umask=0x41,period=200003",
+	.desc = "Load uops retired that split a cache-line (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts load uops retired where the data requested spans a 64 byte cache line boundary (Must be precise)",
+},
+{
+	.name = "mem_uops_retired.split_stores",
+	.event = "event=0xD0,umask=0x42,period=200003",
+	.desc = "Stores uops retired that split a cache-line (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts store uops retired where the data requested spans a 64 byte cache line boundary (Must be precise)",
+},
+{
+	.name = "mem_uops_retired.split",
+	.event = "event=0xD0,umask=0x43,period=200003",
+	.desc = "Memory uops retired that split a cache-line (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts memory uops retired where the data requested spans a 64 byte cache line boundary (Must be precise)",
+},
+{
+	.name = "mem_load_uops_retired.l1_hit",
+	.event = "event=0xD1,umask=0x1,period=200003",
+	.desc = "Load uops retired that hit L1 data cache (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts load uops retired that hit the L1 data cache (Must be precise)",
+},
+{
+	.name = "mem_load_uops_retired.l1_miss",
+	.event = "event=0xD1,umask=0x8,period=200003",
+	.desc = "Load uops retired that missed L1 data cache (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts load uops retired that miss the L1 data cache (Must be precise)",
+},
+{
+	.name = "mem_load_uops_retired.l2_hit",
+	.event = "event=0xD1,umask=0x2,period=200003",
+	.desc = "Load uops retired that hit L2 (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts load uops retired that hit in the L2 cache (Must be precise)",
+},
+{
+	.name = "mem_load_uops_retired.l2_miss",
+	.event = "event=0xD1,umask=0x10,period=200003",
+	.desc = "Load uops retired that missed L2 (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts load uops retired that miss in the L2 cache (Must be precise)",
+},
+{
+	.name = "mem_load_uops_retired.hitm",
+	.event = "event=0xD1,umask=0x20,period=200003",
+	.desc = "Memory uop retired where cross core or cross module HITM occurred (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts load uops retired where the cache line containing the data was in the modified state of another core or modules cache (HITM).  More specifically, this means that when the load address was checked by other caching agents (typically another processor) in the system, one of those caching agents indicated that they had a dirty copy of the data.  Loads that obtain a HITM response incur greater latency than most is typical for a load.  In addition, since HITM indicates that some other processor had this data in its cache, it implies that the data was shared between processors, or potentially was a lock or semaphore value.  This event is useful for locating sharing, false sharing, and contended locks (Must be precise)",
+},
+{
+	.name = "mem_load_uops_retired.wcb_hit",
+	.event = "event=0xD1,umask=0x40,period=200003",
+	.desc = "Loads retired that hit WCB (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts memory load uops retired where the data is retrieved from the WCB (or fill buffer), indicating that the load found its data while that data was in the process of being brought into the L1 cache.  Typically a load will receive this indication when some other load or prefetch missed the L1 cache and was in the process of retrieving the cache line containing the data, but that process had not yet finished (and written the data back to the cache). For example, consider load X and Y, both referencing the same cache line that is not in the L1 cache.  If load X misses cache first, it obtains and WCB (or fill buffer) and begins the process of requesting the data.  When load Y requests the data, it will either hit the WCB, or the L1 cache, depending on exactly what time the request to Y occurs (Must be precise)",
+},
+{
+	.name = "mem_load_uops_retired.dram_hit",
+	.event = "event=0xD1,umask=0x80,period=200003",
+	.desc = "Loads retired that came from DRAM (Precise event capable) (Must be precise)",
+	.topic = "cache",
+	.long_desc = "Counts memory load uops retired where the data is retrieved from DRAM.  Event is counted at retirement, so the speculative loads are ignored.  A memory load can hit (or miss) the L1 cache, hit (or miss) the L2 cache, hit DRAM, hit in the WCB or receive a HITM response (Must be precise)",
+},
+{
+	.name = "dl1.dirty_eviction",
+	.event = "event=0x51,umask=0x1,period=200003",
+	.desc = "L1 Cache evictions for dirty data",
+	.topic = "cache",
+	.long_desc = "Counts when a modified (dirty) cache line is evicted from the data L1 cache and needs to be written back to memory.  No count will occur if the evicted line is clean, and hence does not require a writeback",
+},
+{
+	.name = "offcore_response",
+	.event = "event=0xB7,umask=0x1,period=100007",
+	.desc = "Requires MSR_OFFCORE_RESP[0,1] to specify request type and response. (duplicated for both MSRs)",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x36000032b7 ",
+	.desc = "Counts data read, code read, and read for ownership (RFO) requests (demand & prefetch) that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x10000032b7 ",
+	.desc = "Counts data read, code read, and read for ownership (RFO) requests (demand & prefetch) that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x04000032b7 ",
+	.desc = "Counts data read, code read, and read for ownership (RFO) requests (demand & prefetch) that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x02000032b7 ",
+	.desc = "Counts data read, code read, and read for ownership (RFO) requests (demand & prefetch) that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x00000432b7 ",
+	.desc = "Counts data read, code read, and read for ownership (RFO) requests (demand & prefetch) that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600000022 ",
+	.desc = "Counts reads for ownership (RFO) requests (demand & prefetch) that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000022 ",
+	.desc = "Counts reads for ownership (RFO) requests (demand & prefetch) that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000022 ",
+	.desc = "Counts reads for ownership (RFO) requests (demand & prefetch) that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000022 ",
+	.desc = "Counts reads for ownership (RFO) requests (demand & prefetch) that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000040022 ",
+	.desc = "Counts reads for ownership (RFO) requests (demand & prefetch) that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600003091",
+	.desc = "Counts data reads (demand & prefetch) that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000003091",
+	.desc = "Counts data reads (demand & prefetch) that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400003091",
+	.desc = "Counts data reads (demand & prefetch) that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200003091",
+	.desc = "Counts data reads (demand & prefetch) that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000043091",
+	.desc = "Counts data reads (demand & prefetch) that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_data_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600003010 ",
+	.desc = "Counts data reads generated by L1 or L2 prefetchers that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_data_rd.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000003010 ",
+	.desc = "Counts data reads generated by L1 or L2 prefetchers that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_data_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400003010 ",
+	.desc = "Counts data reads generated by L1 or L2 prefetchers that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_data_rd.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200003010 ",
+	.desc = "Counts data reads generated by L1 or L2 prefetchers that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_data_rd.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000043010 ",
+	.desc = "Counts data reads generated by L1 or L2 prefetchers that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000008000 ",
+	.desc = "Counts requests to the uncore subsystem that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400008000 ",
+	.desc = "Counts requests to the uncore subsystem that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200008000 ",
+	.desc = "Counts requests to the uncore subsystem that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000048000 ",
+	.desc = "Counts requests to the uncore subsystem that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000018000 ",
+	.desc = "Counts requests to the uncore subsystem that have any transaction responses from the uncore subsystem",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600004800 ",
+	.desc = "Counts any data writes to uncacheable write combining (USWC) memory region  that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000044800 ",
+	.desc = "Counts any data writes to uncacheable write combining (USWC) memory region  that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_streaming_stores.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600004000 ",
+	.desc = "Counts partial cache line data writes to uncacheable write combining (USWC) memory region  that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_streaming_stores.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000004000 ",
+	.desc = "Counts partial cache line data writes to uncacheable write combining (USWC) memory region  that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_streaming_stores.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400004000 ",
+	.desc = "Counts partial cache line data writes to uncacheable write combining (USWC) memory region  that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_streaming_stores.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200004000 ",
+	.desc = "Counts partial cache line data writes to uncacheable write combining (USWC) memory region  that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_streaming_stores.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000044000 ",
+	.desc = "Counts partial cache line data writes to uncacheable write combining (USWC) memory region  that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600002000 ",
+	.desc = "Counts data cache line reads generated by hardware L1 data cache prefetcher that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000002000 ",
+	.desc = "Counts data cache line reads generated by hardware L1 data cache prefetcher that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400002000 ",
+	.desc = "Counts data cache line reads generated by hardware L1 data cache prefetcher that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200002000 ",
+	.desc = "Counts data cache line reads generated by hardware L1 data cache prefetcher that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000042000 ",
+	.desc = "Counts data cache line reads generated by hardware L1 data cache prefetcher that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.sw_prefetch.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600001000 ",
+	.desc = "Counts data cache lines requests by software prefetch instructions that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.sw_prefetch.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000001000 ",
+	.desc = "Counts data cache lines requests by software prefetch instructions that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.sw_prefetch.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400001000 ",
+	.desc = "Counts data cache lines requests by software prefetch instructions that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.sw_prefetch.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200001000 ",
+	.desc = "Counts data cache lines requests by software prefetch instructions that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.sw_prefetch.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000041000 ",
+	.desc = "Counts data cache lines requests by software prefetch instructions that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.full_streaming_stores.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600000800 ",
+	.desc = "Counts full cache line data writes to uncacheable write combining (USWC) memory region and full cache-line non-temporal writes that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.full_streaming_stores.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000800 ",
+	.desc = "Counts full cache line data writes to uncacheable write combining (USWC) memory region and full cache-line non-temporal writes that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.full_streaming_stores.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000800 ",
+	.desc = "Counts full cache line data writes to uncacheable write combining (USWC) memory region and full cache-line non-temporal writes that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.full_streaming_stores.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000800 ",
+	.desc = "Counts full cache line data writes to uncacheable write combining (USWC) memory region and full cache-line non-temporal writes that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.full_streaming_stores.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000040800 ",
+	.desc = "Counts full cache line data writes to uncacheable write combining (USWC) memory region and full cache-line non-temporal writes that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010400 ",
+	.desc = "Counts bus lock and split lock requests that have any transaction responses from the uncore subsystem",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600000100 ",
+	.desc = "Counts the number of demand write requests (RFO) generated by a write to partial data cache line, including the writes to uncacheable (UC) and write through (WT), and write protected (WP) types of memory that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600000080 ",
+	.desc = "Counts demand data partial reads, including data in uncacheable (UC) or uncacheable write combining (USWC) memory types that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600000020 ",
+	.desc = "Counts reads for ownership (RFO) requests generated by L2 prefetcher that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000020 ",
+	.desc = "Counts reads for ownership (RFO) requests generated by L2 prefetcher that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000020 ",
+	.desc = "Counts reads for ownership (RFO) requests generated by L2 prefetcher that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000020 ",
+	.desc = "Counts reads for ownership (RFO) requests generated by L2 prefetcher that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000040020 ",
+	.desc = "Counts reads for ownership (RFO) requests generated by L2 prefetcher that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600000010 ",
+	.desc = "Counts data cacheline reads generated by hardware L2 cache prefetcher that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000010 ",
+	.desc = "Counts data cacheline reads generated by hardware L2 cache prefetcher that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000010 ",
+	.desc = "Counts data cacheline reads generated by hardware L2 cache prefetcher that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000010 ",
+	.desc = "Counts data cacheline reads generated by hardware L2 cache prefetcher that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000040010 ",
+	.desc = "Counts data cacheline reads generated by hardware L2 cache prefetcher that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600000008 ",
+	.desc = "Counts the number of writeback transactions caused by L1 or L2 cache evictions that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000008 ",
+	.desc = "Counts the number of writeback transactions caused by L1 or L2 cache evictions that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000008 ",
+	.desc = "Counts the number of writeback transactions caused by L1 or L2 cache evictions that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000008 ",
+	.desc = "Counts the number of writeback transactions caused by L1 or L2 cache evictions that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000040008 ",
+	.desc = "Counts the number of writeback transactions caused by L1 or L2 cache evictions that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000004 ",
+	.desc = "Counts demand instruction cacheline and I-side prefetch requests that miss the instruction cache that are outstanding, per cycle, from the time of the L2 miss to when any response is received",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600000004 ",
+	.desc = "Counts demand instruction cacheline and I-side prefetch requests that miss the instruction cache that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000004 ",
+	.desc = "Counts demand instruction cacheline and I-side prefetch requests that miss the instruction cache that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000004 ",
+	.desc = "Counts demand instruction cacheline and I-side prefetch requests that miss the instruction cache that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000040004 ",
+	.desc = "Counts demand instruction cacheline and I-side prefetch requests that miss the instruction cache that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000002 ",
+	.desc = "Counts demand reads for ownership (RFO) requests generated by a write to full data cache line that are outstanding, per cycle, from the time of the L2 miss to when any response is received",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600000002 ",
+	.desc = "Counts demand reads for ownership (RFO) requests generated by a write to full data cache line that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000002 ",
+	.desc = "Counts demand reads for ownership (RFO) requests generated by a write to full data cache line that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000002 ",
+	.desc = "Counts demand reads for ownership (RFO) requests generated by a write to full data cache line that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000002 ",
+	.desc = "Counts demand reads for ownership (RFO) requests generated by a write to full data cache line that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000040002 ",
+	.desc = "Counts demand reads for ownership (RFO) requests generated by a write to full data cache line that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000001 ",
+	.desc = "Counts demand cacheable data reads of full cache lines that are outstanding, per cycle, from the time of the L2 miss to when any response is received",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x3600000001 ",
+	.desc = "Counts demand cacheable data reads of full cache lines that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000001 ",
+	.desc = "Counts demand cacheable data reads of full cache lines that miss the L2 cache with a snoop hit in the other processor module, data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000001 ",
+	.desc = "Counts demand cacheable data reads of full cache lines that miss the L2 cache with a snoop hit in the other processor module, no data forwarding is required",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_miss.snoop_miss_or_no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000001 ",
+	.desc = "Counts demand cacheable data reads of full cache lines that true miss for the L2 cache with a snoop miss in the other processor module",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_hit",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000040001 ",
+	.desc = "Counts demand cacheable data reads of full cache lines that hit the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_ivybridge[] = {
+{
+	.name = "fp_comp_ops_exe.x87",
+	.event = "event=0x10,umask=0x1,period=2000003",
+	.desc = "Number of FP Computational Uops Executed this cycle. The number of FADD, FSUB, FCOM, FMULs, integer MULsand IMULs, FDIVs, FPREMs, FSQRTS, integer DIVs, and IDIVs. This event does not distinguish an FADD used in the middle of a transcendental flow from a s",
+	.topic = "floating point",
+	.long_desc = "Counts number of X87 uops executed",
+},
+{
+	.name = "fp_comp_ops_exe.sse_packed_double",
+	.event = "event=0x10,umask=0x10,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational packed double-precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Number of SSE* or AVX-128 FP Computational packed double-precision uops issued this cycle",
+},
+{
+	.name = "fp_comp_ops_exe.sse_scalar_single",
+	.event = "event=0x10,umask=0x20,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational scalar single-precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Number of SSE* or AVX-128 FP Computational scalar single-precision uops issued this cycle",
+},
+{
+	.name = "fp_comp_ops_exe.sse_packed_single",
+	.event = "event=0x10,umask=0x40,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational packed single-precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Number of SSE* or AVX-128 FP Computational packed single-precision uops issued this cycle",
+},
+{
+	.name = "fp_comp_ops_exe.sse_scalar_double",
+	.event = "event=0x10,umask=0x80,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational scalar double-precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Counts number of SSE* or AVX-128 double precision FP scalar uops executed",
+},
+{
+	.name = "simd_fp_256.packed_single",
+	.event = "event=0x11,umask=0x1,period=2000003",
+	.desc = "number of GSSE-256 Computational FP single precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Counts 256-bit packed single-precision floating-point instructions",
+},
+{
+	.name = "simd_fp_256.packed_double",
+	.event = "event=0x11,umask=0x2,period=2000003",
+	.desc = "number of AVX-256 Computational FP double precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Counts 256-bit packed double-precision floating-point instructions",
+},
+{
+	.name = "other_assists.avx_store",
+	.event = "event=0xC1,umask=0x8,period=100003",
+	.desc = "Number of GSSE memory assist for stores. GSSE microcode assist is being invoked whenever the hardware is unable to properly handle GSSE-256b operations",
+	.topic = "floating point",
+	.long_desc = "Number of assists associated with 256-bit AVX store operations",
+},
+{
+	.name = "other_assists.avx_to_sse",
+	.event = "event=0xC1,umask=0x10,period=100003",
+	.desc = "Number of transitions from AVX-256 to legacy SSE when penalty applicable",
+	.topic = "floating point",
+},
+{
+	.name = "other_assists.sse_to_avx",
+	.event = "event=0xC1,umask=0x20,period=100003",
+	.desc = "Number of transitions from SSE to AVX-256 when penalty applicable",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.x87_output",
+	.event = "event=0xCA,umask=0x2,period=100003",
+	.desc = "Number of X87 assists due to output value",
+	.topic = "floating point",
+	.long_desc = "Number of X87 FP assists due to output values",
+},
+{
+	.name = "fp_assist.x87_input",
+	.event = "event=0xCA,umask=0x4,period=100003",
+	.desc = "Number of X87 assists due to input value",
+	.topic = "floating point",
+	.long_desc = "Number of X87 FP assists due to input values",
+},
+{
+	.name = "fp_assist.simd_output",
+	.event = "event=0xCA,umask=0x8,period=100003",
+	.desc = "Number of SIMD FP assists due to Output values",
+	.topic = "floating point",
+	.long_desc = "Number of SIMD FP assists due to output values",
+},
+{
+	.name = "fp_assist.simd_input",
+	.event = "event=0xCA,umask=0x10,period=100003",
+	.desc = "Number of SIMD FP assists due to input values",
+	.topic = "floating point",
+	.long_desc = "Number of SIMD FP assists due to input values",
+},
+{
+	.name = "fp_assist.any",
+	.event = "event=0xCA,umask=0x1e,period=100003,cmask=1",
+	.desc = "Cycles with any input/output SSE or FP assist",
+	.topic = "floating point",
+	.long_desc = "Cycles with any input/output SSE* or FP assists",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired from execution",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Core cycles when the thread is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Reference cycles when the core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks.store_forward",
+	.event = "event=0x03,umask=0x2,period=100003",
+	.desc = "Cases when loads get true Block-on-Store blocking code preventing store forwarding",
+	.topic = "pipeline",
+	.long_desc = "Loads blocked by overlapping with store buffer that cannot be forwarded",
+},
+{
+	.name = "ld_blocks.no_sr",
+	.event = "event=0x03,umask=0x8,period=100003",
+	.desc = "This event counts the number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+	.topic = "pipeline",
+	.long_desc = "The number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+},
+{
+	.name = "ld_blocks_partial.address_alias",
+	.event = "event=0x07,umask=0x1,period=100003",
+	.desc = "False dependencies in MOB due to partial compare on address",
+	.topic = "pipeline",
+	.long_desc = "False dependencies in MOB due to partial compare on address",
+},
+{
+	.name = "int_misc.recovery_cycles",
+	.event = "event=0x0D,umask=0x3,period=2000003,cmask=1",
+	.desc = "Number of cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc.)",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_stalls_count",
+	.event = "event=0x0D,umask=0x3,edge=1,period=2000003,cmask=1",
+	.desc = "Number of occurences waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc.)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0x0E,umask=0x1,period=2000003",
+	.desc = "Uops that Resource Allocation Table (RAT) issues to Reservation Station (RS)",
+	.topic = "pipeline",
+	.long_desc = "Increments each cycle the # of Uops issued by the RAT to RS. Set Cmask = 1, Inv = 1, Any= 1to count stalled cycles of this core",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+},
+{
+	.name = "uops_issued.core_stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for all threads",
+	.topic = "pipeline",
+	.long_desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for all threads",
+},
+{
+	.name = "uops_issued.flags_merge",
+	.event = "event=0x0E,umask=0x10,period=2000003",
+	.desc = "Number of flags-merge uops being allocated",
+	.topic = "pipeline",
+	.long_desc = "Number of flags-merge uops allocated. Such uops adds delay",
+},
+{
+	.name = "uops_issued.slow_lea",
+	.event = "event=0x0E,umask=0x20,period=2000003",
+	.desc = "Number of slow LEA uops being allocated. A uop is generally considered SlowLea if it has 3 sources (e.g. 2 sources + immediate) regardless if as a result of LEA instruction or not",
+	.topic = "pipeline",
+	.long_desc = "Number of slow LEA or similar uops allocated. Such uop has 3 sources (e.g. 2 sources + immediate) regardless if as a result of LEA instruction or not",
+},
+{
+	.name = "uops_issued.single_mul",
+	.event = "event=0x0E,umask=0x40,period=2000003",
+	.desc = "Number of Multiply packed/scalar single precision uops allocated",
+	.topic = "pipeline",
+	.long_desc = "Number of multiply packed/scalar single precision uops allocated",
+},
+{
+	.name = "arith.fpu_div_active",
+	.event = "event=0x14,umask=0x1,period=2000003",
+	.desc = "Cycles when divider is busy executing divide operations",
+	.topic = "pipeline",
+	.long_desc = "Cycles that the divider is active, includes INT and FP. Set 'edge =1, cmask=1' to count the number of divides",
+},
+{
+	.name = "arith.fpu_div",
+	.event = "event=0x14,umask=0x4,edge=1,period=100003,cmask=1",
+	.desc = "Divide operations executed",
+	.topic = "pipeline",
+	.long_desc = "Divide operations executed",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Thread cycles when thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of thread cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. The core frequency may change from time to time due to power or thermal throttling",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Increments at the frequency of XCLK (100 MHz) when not halted",
+},
+{
+	.name = "cpu_clk_thread_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.sw_pf",
+	.event = "event=0x4C,umask=0x1,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for software prefetch",
+	.topic = "pipeline",
+	.long_desc = "Non-SW-prefetch load dispatches that hit fill buffer allocated for S/W prefetch",
+},
+{
+	.name = "load_hit_pre.hw_pf",
+	.event = "event=0x4C,umask=0x2,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for hardware prefetch",
+	.topic = "pipeline",
+	.long_desc = "Non-SW-prefetch load dispatches that hit fill buffer allocated for H/W prefetch",
+},
+{
+	.name = "move_elimination.int_not_eliminated",
+	.event = "event=0x58,umask=0x4,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.simd_not_eliminated",
+	.event = "event=0x58,umask=0x8,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.int_eliminated",
+	.event = "event=0x58,umask=0x1,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.simd_eliminated",
+	.event = "event=0x58,umask=0x2,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_cycles",
+	.event = "event=0x5E,umask=0x1,period=2000003",
+	.desc = "Cycles when Reservation Station (RS) is empty for the thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles the RS is empty for the thread",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000003",
+	.desc = "Stalls caused by changing prefix length of the instruction",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.iq_full",
+	.event = "event=0x87,umask=0x4,period=2000003",
+	.desc = "Stall cycles because IQ is full",
+	.topic = "pipeline",
+	.long_desc = "Stall cycles due to IQ is full",
+},
+{
+	.name = "br_inst_exec.nontaken_conditional",
+	.event = "event=0x88,umask=0x41,period=200003",
+	.desc = "Not taken macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Not taken macro-conditional branches",
+},
+{
+	.name = "br_inst_exec.taken_conditional",
+	.event = "event=0x88,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired macro-conditional branches",
+},
+{
+	.name = "br_inst_exec.taken_direct_jump",
+	.event = "event=0x88,umask=0x82,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branch instructions excluding calls and indirects",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired macro-conditional branch instructions excluding calls and indirects",
+},
+{
+	.name = "br_inst_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired indirect branches excluding calls and returns",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_return",
+	.event = "event=0x88,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired indirect branches with return mnemonic",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired indirect branches with return mnemonic",
+},
+{
+	.name = "br_inst_exec.taken_direct_near_call",
+	.event = "event=0x88,umask=0x90,period=200003",
+	.desc = "Taken speculative and retired direct near calls",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired direct near calls",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_call",
+	.event = "event=0x88,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired indirect calls",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired indirect calls",
+},
+{
+	.name = "br_inst_exec.all_conditional",
+	.event = "event=0x88,umask=0xc1,period=200003",
+	.desc = "Speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Speculative and retired macro-conditional branches",
+},
+{
+	.name = "br_inst_exec.all_direct_jmp",
+	.event = "event=0x88,umask=0xc2,period=200003",
+	.desc = "Speculative and retired macro-unconditional branches excluding calls and indirects",
+	.topic = "pipeline",
+	.long_desc = "Speculative and retired macro-unconditional branches excluding calls and indirects",
+},
+{
+	.name = "br_inst_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0xc4,period=200003",
+	.desc = "Speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "Speculative and retired indirect branches excluding calls and returns",
+},
+{
+	.name = "br_inst_exec.all_indirect_near_return",
+	.event = "event=0x88,umask=0xc8,period=200003",
+	.desc = "Speculative and retired indirect return branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_direct_near_call",
+	.event = "event=0x88,umask=0xd0,period=200003",
+	.desc = "Speculative and retired direct near calls",
+	.topic = "pipeline",
+	.long_desc = "Speculative and retired direct near calls",
+},
+{
+	.name = "br_inst_exec.all_branches",
+	.event = "event=0x88,umask=0xff,period=200003",
+	.desc = "Speculative and retired  branches",
+	.topic = "pipeline",
+	.long_desc = "Counts all near executed branches (not necessarily retired)",
+},
+{
+	.name = "br_misp_exec.nontaken_conditional",
+	.event = "event=0x89,umask=0x41,period=200003",
+	.desc = "Not taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Not taken speculative and retired mispredicted macro conditional branches",
+},
+{
+	.name = "br_misp_exec.taken_conditional",
+	.event = "event=0x89,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired mispredicted macro conditional branches",
+},
+{
+	.name = "br_misp_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired mispredicted indirect branches excluding calls and returns",
+},
+{
+	.name = "br_misp_exec.taken_return_near",
+	.event = "event=0x89,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches with return mnemonic",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired mispredicted indirect branches with return mnemonic",
+},
+{
+	.name = "br_misp_exec.taken_indirect_near_call",
+	.event = "event=0x89,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect calls",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired mispredicted indirect calls",
+},
+{
+	.name = "br_misp_exec.all_conditional",
+	.event = "event=0x89,umask=0xc1,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Speculative and retired mispredicted macro conditional branches",
+},
+{
+	.name = "br_misp_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0xc4,period=200003",
+	.desc = "Mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted indirect branches excluding calls and returns",
+},
+{
+	.name = "br_misp_exec.all_branches",
+	.event = "event=0x89,umask=0xff,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Counts all near executed branches (not necessarily retired)",
+},
+{
+	.name = "uops_dispatched_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 0",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 0",
+},
+{
+	.name = "uops_dispatched_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 1",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 1",
+},
+{
+	.name = "uops_dispatched_port.port_4",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 4",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 4",
+},
+{
+	.name = "uops_dispatched_port.port_5",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 5",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 5",
+},
+{
+	.name = "uops_dispatched_port.port_0_core",
+	.event = "event=0xA1,umask=0x1,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 0",
+	.topic = "pipeline",
+	.long_desc = "Cycles per core when uops are dispatched to port 0",
+},
+{
+	.name = "uops_dispatched_port.port_1_core",
+	.event = "event=0xA1,umask=0x2,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 1",
+	.topic = "pipeline",
+	.long_desc = "Cycles per core when uops are dispatched to port 1",
+},
+{
+	.name = "uops_dispatched_port.port_4_core",
+	.event = "event=0xA1,umask=0x40,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 4",
+	.topic = "pipeline",
+	.long_desc = "Cycles per core when uops are dispatched to port 4",
+},
+{
+	.name = "uops_dispatched_port.port_5_core",
+	.event = "event=0xA1,umask=0x80,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 5",
+	.topic = "pipeline",
+	.long_desc = "Cycles per core when uops are dispatched to port 5",
+},
+{
+	.name = "uops_dispatched_port.port_2",
+	.event = "event=0xA1,umask=0xc,period=2000003",
+	.desc = "Cycles per thread when load or STA uops are dispatched to port 2",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 2",
+},
+{
+	.name = "uops_dispatched_port.port_3",
+	.event = "event=0xA1,umask=0x30,period=2000003",
+	.desc = "Cycles per thread when load or STA uops are dispatched to port 3",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 3",
+},
+{
+	.name = "uops_dispatched_port.port_2_core",
+	.event = "event=0xA1,umask=0xc,any=1,period=2000003",
+	.desc = "Uops dispatched to port 2, loads and stores per core (speculative and retired)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_3_core",
+	.event = "event=0xA1,umask=0x30,any=1,period=2000003",
+	.desc = "Cycles per core when load or STA uops are dispatched to port 3",
+	.topic = "pipeline",
+	.long_desc = "Cycles per core when load or STA uops are dispatched to port 3",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000003",
+	.desc = "Resource-related stall cycles",
+	.topic = "pipeline",
+	.long_desc = "Cycles Allocation is stalled due to Resource Related reason",
+},
+{
+	.name = "resource_stalls.rs",
+	.event = "event=0xA2,umask=0x4,period=2000003",
+	.desc = "Cycles stalled due to no eligible RS entry available",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.sb",
+	.event = "event=0xA2,umask=0x8,period=2000003",
+	.desc = "Cycles stalled due to no store buffers available. (not including draining form sync)",
+	.topic = "pipeline",
+	.long_desc = "Cycles stalled due to no store buffers available (not including draining form sync)",
+},
+{
+	.name = "resource_stalls.rob",
+	.event = "event=0xA2,umask=0x10,period=2000003",
+	.desc = "Cycles stalled due to re-order buffer full",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l2_pending",
+	.event = "event=0xA3,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with pending L2 cache miss loads",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending L2 miss loads. Set AnyThread to count per core",
+},
+{
+	.name = "cycle_activity.cycles_l1d_pending",
+	.event = "event=0xA3,umask=0x8,period=2000003,cmask=8",
+	.desc = "Cycles with pending L1 cache miss loads",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending L1 cache miss loads. Set AnyThread to count per core",
+},
+{
+	.name = "cycle_activity.cycles_ldm_pending",
+	.event = "event=0xA3,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles with pending memory loads",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending memory loads. Set AnyThread to count per core",
+},
+{
+	.name = "cycle_activity.cycles_no_execute",
+	.event = "event=0xA3,umask=0x4,period=2000003,cmask=4",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+	.long_desc = "Total execution stalls",
+},
+{
+	.name = "cycle_activity.stalls_l2_pending",
+	.event = "event=0xA3,umask=0x5,period=2000003,cmask=5",
+	.desc = "Execution stalls due to L2 cache misses",
+	.topic = "pipeline",
+	.long_desc = "Number of loads missed L2",
+},
+{
+	.name = "cycle_activity.stalls_ldm_pending",
+	.event = "event=0xA3,umask=0x6,period=2000003,cmask=6",
+	.desc = "Execution stalls due to memory subsystem",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l1d_pending",
+	.event = "event=0xA3,umask=0xc,period=2000003,cmask=12",
+	.desc = "Execution stalls due to L1 data cache misses",
+	.topic = "pipeline",
+	.long_desc = "Execution stalls due to L1 data cache miss loads. Set Cmask=0CH",
+},
+{
+	.name = "lsd.uops",
+	.event = "event=0xA8,umask=0x1,period=2000003",
+	.desc = "Number of Uops delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_active",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+	.long_desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+},
+{
+	.name = "uops_executed.thread",
+	.event = "event=0xB1,umask=0x1,period=2000003",
+	.desc = "Counts the number of uops to be executed per-thread each cycle",
+	.topic = "pipeline",
+	.long_desc = "Counts total number of uops to be executed per-thread each cycle. Set Cmask = 1, INV =1 to count stall cycles",
+},
+{
+	.name = "uops_executed.core",
+	.event = "event=0xB1,umask=0x2,period=2000003",
+	.desc = "Number of uops executed on the core",
+	.topic = "pipeline",
+	.long_desc = "Counts total number of uops to be executed per-core each cycle",
+},
+{
+	.name = "uops_executed.stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts number of cycles no uops were dispatched to be executed on this thread",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Number of instructions retired. General Counter   - architectural event",
+	.topic = "pipeline",
+	.long_desc = "Number of instructions at retirement",
+},
+{
+	.name = "inst_retired.prec_dist",
+	.event = "event=0xC0,umask=0x1,period=2000003",
+	.desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution (Must be precise)",
+},
+{
+	.name = "other_assists.any_wb_assist",
+	.event = "event=0xC1,umask=0x80,period=100003",
+	.desc = "Number of times any microcode assist is invoked by HW upon uop writeback",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.all",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "Actually retired uops (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of micro-ops retired, Use cmask=1 and invert to count active cycles or stalled cycles (Precise event)",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000003",
+	.desc = "Retirement slots used (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of retirement slots used each cycle (Precise event)",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=10",
+	.desc = "Cycles with less than 10 actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.core_stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=100003",
+	.desc = "Self-modifying code (SMC) detected",
+	.topic = "pipeline",
+	.long_desc = "Number of self-modifying-code machine clears detected",
+},
+{
+	.name = "machine_clears.maskmov",
+	.event = "event=0xC3,umask=0x20,period=100003",
+	.desc = "This event counts the number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of executed AVX masked load operations that refer to an illegal address range with the mask bits set to 0",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=400009",
+	.desc = "Conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=100007",
+	.desc = "Direct and indirect near call instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Direct and indirect near call instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=400009",
+	.desc = "All (macro) branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Branch instructions at retirement",
+},
+{
+	.name = "br_inst_retired.near_return",
+	.event = "event=0xC4,umask=0x8,period=100007",
+	.desc = "Return instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of near return instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.not_taken",
+	.event = "event=0xC4,umask=0x10,period=400009",
+	.desc = "Not taken branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of not taken branch instructions retired",
+},
+{
+	.name = "br_inst_retired.near_taken",
+	.event = "event=0xC4,umask=0x20,period=400009",
+	.desc = "Taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Number of near taken branches retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0x40,period=100007",
+	.desc = "Far branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Number of far branches retired",
+},
+{
+	.name = "br_inst_retired.all_branches_pebs",
+	.event = "event=0xC4,umask=0x4,period=400009",
+	.desc = "All (macro) branch instructions retired (Must be precise)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=400009",
+	.desc = "Mispredicted conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=400009",
+	.desc = "All mispredicted macro branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted branch instructions at retirement",
+},
+{
+	.name = "br_misp_retired.near_taken",
+	.event = "event=0xC5,umask=0x20,period=400009",
+	.desc = "number of near branch instructions retired that were mispredicted and taken (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted taken branch instructions retired (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches_pebs",
+	.event = "event=0xC5,umask=0x4,period=400009",
+	.desc = "Mispredicted macro branch instructions retired (Must be precise)",
+	.topic = "pipeline",
+},
+{
+	.name = "rob_misc_events.lbr_inserts",
+	.event = "event=0xCC,umask=0x20,period=2000003",
+	.desc = "Count cases of saving new LBR",
+	.topic = "pipeline",
+	.long_desc = "Count cases of saving new LBR records by hardware",
+},
+{
+	.name = "baclears.any",
+	.event = "event=0xE6,umask=0x1f,period=100003",
+	.desc = "Counts the total number when the front end is resteered, mainly when the BPU cannot provide a correct prediction and this is corrected by other branch handling mechanisms at the front end",
+	.topic = "pipeline",
+	.long_desc = "Number of front end re-steers due to BPU misprediction",
+},
+{
+	.name = "uops_executed.cycles_ge_1_uop_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles where at least 1 uop was executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 1 uop was executed per-thread",
+},
+{
+	.name = "uops_executed.cycles_ge_2_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles where at least 2 uops were executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 2 uops were executed per-thread",
+},
+{
+	.name = "uops_executed.cycles_ge_3_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles where at least 3 uops were executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 3 uops were executed per-thread",
+},
+{
+	.name = "uops_executed.cycles_ge_4_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles where at least 4 uops were executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 4 uops were executed per-thread",
+},
+{
+	.name = "rs_events.empty_end",
+	.event = "event=0x5E,inv=1,umask=0x1,edge=1,period=200003,cmask=1",
+	.desc = "Counts end of periods where the Reservation Station (RS) was empty. Could be useful to precisely locate Frontend Latency Bound issues",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.count",
+	.event = "event=0xC3,umask=0x1,edge=1,period=100003,cmask=1",
+	.desc = "Number of machine clears (nukes) of any type",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_4_uops",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+	.long_desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+},
+{
+	.name = "cycle_activity.cycles_l1d_miss",
+	.event = "event=0xA3,umask=0x8,period=2000003,cmask=8",
+	.desc = "Cycles while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l2_miss",
+	.event = "event=0xA3,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles while L2 cache miss load* is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_mem_any",
+	.event = "event=0xA3,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_total",
+	.event = "event=0xA3,umask=0x4,period=2000003,cmask=4",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l1d_miss",
+	.event = "event=0xA3,umask=0xc,period=2000003,cmask=12",
+	.desc = "Execution stalls while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l2_miss",
+	.event = "event=0xA3,umask=0x5,period=2000003,cmask=5",
+	.desc = "Execution stalls while L2 cache miss load* is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_mem_any",
+	.event = "event=0xA3,umask=0x6,period=2000003,cmask=6",
+	.desc = "Execution stalls while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_any",
+	.event = "event=0x3c,any=1",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "Core cycles when at least one thread on the physical core is not in halt state",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p_any",
+	.event = "event=0x3C,umask=0x0,any=1,period=2000003",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "Core cycles when at least one thread on the physical core is not in halt state",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted. (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_cycles_any",
+	.event = "event=0x0D,umask=0x3,any=1,period=2000003,cmask=1",
+	.desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_1",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles at least 1 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+	.long_desc = "Cycles at least 1 micro-op is executed from any thread on physical core",
+},
+{
+	.name = "uops_executed.core_cycles_ge_2",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles at least 2 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+	.long_desc = "Cycles at least 2 micro-op is executed from any thread on physical core",
+},
+{
+	.name = "uops_executed.core_cycles_ge_3",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=3",
+	.desc = "Cycles at least 3 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+	.long_desc = "Cycles at least 3 micro-op is executed from any thread on physical core",
+},
+{
+	.name = "uops_executed.core_cycles_ge_4",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=4",
+	.desc = "Cycles at least 4 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+	.long_desc = "Cycles at least 4 micro-op is executed from any thread on physical core",
+},
+{
+	.name = "uops_executed.core_cycles_none",
+	.event = "event=0xB1,inv=1,umask=0x2,period=2000003",
+	.desc = "Cycles with no micro-ops executed from any thread on physical core",
+	.topic = "pipeline",
+	.long_desc = "Cycles with no micro-ops executed from any thread on physical core",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the thread is unhalted. (counts at 100 MHz rate)",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted. (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "idq.empty",
+	.event = "event=0x79,umask=0x2,period=2000003",
+	.desc = "Instruction Decode Queue (IDQ) empty cycles",
+	.topic = "frontend",
+	.long_desc = "Counts cycles the IDQ is empty",
+},
+{
+	.name = "idq.mite_uops",
+	.event = "event=0x79,umask=0x4,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ from MITE path. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.dsb_uops",
+	.event = "event=0x79,umask=0x8,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle. # of uops delivered to IDQ from DSB path. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.ms_dsb_uops",
+	.event = "event=0x79,umask=0x10,period=2000003",
+	.desc = "Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ when MS_busy by DSB. Set Cmask = 1 to count cycles. Add Edge=1 to count # of delivery",
+},
+{
+	.name = "idq.ms_mite_uops",
+	.event = "event=0x79,umask=0x20,period=2000003",
+	.desc = "Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ when MS_busy by MITE. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.ms_uops",
+	.event = "event=0x79,umask=0x30,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ from MS by either DSB or MITE. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.ms_cycles",
+	.event = "event=0x79,umask=0x30,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+},
+{
+	.name = "idq.mite_cycles",
+	.event = "event=0x79,umask=0x4,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+},
+{
+	.name = "idq.dsb_cycles",
+	.event = "event=0x79,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+},
+{
+	.name = "idq.ms_dsb_cycles",
+	.event = "event=0x79,umask=0x10,period=2000003,cmask=1",
+	.desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+},
+{
+	.name = "idq.ms_dsb_occur",
+	.event = "event=0x79,umask=0x10,edge=1,period=2000003,cmask=1",
+	.desc = "Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequenser (MS) is busy",
+},
+{
+	.name = "idq.all_dsb_cycles_4_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=4",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "Counts cycles DSB is delivered four uops. Set Cmask = 4",
+},
+{
+	.name = "idq.all_dsb_cycles_any_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=1",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "Counts cycles DSB is delivered at least one uops. Set Cmask = 1",
+},
+{
+	.name = "idq.all_mite_cycles_4_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=4",
+	.desc = "Cycles MITE is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "Counts cycles MITE is delivered four uops. Set Cmask = 4",
+},
+{
+	.name = "idq.all_mite_cycles_any_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=1",
+	.desc = "Cycles MITE is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "Counts cycles MITE is delivered at least one uops. Set Cmask = 1",
+},
+{
+	.name = "idq.mite_all_uops",
+	.event = "event=0x79,umask=0x3c,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "Number of uops delivered to IDQ from any path",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=2000003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Reads. both cacheable and noncacheable, including UC fetches",
+	.topic = "frontend",
+	.long_desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Reads. both cacheable and noncacheable, including UC fetches",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "Instruction cache, streaming buffer and victim cache misses",
+	.topic = "frontend",
+	.long_desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Misses. Includes UC accesses",
+},
+{
+	.name = "icache.ifetch_stall",
+	.event = "event=0x80,umask=0x4,period=2000003",
+	.desc = "Cycles where a code-fetch stalled due to L1 instruction-cache miss or an iTLB miss",
+	.topic = "frontend",
+	.long_desc = "Cycles where a code-fetch stalled due to L1 instruction-cache miss or an iTLB miss",
+},
+{
+	.name = "idq_uops_not_delivered.core",
+	.event = "event=0x9C,umask=0x1,period=2000003",
+	.desc = "Uops not delivered to Resource Allocation Table (RAT) per thread when backend of the machine is not stalled ",
+	.topic = "frontend",
+	.long_desc = "Count issue pipeline slots where no uop was delivered from the front end to the back end when there is no back-end stall",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_0_uops_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles per thread when 4 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_1_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles per thread when 3 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_2_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles with less than 2 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_3_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with less than 3 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_fe_was_ok",
+	.event = "event=0x9C,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts cycles FE delivered 4 uops or Resource Allocation Table (RAT) was stalling FE",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.count",
+	.event = "event=0xAB,umask=0x1,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switches",
+	.topic = "frontend",
+	.long_desc = "Number of DSB to MITE switches",
+},
+{
+	.name = "dsb2mite_switches.penalty_cycles",
+	.event = "event=0xAB,umask=0x2,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles",
+	.topic = "frontend",
+	.long_desc = "Cycles DSB to MITE switches caused delay",
+},
+{
+	.name = "dsb_fill.exceed_dsb_lines",
+	.event = "event=0xAC,umask=0x8,period=2000003",
+	.desc = "Cycles when Decode Stream Buffer (DSB) fill encounter more than 3 Decode Stream Buffer (DSB) lines",
+	.topic = "frontend",
+	.long_desc = "DSB Fill encountered > 3 DSB lines",
+},
+{
+	.name = "idq.ms_switches",
+	.event = "event=0x79,umask=0x30,edge=1,period=2000003,cmask=1",
+	.desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+	.topic = "frontend",
+	.long_desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+},
+{
+	.name = "cpl_cycles.ring0",
+	.event = "event=0x5C,umask=0x1,period=2000003",
+	.desc = "Unhalted core cycles when the thread is in ring 0",
+	.topic = "other",
+	.long_desc = "Unhalted core cycles when the thread is in ring 0",
+},
+{
+	.name = "cpl_cycles.ring123",
+	.event = "event=0x5C,umask=0x2,period=2000003",
+	.desc = "Unhalted core cycles when thread is in rings 1, 2, or 3",
+	.topic = "other",
+	.long_desc = "Unhalted core cycles when the thread is not in ring 0",
+},
+{
+	.name = "cpl_cycles.ring0_trans",
+	.event = "event=0x5C,umask=0x1,edge=1,period=100007,cmask=1",
+	.desc = "Number of intervals between processor halts while thread is in ring 0",
+	.topic = "other",
+	.long_desc = "Number of intervals between processor halts while thread is in ring 0",
+},
+{
+	.name = "lock_cycles.split_lock_uc_lock_duration",
+	.event = "event=0x63,umask=0x1,period=2000003",
+	.desc = "Cycles when L1 and L2 are locked due to UC or split lock",
+	.topic = "other",
+	.long_desc = "Cycles in which the L1D and L2 are locked, due to a UC lock or split lock",
+},
+{
+	.name = "misalign_mem_ref.loads",
+	.event = "event=0x05,umask=0x1,period=2000003",
+	.desc = "Speculative cache line split load uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "Speculative cache-line split load uops dispatched to L1D",
+},
+{
+	.name = "misalign_mem_ref.stores",
+	.event = "event=0x05,umask=0x2,period=2000003",
+	.desc = "Speculative cache line split STA uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "Speculative cache-line split Store-address uops dispatched to L1D",
+},
+{
+	.name = "page_walks.llc_miss",
+	.event = "event=0xBE,umask=0x1,period=100003",
+	.desc = "Number of any page walk that had a miss in LLC",
+	.topic = "memory",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=100003",
+	.desc = "Counts the number of machine clears due to memory order conflicts",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.precise_store",
+	.event = "event=0xCD,umask=0x2,period=2000003",
+	.desc = "Sample stores and collect precise store operation via PEBS record. PMC3 only (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_4",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x4",
+	.desc = "Loads with latency value being above 4 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 4 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_8",
+	.event = "event=0xCD,umask=0x1,period=50021,ldlat=0x8",
+	.desc = "Loads with latency value being above 8 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 8 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_16",
+	.event = "event=0xCD,umask=0x1,period=20011,ldlat=0x10",
+	.desc = "Loads with latency value being above 16 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 16 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_32",
+	.event = "event=0xCD,umask=0x1,period=100007,ldlat=0x20",
+	.desc = "Loads with latency value being above 32 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 32 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_64",
+	.event = "event=0xCD,umask=0x1,period=2003,ldlat=0x40",
+	.desc = "Loads with latency value being above 64 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 64 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_128",
+	.event = "event=0xCD,umask=0x1,period=1009,ldlat=0x80",
+	.desc = "Loads with latency value being above 128 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 128 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_256",
+	.event = "event=0xCD,umask=0x1,period=503,ldlat=0x100",
+	.desc = "Loads with latency value being above 256 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 256 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_512",
+	.event = "event=0xCD,umask=0x1,period=101,ldlat=0x200",
+	.desc = "Loads with latency value being above 512 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 512 (Must be precise)",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400244",
+	.desc = "Counts all demand & prefetch code reads that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400091",
+	.desc = "Counts all demand & prefetch data reads that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3004003f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400004",
+	.desc = "Counts demand code reads that miss the LLC and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400001",
+	.desc = "Counts demand data reads that miss the LLC and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in_socket.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x6004001b3",
+	.desc = "Counts LLC replacements",
+	.topic = "memory",
+},
+{
+	.name = "dtlb_load_misses.large_page_walk_completed",
+	.event = "event=0x08,umask=0x88,period=100003",
+	.desc = "Page walk for a large page completed for Demand load",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.miss_causes_a_walk",
+	.event = "event=0x49,umask=0x1,period=100003",
+	.desc = "Store misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Miss in all TLB levels causes a page walk of any page size (4K/2M/4M/1G)",
+},
+{
+	.name = "dtlb_store_misses.walk_completed",
+	.event = "event=0x49,umask=0x2,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks",
+	.topic = "virtual memory",
+	.long_desc = "Miss in all TLB levels causes a page walk that completes of any page size (4K/2M/4M/1G)",
+},
+{
+	.name = "dtlb_store_misses.walk_duration",
+	.event = "event=0x49,umask=0x4,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "Cycles PMH is busy with this walk",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit",
+	.event = "event=0x49,umask=0x10,period=100003",
+	.desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4F,umask=0x10,period=2000003",
+	.desc = "Cycle count for an Extended Page table walk.  The Extended Page Directory cache is used by Virtual Machine operating systems while the guest operating systems use the standard TLB caches",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x5F,umask=0x4,period=100003",
+	.desc = "Load operations that miss the first DTLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Counts load operations that missed 1st level DTLB but hit the 2nd level",
+},
+{
+	.name = "itlb_misses.miss_causes_a_walk",
+	.event = "event=0x85,umask=0x1,period=100003",
+	.desc = "Misses at all ITLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Misses in all ITLB levels that cause page walks",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0x2,period=100003",
+	.desc = "Misses in all ITLB levels that cause completed page walks",
+	.topic = "virtual memory",
+	.long_desc = "Misses in all ITLB levels that cause completed page walks",
+},
+{
+	.name = "itlb_misses.walk_duration",
+	.event = "event=0x85,umask=0x4,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "Cycle PMH is busy with a walk",
+},
+{
+	.name = "itlb_misses.stlb_hit",
+	.event = "event=0x85,umask=0x10,period=100003",
+	.desc = "Operations that miss the first ITLB level but hit the second and do not cause any page walks",
+	.topic = "virtual memory",
+	.long_desc = "Number of cache load STLB hits. No page walk",
+},
+{
+	.name = "itlb_misses.large_page_walk_completed",
+	.event = "event=0x85,umask=0x80,period=100003",
+	.desc = "Completed page walks in ITLB due to STLB load misses for large pages",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks in ITLB due to STLB load misses for large pages",
+},
+{
+	.name = "itlb.itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=100007",
+	.desc = "Flushing of the Instruction TLB (ITLB) pages, includes 4k/2M/4M pages",
+	.topic = "virtual memory",
+	.long_desc = "Counts the number of ITLB flushes, includes 4k/2M/4M pages",
+},
+{
+	.name = "tlb_flush.dtlb_thread",
+	.event = "event=0xBD,umask=0x1,period=100007",
+	.desc = "DTLB flush attempts of the thread-specific entries",
+	.topic = "virtual memory",
+	.long_desc = "DTLB flush attempts of the thread-specific entries",
+},
+{
+	.name = "tlb_flush.stlb_any",
+	.event = "event=0xBD,umask=0x20,period=100007",
+	.desc = "STLB flush attempts",
+	.topic = "virtual memory",
+	.long_desc = "Count number of STLB flush attempts",
+},
+{
+	.name = "dtlb_load_misses.miss_causes_a_walk",
+	.event = "event=0x08,umask=0x81,period=100003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes an page walk of any page size",
+	.topic = "virtual memory",
+	.long_desc = "Misses in all TLB levels that cause a page walk of any page size from demand loads",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x08,umask=0x82,period=100003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes of any page size",
+	.topic = "virtual memory",
+	.long_desc = "Misses in all TLB levels that caused page walk completed of any size by demand loads",
+},
+{
+	.name = "dtlb_load_misses.walk_duration",
+	.event = "event=0x08,umask=0x84,period=2000003",
+	.desc = "Demand load cycles page miss handler (PMH) is busy with this walk",
+	.topic = "virtual memory",
+	.long_desc = "Cycle PMH is busy with a walk due to demand loads",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_hit",
+	.event = "event=0x24,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "Demand Data Read requests that hit L2 cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x4,period=200003",
+	.desc = "RFO requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "RFO requests that hit L2 cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x8,period=200003",
+	.desc = "RFO requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts the number of store RFO requests that miss the L2 cache",
+},
+{
+	.name = "l2_rqsts.code_rd_hit",
+	.event = "event=0x24,umask=0x10,period=200003",
+	.desc = "L2 cache hits when fetching instructions, code reads",
+	.topic = "cache",
+	.long_desc = "Number of instruction fetches that hit the L2 cache",
+},
+{
+	.name = "l2_rqsts.code_rd_miss",
+	.event = "event=0x24,umask=0x20,period=200003",
+	.desc = "L2 cache misses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "Number of instruction fetches that missed the L2 cache",
+},
+{
+	.name = "l2_rqsts.pf_hit",
+	.event = "event=0x24,umask=0x40,period=200003",
+	.desc = "Requests from the L2 hardware prefetchers that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests that hit L2",
+},
+{
+	.name = "l2_rqsts.pf_miss",
+	.event = "event=0x24,umask=0x80,period=200003",
+	.desc = "Requests from the L2 hardware prefetchers that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests that missed L2",
+},
+{
+	.name = "l2_rqsts.all_demand_data_rd",
+	.event = "event=0x24,umask=0x3,period=200003",
+	.desc = "Demand Data Read requests",
+	.topic = "cache",
+	.long_desc = "Counts any demand and L1 HW prefetch data load requests to L2",
+},
+{
+	.name = "l2_rqsts.all_rfo",
+	.event = "event=0x24,umask=0xc,period=200003",
+	.desc = "RFO requests to L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 store RFO requests",
+},
+{
+	.name = "l2_rqsts.all_code_rd",
+	.event = "event=0x24,umask=0x30,period=200003",
+	.desc = "L2 code requests",
+	.topic = "cache",
+	.long_desc = "Counts all L2 code requests",
+},
+{
+	.name = "l2_rqsts.all_pf",
+	.event = "event=0x24,umask=0xc0,period=200003",
+	.desc = "Requests from L2 hardware prefetchers",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests",
+},
+{
+	.name = "l2_store_lock_rqsts.miss",
+	.event = "event=0x27,umask=0x1,period=200003",
+	.desc = "RFOs that miss cache lines",
+	.topic = "cache",
+	.long_desc = "RFOs that miss cache lines",
+},
+{
+	.name = "l2_store_lock_rqsts.hit_m",
+	.event = "event=0x27,umask=0x8,period=200003",
+	.desc = "RFOs that hit cache lines in M state",
+	.topic = "cache",
+	.long_desc = "RFOs that hit cache lines in M state",
+},
+{
+	.name = "l2_store_lock_rqsts.all",
+	.event = "event=0x27,umask=0xf,period=200003",
+	.desc = "RFOs that access cache lines in any state",
+	.topic = "cache",
+	.long_desc = "RFOs that access cache lines in any state",
+},
+{
+	.name = "l2_l1d_wb_rqsts.miss",
+	.event = "event=0x28,umask=0x1,period=200003",
+	.desc = "Count the number of modified Lines evicted from L1 and missed L2. (Non-rejected WBs from the DCU.)",
+	.topic = "cache",
+	.long_desc = "Not rejected writebacks that missed LLC",
+},
+{
+	.name = "l2_l1d_wb_rqsts.hit_e",
+	.event = "event=0x28,umask=0x4,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in E state",
+	.topic = "cache",
+	.long_desc = "Not rejected writebacks from L1D to L2 cache lines in E state",
+},
+{
+	.name = "l2_l1d_wb_rqsts.hit_m",
+	.event = "event=0x28,umask=0x8,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in M state",
+	.topic = "cache",
+	.long_desc = "Not rejected writebacks from L1D to L2 cache lines in M state",
+},
+{
+	.name = "l2_l1d_wb_rqsts.all",
+	.event = "event=0x28,umask=0xf,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in any state",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100003",
+	.desc = "Core-originated cacheable demand requests missed LLC",
+	.topic = "cache",
+	.long_desc = "This event counts each cache miss condition for references to the last level cache",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=100003",
+	.desc = "Core-originated cacheable demand requests that refer to LLC",
+	.topic = "cache",
+	.long_desc = "This event counts requests originating from the core that reference a cache line in the last level cache",
+},
+{
+	.name = "l1d_pend_miss.pending",
+	.event = "event=0x48,umask=0x1,period=2000003",
+	.desc = "L1D miss oustandings duration in cycles",
+	.topic = "cache",
+	.long_desc = "Increments the number of outstanding L1D misses every cycle. Set Cmask = 1 and Edge =1 to count occurrences",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles",
+	.event = "event=0x48,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding",
+	.topic = "cache",
+},
+{
+	.name = "l1d.replacement",
+	.event = "event=0x51,umask=0x1,period=2000003",
+	.desc = "L1D data line replacements",
+	.topic = "cache",
+	.long_desc = "Counts the number of lines brought into the L1 data cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003",
+	.desc = "Offcore outstanding Demand Data Read transactions in uncore queue",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding Demand Data Read transactions in SQ to uncore. Set Cmask=1 to count cycles",
+},
+{
+	.name = "offcore_requests_outstanding.demand_code_rd",
+	.event = "event=0x60,umask=0x2,period=2000003",
+	.desc = "Offcore outstanding code reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding Demand Code Read transactions in SQ to uncore. Set Cmask=1 to count cycles",
+},
+{
+	.name = "offcore_requests_outstanding.demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003",
+	.desc = "Offcore outstanding RFO store transactions in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding RFO store transactions in SQ to uncore. Set Cmask=1 to count cycles",
+},
+{
+	.name = "offcore_requests_outstanding.all_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003",
+	.desc = "Offcore outstanding cacheable Core Data Read transactions in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding cacheable data read transactions in SQ to uncore. Set Cmask=1 to count cycles",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_code_rd",
+	.event = "event=0x60,umask=0x2,period=2000003,cmask=1",
+	.desc = "Offcore outstanding code reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding code reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003,cmask=1",
+	.desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+},
+{
+	.name = "lock_cycles.cache_lock_duration",
+	.event = "event=0x63,umask=0x2,period=2000003",
+	.desc = "Cycles when L1D is locked",
+	.topic = "cache",
+	.long_desc = "Cycles in which the L1D is locked",
+},
+{
+	.name = "offcore_requests.demand_data_rd",
+	.event = "event=0xB0,umask=0x1,period=100003",
+	.desc = "Demand Data Read requests sent to uncore",
+	.topic = "cache",
+	.long_desc = "Demand data read requests sent to uncore",
+},
+{
+	.name = "offcore_requests.demand_code_rd",
+	.event = "event=0xB0,umask=0x2,period=100003",
+	.desc = "Cacheable and noncachaeble code read requests",
+	.topic = "cache",
+	.long_desc = "Demand code read requests sent to uncore",
+},
+{
+	.name = "offcore_requests.demand_rfo",
+	.event = "event=0xB0,umask=0x4,period=100003",
+	.desc = "Demand RFO requests including regular RFOs, locks, ItoM",
+	.topic = "cache",
+	.long_desc = "Demand RFO read requests sent to uncore, including regular RFOs, locks, ItoM",
+},
+{
+	.name = "offcore_requests.all_data_rd",
+	.event = "event=0xB0,umask=0x8,period=100003",
+	.desc = "Demand and prefetch data reads",
+	.topic = "cache",
+	.long_desc = "Data read requests sent to uncore (demand and prefetch)",
+},
+{
+	.name = "offcore_requests_buffer.sq_full",
+	.event = "event=0xB2,umask=0x1,period=2000003",
+	.desc = "Cases when offcore requests buffer cannot take more entries for core",
+	.topic = "cache",
+	.long_desc = "Cases when offcore requests buffer cannot take more entries for core",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_loads",
+	.event = "event=0xD0,umask=0x11,period=100003",
+	.desc = "Retired load uops that miss the STLB (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_stores",
+	.event = "event=0xD0,umask=0x12,period=100003",
+	.desc = "Retired store uops that miss the STLB (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.lock_loads",
+	.event = "event=0xD0,umask=0x21,period=100007",
+	.desc = "Retired load uops with locked access (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.split_loads",
+	.event = "event=0xD0,umask=0x41,period=100003",
+	.desc = "Retired load uops that split across a cacheline boundary (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.split_stores",
+	.event = "event=0xD0,umask=0x42,period=100003",
+	.desc = "Retired store uops that split across a cacheline boundary (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0xD0,umask=0x81,period=2000003",
+	.desc = "All retired load uops (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0xD0,umask=0x82,period=2000003",
+	.desc = "All retired store uops (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.l1_hit",
+	.event = "event=0xD1,umask=0x1,period=2000003",
+	.desc = "Retired load uops with L1 cache hits as data sources (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops with L1 cache hits as data sources (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_hit",
+	.event = "event=0xD1,umask=0x2,period=100003",
+	.desc = "Retired load uops with L2 cache hits as data sources (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops with L2 cache hits as data sources (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.llc_hit",
+	.event = "event=0xD1,umask=0x4,period=50021",
+	.desc = "Retired load uops which data sources were data hits in LLC without snoops required (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source was LLC hit with no snoop required (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_miss",
+	.event = "event=0xD1,umask=0x8,period=100003",
+	.desc = "Retired load uops which data sources following L1 data-cache miss (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source followed an L1 miss (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_miss",
+	.event = "event=0xD1,umask=0x10,period=50021",
+	.desc = "Miss in mid-level (L2) cache. Excludes Unknown data-source (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops that missed L2, excluding unknown sources (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.llc_miss",
+	.event = "event=0xD1,umask=0x20,period=100007",
+	.desc = "Miss in last-level (L3) cache. Excludes Unknown data-source (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source is LLC miss (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.hit_lfb",
+	.event = "event=0xD1,umask=0x40,period=100003",
+	.desc = "Retired load uops which data sources were load uops missed L1 but hit FB due to preceding miss to the same cache line with data not ready (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops which data sources were load uops missed L1 but hit FB due to preceding miss to the same cache line with data not ready (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_miss",
+	.event = "event=0xD2,umask=0x1,period=20011",
+	.desc = "Retired load uops which data sources were LLC hit and cross-core snoop missed in on-pkg core cache (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source was an on-package core cache LLC hit and cross-core snoop missed (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_hit",
+	.event = "event=0xD2,umask=0x2,period=20011",
+	.desc = "Retired load uops which data sources were LLC and cross-core snoop hits in on-pkg core cache (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source was an on-package LLC hit and cross-core snoop hits (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_hitm",
+	.event = "event=0xD2,umask=0x4,period=20011",
+	.desc = "Retired load uops which data sources were HitM responses from shared LLC (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source was an on-package core cache with HitM responses (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_none",
+	.event = "event=0xD2,umask=0x8,period=100003",
+	.desc = "Retired load uops which data sources were hits in LLC without snoops required (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source was LLC hit with no snoop required (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_miss_retired.local_dram",
+	.event = "event=0xD3,umask=0x1,period=100007",
+	.desc = "Retired load uops which data sources missed LLC but serviced from local dram",
+	.topic = "cache",
+	.long_desc = "Retired load uop whose Data Source was: local DRAM either Snoop not needed or Snoop Miss (RspI)",
+},
+{
+	.name = "l2_trans.demand_data_rd",
+	.event = "event=0xF0,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "Demand Data Read requests that access L2 cache",
+},
+{
+	.name = "l2_trans.rfo",
+	.event = "event=0xF0,umask=0x2,period=200003",
+	.desc = "RFO requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "RFO requests that access L2 cache",
+},
+{
+	.name = "l2_trans.code_rd",
+	.event = "event=0xF0,umask=0x4,period=200003",
+	.desc = "L2 cache accesses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "L2 cache accesses when fetching instructions",
+},
+{
+	.name = "l2_trans.all_pf",
+	.event = "event=0xF0,umask=0x8,period=200003",
+	.desc = "L2 or LLC HW prefetches that access L2 cache",
+	.topic = "cache",
+	.long_desc = "Any MLC or LLC HW prefetch accessing L2, including rejects",
+},
+{
+	.name = "l2_trans.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200003",
+	.desc = "L1D writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L1D writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_fill",
+	.event = "event=0xF0,umask=0x20,period=200003",
+	.desc = "L2 fill requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L2 fill requests that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_wb",
+	.event = "event=0xF0,umask=0x40,period=200003",
+	.desc = "L2 writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L2 writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.all_requests",
+	.event = "event=0xF0,umask=0x80,period=200003",
+	.desc = "Transactions accessing L2 pipe",
+	.topic = "cache",
+	.long_desc = "Transactions accessing L2 pipe",
+},
+{
+	.name = "l2_lines_in.i",
+	.event = "event=0xF1,umask=0x1,period=100003",
+	.desc = "L2 cache lines in I state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in I state filling L2",
+},
+{
+	.name = "l2_lines_in.s",
+	.event = "event=0xF1,umask=0x2,period=100003",
+	.desc = "L2 cache lines in S state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in S state filling L2",
+},
+{
+	.name = "l2_lines_in.e",
+	.event = "event=0xF1,umask=0x4,period=100003",
+	.desc = "L2 cache lines in E state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in E state filling L2",
+},
+{
+	.name = "l2_lines_in.all",
+	.event = "event=0xF1,umask=0x7,period=100003",
+	.desc = "L2 cache lines filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines filling L2",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x1,period=100003",
+	.desc = "Clean L2 cache lines evicted by demand",
+	.topic = "cache",
+	.long_desc = "Clean L2 cache lines evicted by demand",
+},
+{
+	.name = "l2_lines_out.demand_dirty",
+	.event = "event=0xF2,umask=0x2,period=100003",
+	.desc = "Dirty L2 cache lines evicted by demand",
+	.topic = "cache",
+	.long_desc = "Dirty L2 cache lines evicted by demand",
+},
+{
+	.name = "l2_lines_out.pf_clean",
+	.event = "event=0xF2,umask=0x4,period=100003",
+	.desc = "Clean L2 cache lines evicted by L2 prefetch",
+	.topic = "cache",
+	.long_desc = "Clean L2 cache lines evicted by the MLC prefetcher",
+},
+{
+	.name = "l2_lines_out.pf_dirty",
+	.event = "event=0xF2,umask=0x8,period=100003",
+	.desc = "Dirty L2 cache lines evicted by L2 prefetch",
+	.topic = "cache",
+	.long_desc = "Dirty L2 cache lines evicted by the MLC prefetcher",
+},
+{
+	.name = "l2_lines_out.dirty_all",
+	.event = "event=0xF2,umask=0xa,period=100003",
+	.desc = "Dirty L2 cache lines filling the L2",
+	.topic = "cache",
+	.long_desc = "Dirty L2 cache lines filling the L2",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xF4,umask=0x10,period=100003",
+	.desc = "Split locks in SQ",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_llc_miss_retired.local_dram",
+	.event = "event=0xD3,umask=0x1,period=100007",
+	.desc = "Retired load uops which data sources missed LLC but serviced from local dram",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source was local memory (cross-socket snoop not needed or missed)",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd_ge_6",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=6",
+	.desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue",
+	.topic = "cache",
+	.long_desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles_any",
+	.event = "event=0x48,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+	.topic = "cache",
+	.long_desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+},
+{
+	.name = "l1d_pend_miss.fb_full",
+	.event = "event=0x48,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles a demand request was blocked due to Fill Buffers inavailability",
+	.topic = "cache",
+	.long_desc = "Cycles a demand request was blocked due to Fill Buffers inavailability",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0244",
+	.desc = "Counts all demand & prefetch code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0244",
+	.desc = "Counts demand & prefetch code reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0091",
+	.desc = "Counts all demand & prefetch data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0122",
+	.desc = "Counts all demand & prefetch RFOs that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0122",
+	.desc = "Counts demand & prefetch RFOs that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10008",
+	.desc = "Counts all writebacks from the core to the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0004",
+	.desc = "Counts all demand code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0004",
+	.desc = "Counts demand code reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0001",
+	.desc = "Counts all demand data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0002",
+	.desc = "Counts all demand data writes (RFOs) that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0002",
+	.desc = "Counts demand data writes (RFOs) that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0002",
+	.desc = "Counts demand data writes (RFOs) that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x18000",
+	.desc = "Counts miscellaneous accesses that include port i/o, MMIO and uncacheable memory accesses. It also includes L2 hints sent to LLC to keep a line from being evicted out of the core caches",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.split_lock_uc_lock.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10400",
+	.desc = "Counts requests where the address of an atomic lock instruction spans a cache line boundary or the lock instruction is executed on uncacheable address ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10800",
+	.desc = "Counts non-temporal stores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010001",
+	.desc = "Counts all demand data reads ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010002",
+	.desc = "Counts all demand rfo's ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010004",
+	.desc = "Counts all demand code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x000105B3",
+	.desc = "Counts all demand & prefetch data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010122",
+	.desc = "Counts all demand & prefetch prefetch RFOs ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x000107F7",
+	.desc = "Counts all data/code/rfo references (demand & prefetch) ",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_westmereep_dp[] = {
+{
+	.name = "fp_assist.all",
+	.event = "event=0xF7,umask=0x1,period=20000",
+	.desc = "X87 Floating point assists (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.input",
+	.event = "event=0xF7,umask=0x4,period=20000",
+	.desc = "X87 Floating poiint assists for invalid input value (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.output",
+	.event = "event=0xF7,umask=0x2,period=20000",
+	.desc = "X87 Floating point assists for invalid output value (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.mmx",
+	.event = "event=0x10,umask=0x2,period=2000000",
+	.desc = "MMX Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_double_precision",
+	.event = "event=0x10,umask=0x80,period=2000000",
+	.desc = "SSE* FP double precision Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp",
+	.event = "event=0x10,umask=0x4,period=2000000",
+	.desc = "SSE and SSE2 FP Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp_packed",
+	.event = "event=0x10,umask=0x10,period=2000000",
+	.desc = "SSE FP packed Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp_scalar",
+	.event = "event=0x10,umask=0x20,period=2000000",
+	.desc = "SSE FP scalar Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_single_precision",
+	.event = "event=0x10,umask=0x40,period=2000000",
+	.desc = "SSE* FP single precision Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse2_integer",
+	.event = "event=0x10,umask=0x8,period=2000000",
+	.desc = "SSE2 integer Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.x87",
+	.event = "event=0x10,umask=0x1,period=2000000",
+	.desc = "Computational floating-point operations executed",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.any",
+	.event = "event=0xCC,umask=0x3,period=2000000",
+	.desc = "All Floating Point to and from MMX transitions",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.to_fp",
+	.event = "event=0xCC,umask=0x1,period=2000000",
+	.desc = "Transitions from MMX to Floating Point instructions",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.to_mmx",
+	.event = "event=0xCC,umask=0x2,period=2000000",
+	.desc = "Transitions from Floating Point to MMX instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.pack",
+	.event = "event=0x12,umask=0x4,period=200000",
+	.desc = "128 bit SIMD integer pack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_arith",
+	.event = "event=0x12,umask=0x20,period=200000",
+	.desc = "128 bit SIMD integer arithmetic operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_logical",
+	.event = "event=0x12,umask=0x10,period=200000",
+	.desc = "128 bit SIMD integer logical operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_mpy",
+	.event = "event=0x12,umask=0x1,period=200000",
+	.desc = "128 bit SIMD integer multiply operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_shift",
+	.event = "event=0x12,umask=0x2,period=200000",
+	.desc = "128 bit SIMD integer shift operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.shuffle_move",
+	.event = "event=0x12,umask=0x40,period=200000",
+	.desc = "128 bit SIMD integer shuffle/move operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.unpack",
+	.event = "event=0x12,umask=0x8,period=200000",
+	.desc = "128 bit SIMD integer unpack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.pack",
+	.event = "event=0xFD,umask=0x4,period=200000",
+	.desc = "SIMD integer 64 bit pack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_arith",
+	.event = "event=0xFD,umask=0x20,period=200000",
+	.desc = "SIMD integer 64 bit arithmetic operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_logical",
+	.event = "event=0xFD,umask=0x10,period=200000",
+	.desc = "SIMD integer 64 bit logical operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_mpy",
+	.event = "event=0xFD,umask=0x1,period=200000",
+	.desc = "SIMD integer 64 bit packed multiply operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_shift",
+	.event = "event=0xFD,umask=0x2,period=200000",
+	.desc = "SIMD integer 64 bit shift operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.shuffle_move",
+	.event = "event=0xFD,umask=0x40,period=200000",
+	.desc = "SIMD integer 64 bit shuffle/move operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.unpack",
+	.event = "event=0xFD,umask=0x8,period=200000",
+	.desc = "SIMD integer 64 bit unpack operations",
+	.topic = "floating point",
+},
+{
+	.name = "arith.cycles_div_busy",
+	.event = "event=0x14,umask=0x1,period=2000000",
+	.desc = "Cycles the divider is busy",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.div",
+	.event = "event=0x14,inv=1,umask=0x1,period=2000000,cmask=1,edge=1",
+	.desc = "Divide Operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.mul",
+	.event = "event=0x14,umask=0x2,period=2000000",
+	.desc = "Multiply operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear.bad_target",
+	.event = "event=0xE6,umask=0x2,period=2000000",
+	.desc = "BACLEAR asserted with bad target address",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear.clear",
+	.event = "event=0xE6,umask=0x1,period=2000000",
+	.desc = "BACLEAR asserted, regardless of cause ",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear_force_iq",
+	.event = "event=0xA7,umask=0x1,period=2000000",
+	.desc = "Instruction queue forced BACLEAR",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_decoded",
+	.event = "event=0xE0,umask=0x1,period=2000000",
+	.desc = "Branch instructions decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.any",
+	.event = "event=0x88,umask=0x7f,period=200000",
+	.desc = "Branch instructions executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.cond",
+	.event = "event=0x88,umask=0x1,period=200000",
+	.desc = "Conditional branch instructions executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.direct",
+	.event = "event=0x88,umask=0x2,period=200000",
+	.desc = "Unconditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.direct_near_call",
+	.event = "event=0x88,umask=0x10,period=20000",
+	.desc = "Unconditional call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.indirect_near_call",
+	.event = "event=0x88,umask=0x20,period=20000",
+	.desc = "Indirect call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.indirect_non_call",
+	.event = "event=0x88,umask=0x4,period=20000",
+	.desc = "Indirect non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.near_calls",
+	.event = "event=0x88,umask=0x30,period=20000",
+	.desc = "Call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.non_calls",
+	.event = "event=0x88,umask=0x7,period=200000",
+	.desc = "All non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.return_near",
+	.event = "event=0x88,umask=0x8,period=20000",
+	.desc = "Indirect return branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken",
+	.event = "event=0x88,umask=0x40,period=200000",
+	.desc = "Taken branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x4,period=200000",
+	.desc = "Retired branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=200000",
+	.desc = "Retired conditional branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=20000",
+	.desc = "Retired near call instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.any",
+	.event = "event=0x89,umask=0x7f,period=20000",
+	.desc = "Mispredicted branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.cond",
+	.event = "event=0x89,umask=0x1,period=20000",
+	.desc = "Mispredicted conditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.direct",
+	.event = "event=0x89,umask=0x2,period=20000",
+	.desc = "Mispredicted unconditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.direct_near_call",
+	.event = "event=0x89,umask=0x10,period=2000",
+	.desc = "Mispredicted non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.indirect_near_call",
+	.event = "event=0x89,umask=0x20,period=2000",
+	.desc = "Mispredicted indirect call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.indirect_non_call",
+	.event = "event=0x89,umask=0x4,period=2000",
+	.desc = "Mispredicted indirect non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.near_calls",
+	.event = "event=0x89,umask=0x30,period=2000",
+	.desc = "Mispredicted call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.non_calls",
+	.event = "event=0x89,umask=0x7,period=20000",
+	.desc = "Mispredicted non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.return_near",
+	.event = "event=0x89,umask=0x8,period=2000",
+	.desc = "Mispredicted return branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken",
+	.event = "event=0x89,umask=0x40,period=20000",
+	.desc = "Mispredicted taken branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x4,period=20000",
+	.desc = "Mispredicted retired branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=20000",
+	.desc = "Mispredicted conditional retired branches (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_call",
+	.event = "event=0xC5,umask=0x2,period=2000",
+	.desc = "Mispredicted near retired calls (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref",
+	.event = "event=0x0,umask=0x03",
+	.desc = "Reference cycles when thread is not halted (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_p",
+	.event = "event=0x3C,umask=0x1,period=100000",
+	.desc = "Reference base clock (133 Mhz) cycles when thread is not halted (programmable counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Cycles when thread is not halted (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000000",
+	.desc = "Cycles when thread is not halted (programmable counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.total_cycles",
+	.event = "event=0x3C,inv=1,umask=0x0,period=2000000,cmask=2",
+	.desc = "Total CPU cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.any",
+	.event = "event=0x87,umask=0xf,period=2000000",
+	.desc = "Any Instruction Length Decoder stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.iq_full",
+	.event = "event=0x87,umask=0x4,period=2000000",
+	.desc = "Instruction Queue full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000000",
+	.desc = "Length Change Prefix stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.mru",
+	.event = "event=0x87,umask=0x2,period=2000000",
+	.desc = "Stall cycles due to BPU MRU bypass",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.regen",
+	.event = "event=0x87,umask=0x8,period=2000000",
+	.desc = "Regen stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_decoded.dec0",
+	.event = "event=0x18,umask=0x1,period=2000000",
+	.desc = "Instructions that must be decoded by decoder 0",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_queue_write_cycles",
+	.event = "event=0x1E,umask=0x1,period=2000000",
+	.desc = "Cycles instructions are written to the instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_queue_writes",
+	.event = "event=0x17,umask=0x1,period=2000000",
+	.desc = "Instructions written to instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (Programmable counter and Precise Event) (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.mmx",
+	.event = "event=0xC0,umask=0x4,period=2000000",
+	.desc = "Retired MMX instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.total_cycles",
+	.event = "event=0xC0,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.x87",
+	.event = "event=0xC0,umask=0x2,period=2000000",
+	.desc = "Retired floating-point operations (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre",
+	.event = "event=0x4C,umask=0x1,period=200000",
+	.desc = "Load operations conflicting with software prefetches",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.active",
+	.event = "event=0xA8,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles when uops were delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.inactive",
+	.event = "event=0xA8,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no uops were delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd_overflow",
+	.event = "event=0x20,umask=0x1,period=2000000",
+	.desc = "Loops that can't stream from the instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.cycles",
+	.event = "event=0xC3,umask=0x1,period=20000",
+	.desc = "Cycles machine clear asserted",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.mem_order",
+	.event = "event=0xC3,umask=0x2,period=20000",
+	.desc = "Execution pipeline restart due to Memory ordering conflicts",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=20000",
+	.desc = "Self-Modifying Code detected",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000000",
+	.desc = "Resource related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.fpcw",
+	.event = "event=0xA2,umask=0x20,period=2000000",
+	.desc = "FPU control word write stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.load",
+	.event = "event=0xA2,umask=0x2,period=2000000",
+	.desc = "Load buffer stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.mxcsr",
+	.event = "event=0xA2,umask=0x40,period=2000000",
+	.desc = "MXCSR rename stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.other",
+	.event = "event=0xA2,umask=0x80,period=2000000",
+	.desc = "Other Resource related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rob_full",
+	.event = "event=0xA2,umask=0x10,period=2000000",
+	.desc = "ROB full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rs_full",
+	.event = "event=0xA2,umask=0x4,period=2000000",
+	.desc = "Reservation Station full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.store",
+	.event = "event=0xA2,umask=0x8,period=2000000",
+	.desc = "Store buffer stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.packed_double",
+	.event = "event=0xC7,umask=0x4,period=200000",
+	.desc = "SIMD Packed-Double Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.packed_single",
+	.event = "event=0xC7,umask=0x1,period=200000",
+	.desc = "SIMD Packed-Single Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.scalar_double",
+	.event = "event=0xC7,umask=0x8,period=200000",
+	.desc = "SIMD Scalar-Double Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.scalar_single",
+	.event = "event=0xC7,umask=0x2,period=200000",
+	.desc = "SIMD Scalar-Single Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.vector_integer",
+	.event = "event=0xC7,umask=0x10,period=200000",
+	.desc = "SIMD Vector Integer Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uop_unfusion",
+	.event = "event=0xDB,umask=0x1,period=2000000",
+	.desc = "Uop unfusions due to FP exceptions",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.esp_folding",
+	.event = "event=0xD1,umask=0x4,period=2000000",
+	.desc = "Stack pointer instructions decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.esp_sync",
+	.event = "event=0xD1,umask=0x8,period=2000000",
+	.desc = "Stack pointer sync operations",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.ms_cycles_active",
+	.event = "event=0xD1,umask=0x2,period=2000000,cmask=1",
+	.desc = "Uops decoded by Microcode Sequencer",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.stall_cycles",
+	.event = "event=0xD1,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops are decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_active_cycles",
+	.event = "event=0xB1,umask=0x3f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops executed on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_active_cycles_no_port5",
+	.event = "event=0xB1,umask=0x1f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops executed on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_count",
+	.event = "event=0xB1,inv=1,umask=0x3f,any=1,period=2000000,cmask=1,edge=1",
+	.desc = "Uops executed on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_count_no_port5",
+	.event = "event=0xB1,inv=1,umask=0x1f,any=1,period=2000000,cmask=1,edge=1",
+	.desc = "Uops executed on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x3f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_cycles_no_port5",
+	.event = "event=0xB1,inv=1,umask=0x1f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port0",
+	.event = "event=0xB1,umask=0x1,period=2000000",
+	.desc = "Uops executed on port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port015",
+	.event = "event=0xB1,umask=0x40,period=2000000",
+	.desc = "Uops issued on ports 0, 1 or 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port015_stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x40,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on ports 0, 1 or 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port1",
+	.event = "event=0xB1,umask=0x2,period=2000000",
+	.desc = "Uops executed on port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port2_core",
+	.event = "event=0xB1,umask=0x4,any=1,period=2000000",
+	.desc = "Uops executed on port 2 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port234_core",
+	.event = "event=0xB1,umask=0x80,any=1,period=2000000",
+	.desc = "Uops issued on ports 2, 3 or 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port3_core",
+	.event = "event=0xB1,umask=0x8,any=1,period=2000000",
+	.desc = "Uops executed on port 3 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port4_core",
+	.event = "event=0xB1,umask=0x10,any=1,period=2000000",
+	.desc = "Uops executed on port 4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port5",
+	.event = "event=0xB1,umask=0x20,period=2000000",
+	.desc = "Uops executed on port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0xE,umask=0x1,period=2000000",
+	.desc = "Uops issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.core_stall_cycles",
+	.event = "event=0xE,inv=1,umask=0x1,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops were issued on any thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.cycles_all_threads",
+	.event = "event=0xE,umask=0x1,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops were issued on either thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.fused",
+	.event = "event=0xE,umask=0x2,period=2000000",
+	.desc = "Fused Uops issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0xE,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops were issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.active_cycles",
+	.event = "event=0xC2,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles Uops are being retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.any",
+	.event = "event=0xC2,umask=0x1,period=2000000",
+	.desc = "Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.macro_fused",
+	.event = "event=0xC2,umask=0x4,period=2000000",
+	.desc = "Macro-fused Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000000",
+	.desc = "Retirement slots used (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles Uops are not retiring (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles using precise uop retired event (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.total_cycles_ps",
+	.event = "event=0xC0,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "macro_insts.decoded",
+	.event = "event=0xD0,umask=0x1,period=2000000",
+	.desc = "Instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "macro_insts.fusions_decoded",
+	.event = "event=0xA6,umask=0x1,period=2000000",
+	.desc = "Macro-fused instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "two_uop_insts_decoded",
+	.event = "event=0x19,umask=0x1,period=2000000",
+	.desc = "Two Uop instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "bpu_clears.early",
+	.event = "event=0xE8,umask=0x1,period=2000000",
+	.desc = "Early Branch Prediciton Unit clears",
+	.topic = "other",
+},
+{
+	.name = "bpu_clears.late",
+	.event = "event=0xE8,umask=0x2,period=2000000",
+	.desc = "Late Branch Prediction Unit clears",
+	.topic = "other",
+},
+{
+	.name = "bpu_missed_call_ret",
+	.event = "event=0xE5,umask=0x1,period=2000000",
+	.desc = "Branch prediction unit missed call or return",
+	.topic = "other",
+},
+{
+	.name = "es_reg_renames",
+	.event = "event=0xD5,umask=0x1,period=2000000",
+	.desc = "ES segment renames",
+	.topic = "other",
+},
+{
+	.name = "io_transactions",
+	.event = "event=0x6C,umask=0x1,period=2000000",
+	.desc = "I/O transactions",
+	.topic = "other",
+},
+{
+	.name = "l1i.cycles_stalled",
+	.event = "event=0x80,umask=0x4,period=2000000",
+	.desc = "L1I instruction fetch stall cycles",
+	.topic = "other",
+},
+{
+	.name = "l1i.hits",
+	.event = "event=0x80,umask=0x1,period=2000000",
+	.desc = "L1I instruction fetch hits",
+	.topic = "other",
+},
+{
+	.name = "l1i.misses",
+	.event = "event=0x80,umask=0x2,period=2000000",
+	.desc = "L1I instruction fetch misses",
+	.topic = "other",
+},
+{
+	.name = "l1i.reads",
+	.event = "event=0x80,umask=0x3,period=2000000",
+	.desc = "L1I Instruction fetches",
+	.topic = "other",
+},
+{
+	.name = "large_itlb.hit",
+	.event = "event=0x82,umask=0x1,period=200000",
+	.desc = "Large ITLB hit",
+	.topic = "other",
+},
+{
+	.name = "load_block.overlap_store",
+	.event = "event=0x3,umask=0x2,period=200000",
+	.desc = "Loads that partially overlap an earlier store",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.any",
+	.event = "event=0x13,umask=0x7,period=2000000",
+	.desc = "All loads dispatched",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.mob",
+	.event = "event=0x13,umask=0x4,period=2000000",
+	.desc = "Loads dispatched from the MOB",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.rs",
+	.event = "event=0x13,umask=0x1,period=2000000",
+	.desc = "Loads dispatched that bypass the MOB",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.rs_delayed",
+	.event = "event=0x13,umask=0x2,period=2000000",
+	.desc = "Loads dispatched from stage 305",
+	.topic = "other",
+},
+{
+	.name = "partial_address_alias",
+	.event = "event=0x7,umask=0x1,period=200000",
+	.desc = "False dependencies due to partial address aliasing",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.any",
+	.event = "event=0xD2,umask=0xf,period=2000000",
+	.desc = "All RAT stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.flags",
+	.event = "event=0xD2,umask=0x1,period=2000000",
+	.desc = "Flag stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.registers",
+	.event = "event=0xD2,umask=0x2,period=2000000",
+	.desc = "Partial register stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.rob_read_port",
+	.event = "event=0xD2,umask=0x4,period=2000000",
+	.desc = "ROB read port stalls cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.scoreboard",
+	.event = "event=0xD2,umask=0x8,period=2000000",
+	.desc = "Scoreboard stall cycles",
+	.topic = "other",
+},
+{
+	.name = "sb_drain.any",
+	.event = "event=0x4,umask=0x7,period=200000",
+	.desc = "All Store buffer stall cycles",
+	.topic = "other",
+},
+{
+	.name = "seg_rename_stalls",
+	.event = "event=0xD4,umask=0x1,period=2000000",
+	.desc = "Segment rename stall cycles",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hit",
+	.event = "event=0xB8,umask=0x1,period=100000",
+	.desc = "Thread responded HIT to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hite",
+	.event = "event=0xB8,umask=0x2,period=100000",
+	.desc = "Thread responded HITE to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hitm",
+	.event = "event=0xB8,umask=0x4,period=100000",
+	.desc = "Thread responded HITM to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests.code",
+	.event = "event=0xB4,umask=0x4,period=100000",
+	.desc = "Snoop code requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests.data",
+	.event = "event=0xB4,umask=0x1,period=100000",
+	.desc = "Snoop data requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests.invalidate",
+	.event = "event=0xB4,umask=0x2,period=100000",
+	.desc = "Snoop invalidate requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.code",
+	.event = "event=0xB3,umask=0x4,period=2000000",
+	.desc = "Outstanding snoop code requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.code_not_empty",
+	.event = "event=0xB3,umask=0x4,period=2000000,cmask=1",
+	.desc = "Cycles snoop code requests queued",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.data",
+	.event = "event=0xB3,umask=0x1,period=2000000",
+	.desc = "Outstanding snoop data requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.data_not_empty",
+	.event = "event=0xB3,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles snoop data requests queued",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.invalidate",
+	.event = "event=0xB3,umask=0x2,period=2000000",
+	.desc = "Outstanding snoop invalidate requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.invalidate_not_empty",
+	.event = "event=0xB3,umask=0x2,period=2000000,cmask=1",
+	.desc = "Cycles snoop invalidate requests queued",
+	.topic = "other",
+},
+{
+	.name = "sq_full_stall_cycles",
+	.event = "event=0xF6,umask=0x1,period=2000000",
+	.desc = "Super Queue full stall cycles",
+	.topic = "other",
+},
+{
+	.name = "misalign_mem_ref.store",
+	.event = "event=0x5,umask=0x2,period=200000",
+	.desc = "Misaligned store references",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3011",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf811",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4011",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2011",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3044",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf844",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4044",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2044",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x30ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf8ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x40ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x20ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3022",
+	.desc = "REQUEST = ANY RFO and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf822",
+	.desc = "REQUEST = ANY RFO and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4022",
+	.desc = "REQUEST = ANY RFO and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2022",
+	.desc = "REQUEST = ANY RFO and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3008",
+	.desc = "REQUEST = CORE_WB and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf808",
+	.desc = "REQUEST = CORE_WB and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4008",
+	.desc = "REQUEST = CORE_WB and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2008",
+	.desc = "REQUEST = CORE_WB and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3077",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf877",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4077",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2077",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3033",
+	.desc = "REQUEST = DATA_IN and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf833",
+	.desc = "REQUEST = DATA_IN and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4033",
+	.desc = "REQUEST = DATA_IN and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2033",
+	.desc = "REQUEST = DATA_IN and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3003",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf803",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4003",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2003",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3001",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf801",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4001",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2001",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3004",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf804",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4004",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2004",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3002",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf802",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4002",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2002",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3080",
+	.desc = "REQUEST = OTHER and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf880",
+	.desc = "REQUEST = OTHER and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4080",
+	.desc = "REQUEST = OTHER and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2080",
+	.desc = "REQUEST = OTHER and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3050",
+	.desc = "REQUEST = PF_DATA and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf850",
+	.desc = "REQUEST = PF_DATA and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4050",
+	.desc = "REQUEST = PF_DATA and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2050",
+	.desc = "REQUEST = PF_DATA and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3010",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf810",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4010",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2010",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3040",
+	.desc = "REQUEST = PF_RFO and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf840",
+	.desc = "REQUEST = PF_RFO and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4040",
+	.desc = "REQUEST = PF_RFO and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2040",
+	.desc = "REQUEST = PF_RFO and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3020",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf820",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4020",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2020",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.any_dram_and_remote_fwd",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3070",
+	.desc = "REQUEST = PREFETCH and RESPONSE = ANY_DRAM AND REMOTE_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xf870",
+	.desc = "REQUEST = PREFETCH and RESPONSE = ANY_LLC_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.other_local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4070",
+	.desc = "REQUEST = PREFETCH and RESPONSE = OTHER_LOCAL_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2070",
+	.desc = "REQUEST = PREFETCH and RESPONSE = REMOTE_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "dtlb_load_misses.any",
+	.event = "event=0x8,umask=0x1,period=200000",
+	.desc = "DTLB load misses",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.large_walk_completed",
+	.event = "event=0x8,umask=0x80,period=200000",
+	.desc = "DTLB load miss large page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.pde_miss",
+	.event = "event=0x8,umask=0x20,period=200000",
+	.desc = "DTLB load miss caused by low part of address",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x8,umask=0x10,period=2000000",
+	.desc = "DTLB second level hit",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x8,umask=0x2,period=200000",
+	.desc = "DTLB load miss page walks complete",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_cycles",
+	.event = "event=0x8,umask=0x4,period=200000",
+	.desc = "DTLB load miss page walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.any",
+	.event = "event=0x49,umask=0x1,period=200000",
+	.desc = "DTLB misses",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.large_walk_completed",
+	.event = "event=0x49,umask=0x80,period=200000",
+	.desc = "DTLB miss large page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.pde_miss",
+	.event = "event=0x49,umask=0x20,period=200000",
+	.desc = "DTLB misses casued by low part of address",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.stlb_hit",
+	.event = "event=0x49,umask=0x10,period=200000",
+	.desc = "DTLB first level misses but second level hit",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.walk_completed",
+	.event = "event=0x49,umask=0x2,period=200000",
+	.desc = "DTLB miss page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.walk_cycles",
+	.event = "event=0x49,umask=0x4,period=2000000",
+	.desc = "DTLB miss page walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4F,umask=0x10,period=2000000",
+	.desc = "Extended Page Table walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=2000000",
+	.desc = "ITLB flushes",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_miss_retired",
+	.event = "event=0xC8,umask=0x20,period=200000",
+	.desc = "Retired instructions that missed the ITLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.any",
+	.event = "event=0x85,umask=0x1,period=200000",
+	.desc = "ITLB miss",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.large_walk_completed",
+	.event = "event=0x85,umask=0x80,period=200000",
+	.desc = "ITLB miss large page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0x2,period=200000",
+	.desc = "ITLB miss page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_cycles",
+	.event = "event=0x85,umask=0x4,period=2000000",
+	.desc = "ITLB miss page walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_load_retired.dtlb_miss",
+	.event = "event=0xCB,umask=0x80,period=200000",
+	.desc = "Retired loads that miss the DTLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_store_retired.dtlb_miss",
+	.event = "event=0xC,umask=0x1,period=200000",
+	.desc = "Retired stores that miss the DTLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "cache_lock_cycles.l1d",
+	.event = "event=0x63,umask=0x2,period=2000000",
+	.desc = "Cycles L1D locked",
+	.topic = "cache",
+},
+{
+	.name = "cache_lock_cycles.l1d_l2",
+	.event = "event=0x63,umask=0x1,period=2000000",
+	.desc = "Cycles L1D and L2 locked",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_evict",
+	.event = "event=0x51,umask=0x4,period=2000000",
+	.desc = "L1D cache lines replaced in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_repl",
+	.event = "event=0x51,umask=0x2,period=2000000",
+	.desc = "L1D cache lines allocated in the M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_snoop_evict",
+	.event = "event=0x51,umask=0x8,period=2000000",
+	.desc = "L1D snoop eviction of cache lines in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.repl",
+	.event = "event=0x51,umask=0x1,period=2000000",
+	.desc = "L1 data cache lines allocated",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_prefetch_lock_fb_hit",
+	.event = "event=0x52,umask=0x1,period=2000000",
+	.desc = "L1D prefetch load lock accepted in fill buffer",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.miss",
+	.event = "event=0x4E,umask=0x2,period=200000",
+	.desc = "L1D hardware prefetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.requests",
+	.event = "event=0x4E,umask=0x1,period=200000",
+	.desc = "L1D hardware prefetch requests",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.triggers",
+	.event = "event=0x4E,umask=0x4,period=200000",
+	.desc = "L1D hardware prefetch requests triggered",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.e_state",
+	.event = "event=0x28,umask=0x4,period=100000",
+	.desc = "L1 writebacks to L2 in E state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.i_state",
+	.event = "event=0x28,umask=0x1,period=100000",
+	.desc = "L1 writebacks to L2 in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.m_state",
+	.event = "event=0x28,umask=0x8,period=100000",
+	.desc = "L1 writebacks to L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.mesi",
+	.event = "event=0x28,umask=0xf,period=100000",
+	.desc = "All L1 writebacks to L2",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.s_state",
+	.event = "event=0x28,umask=0x2,period=100000",
+	.desc = "L1 writebacks to L2 in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.any",
+	.event = "event=0x26,umask=0xff,period=200000",
+	.desc = "All L2 data requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.e_state",
+	.event = "event=0x26,umask=0x4,period=200000",
+	.desc = "L2 data demand loads in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.i_state",
+	.event = "event=0x26,umask=0x1,period=200000",
+	.desc = "L2 data demand loads in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.m_state",
+	.event = "event=0x26,umask=0x8,period=200000",
+	.desc = "L2 data demand loads in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.mesi",
+	.event = "event=0x26,umask=0xf,period=200000",
+	.desc = "L2 data demand requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.s_state",
+	.event = "event=0x26,umask=0x2,period=200000",
+	.desc = "L2 data demand loads in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.e_state",
+	.event = "event=0x26,umask=0x40,period=200000",
+	.desc = "L2 data prefetches in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.i_state",
+	.event = "event=0x26,umask=0x10,period=200000",
+	.desc = "L2 data prefetches in the I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.m_state",
+	.event = "event=0x26,umask=0x80,period=200000",
+	.desc = "L2 data prefetches in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.mesi",
+	.event = "event=0x26,umask=0xf0,period=200000",
+	.desc = "All L2 data prefetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.s_state",
+	.event = "event=0x26,umask=0x20,period=200000",
+	.desc = "L2 data prefetches in the S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.any",
+	.event = "event=0xF1,umask=0x7,period=100000",
+	.desc = "L2 lines alloacated",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.e_state",
+	.event = "event=0xF1,umask=0x4,period=100000",
+	.desc = "L2 lines allocated in the E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.s_state",
+	.event = "event=0xF1,umask=0x2,period=100000",
+	.desc = "L2 lines allocated in the S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.any",
+	.event = "event=0xF2,umask=0xf,period=100000",
+	.desc = "L2 lines evicted",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x1,period=100000",
+	.desc = "L2 lines evicted by a demand request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_dirty",
+	.event = "event=0xF2,umask=0x2,period=100000",
+	.desc = "L2 modified lines evicted by a demand request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.prefetch_clean",
+	.event = "event=0xF2,umask=0x4,period=100000",
+	.desc = "L2 lines evicted by a prefetch request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.prefetch_dirty",
+	.event = "event=0xF2,umask=0x8,period=100000",
+	.desc = "L2 modified lines evicted by a prefetch request",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetch_hit",
+	.event = "event=0x24,umask=0x10,period=200000",
+	.desc = "L2 instruction fetch hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetch_miss",
+	.event = "event=0x24,umask=0x20,period=200000",
+	.desc = "L2 instruction fetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetches",
+	.event = "event=0x24,umask=0x30,period=200000",
+	.desc = "L2 instruction fetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ld_hit",
+	.event = "event=0x24,umask=0x1,period=200000",
+	.desc = "L2 load hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ld_miss",
+	.event = "event=0x24,umask=0x2,period=200000",
+	.desc = "L2 load misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.loads",
+	.event = "event=0x24,umask=0x3,period=200000",
+	.desc = "L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.miss",
+	.event = "event=0x24,umask=0xaa,period=200000",
+	.desc = "All L2 misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetch_hit",
+	.event = "event=0x24,umask=0x40,period=200000",
+	.desc = "L2 prefetch hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetch_miss",
+	.event = "event=0x24,umask=0x80,period=200000",
+	.desc = "L2 prefetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetches",
+	.event = "event=0x24,umask=0xc0,period=200000",
+	.desc = "All L2 prefetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.references",
+	.event = "event=0x24,umask=0xff,period=200000",
+	.desc = "All L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x4,period=200000",
+	.desc = "L2 RFO hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x8,period=200000",
+	.desc = "L2 RFO misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfos",
+	.event = "event=0x24,umask=0xc,period=200000",
+	.desc = "L2 RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.any",
+	.event = "event=0xF0,umask=0x80,period=200000",
+	.desc = "All L2 transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.fill",
+	.event = "event=0xF0,umask=0x20,period=200000",
+	.desc = "L2 fill transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.ifetch",
+	.event = "event=0xF0,umask=0x4,period=200000",
+	.desc = "L2 instruction fetch transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200000",
+	.desc = "L1D writeback to L2 transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.load",
+	.event = "event=0xF0,umask=0x1,period=200000",
+	.desc = "L2 Load transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.prefetch",
+	.event = "event=0xF0,umask=0x8,period=200000",
+	.desc = "L2 prefetch transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.rfo",
+	.event = "event=0xF0,umask=0x2,period=200000",
+	.desc = "L2 RFO transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.wb",
+	.event = "event=0xF0,umask=0x40,period=200000",
+	.desc = "L2 writeback to LLC transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.e_state",
+	.event = "event=0x27,umask=0x40,period=100000",
+	.desc = "L2 demand lock RFOs in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.hit",
+	.event = "event=0x27,umask=0xe0,period=100000",
+	.desc = "All demand L2 lock RFOs that hit the cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.i_state",
+	.event = "event=0x27,umask=0x10,period=100000",
+	.desc = "L2 demand lock RFOs in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.m_state",
+	.event = "event=0x27,umask=0x80,period=100000",
+	.desc = "L2 demand lock RFOs in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.mesi",
+	.event = "event=0x27,umask=0xf0,period=100000",
+	.desc = "All demand L2 lock RFOs",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.s_state",
+	.event = "event=0x27,umask=0x20,period=100000",
+	.desc = "L2 demand lock RFOs in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.hit",
+	.event = "event=0x27,umask=0xe,period=100000",
+	.desc = "All L2 demand store RFOs that hit the cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.i_state",
+	.event = "event=0x27,umask=0x1,period=100000",
+	.desc = "L2 demand store RFOs in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.m_state",
+	.event = "event=0x27,umask=0x8,period=100000",
+	.desc = "L2 demand store RFOs in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.mesi",
+	.event = "event=0x27,umask=0xf,period=100000",
+	.desc = "All L2 demand store RFOs",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.s_state",
+	.event = "event=0x27,umask=0x2,period=100000",
+	.desc = "L2 demand store RFOs in S state",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100000",
+	.desc = "Longest latency cache miss",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=200000",
+	.desc = "Longest latency cache reference",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.loads",
+	.event = "event=0xB,umask=0x1,period=2000000",
+	.desc = "Instructions retired which contains a load (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.stores",
+	.event = "event=0xB,umask=0x2,period=2000000",
+	.desc = "Instructions retired which contains a store (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.hit_lfb",
+	.event = "event=0xCB,umask=0x40,period=200000",
+	.desc = "Retired loads that miss L1D and hit an previously allocated LFB (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l1d_hit",
+	.event = "event=0xCB,umask=0x1,period=2000000",
+	.desc = "Retired loads that hit the L1 data cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l2_hit",
+	.event = "event=0xCB,umask=0x2,period=200000",
+	.desc = "Retired loads that hit the L2 cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.llc_miss",
+	.event = "event=0xCB,umask=0x10,period=10000",
+	.desc = "Retired loads that miss the LLC cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.llc_unshared_hit",
+	.event = "event=0xCB,umask=0x4,period=40000",
+	.desc = "Retired loads that hit valid versions in the LLC cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.other_core_l2_hit_hitm",
+	.event = "event=0xCB,umask=0x8,period=40000",
+	.desc = "Retired loads that hit sibling core's L2 in modified or unmodified states (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.any",
+	.event = "event=0xB0,umask=0x80,period=100000",
+	.desc = "All offcore requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.any.read",
+	.event = "event=0xB0,umask=0x8,period=100000",
+	.desc = "Offcore read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.any.rfo",
+	.event = "event=0xB0,umask=0x10,period=100000",
+	.desc = "Offcore RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand.read_code",
+	.event = "event=0xB0,umask=0x2,period=100000",
+	.desc = "Offcore demand code read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand.read_data",
+	.event = "event=0xB0,umask=0x1,period=100000",
+	.desc = "Offcore demand data read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand.rfo",
+	.event = "event=0xB0,umask=0x4,period=100000",
+	.desc = "Offcore demand RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.l1d_writeback",
+	.event = "event=0xB0,umask=0x40,period=100000",
+	.desc = "Offcore L1 data cache writebacks",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.any.read",
+	.event = "event=0x60,umask=0x8,period=2000000",
+	.desc = "Outstanding offcore reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.any.read_not_empty",
+	.event = "event=0x60,umask=0x8,period=2000000,cmask=1",
+	.desc = "Cycles offcore reads busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_code",
+	.event = "event=0x60,umask=0x2,period=2000000",
+	.desc = "Outstanding offcore demand code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_code_not_empty",
+	.event = "event=0x60,umask=0x2,period=2000000,cmask=1",
+	.desc = "Cycles offcore demand code read busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_data",
+	.event = "event=0x60,umask=0x1,period=2000000",
+	.desc = "Outstanding offcore demand data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_data_not_empty",
+	.event = "event=0x60,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles offcore demand data read busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.rfo",
+	.event = "event=0x60,umask=0x4,period=2000000",
+	.desc = "Outstanding offcore demand RFOs",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.rfo_not_empty",
+	.event = "event=0x60,umask=0x4,period=2000000,cmask=1",
+	.desc = "Cycles offcore demand RFOs busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_sq_full",
+	.event = "event=0xB2,umask=0x1,period=100000",
+	.desc = "Offcore requests blocked due to Super Queue full",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.lru_hints",
+	.event = "event=0xF4,umask=0x4,period=2000000",
+	.desc = "Super Queue LRU hints sent to LLC",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xF4,umask=0x10,period=2000000",
+	.desc = "Super Queue lock splits across a cache line",
+	.topic = "cache",
+},
+{
+	.name = "store_blocks.at_ret",
+	.event = "event=0x6,umask=0x4,period=200000",
+	.desc = "Loads delayed with at-Retirement block code",
+	.topic = "cache",
+},
+{
+	.name = "store_blocks.l1d_block",
+	.event = "event=0x6,umask=0x8,period=200000",
+	.desc = "Cacheable loads delayed with L1D block code",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_0",
+	.event = "event=0xB,umask=0x10,period=2000000,ldlat=0x0",
+	.desc = "Memory instructions retired above 0 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_1024",
+	.event = "event=0xB,umask=0x10,period=100,ldlat=0x400",
+	.desc = "Memory instructions retired above 1024 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_128",
+	.event = "event=0xB,umask=0x10,period=1000,ldlat=0x80",
+	.desc = "Memory instructions retired above 128 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_16",
+	.event = "event=0xB,umask=0x10,period=10000,ldlat=0x10",
+	.desc = "Memory instructions retired above 16 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_16384",
+	.event = "event=0xB,umask=0x10,period=5,ldlat=0x4000",
+	.desc = "Memory instructions retired above 16384 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_2048",
+	.event = "event=0xB,umask=0x10,period=50,ldlat=0x800",
+	.desc = "Memory instructions retired above 2048 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_256",
+	.event = "event=0xB,umask=0x10,period=500,ldlat=0x100",
+	.desc = "Memory instructions retired above 256 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_32",
+	.event = "event=0xB,umask=0x10,period=5000,ldlat=0x20",
+	.desc = "Memory instructions retired above 32 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_32768",
+	.event = "event=0xB,umask=0x10,period=3,ldlat=0x8000",
+	.desc = "Memory instructions retired above 32768 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_4",
+	.event = "event=0xB,umask=0x10,period=50000,ldlat=0x4",
+	.desc = "Memory instructions retired above 4 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_4096",
+	.event = "event=0xB,umask=0x10,period=20,ldlat=0x1000",
+	.desc = "Memory instructions retired above 4096 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_512",
+	.event = "event=0xB,umask=0x10,period=200,ldlat=0x200",
+	.desc = "Memory instructions retired above 512 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_64",
+	.event = "event=0xB,umask=0x10,period=2000,ldlat=0x40",
+	.desc = "Memory instructions retired above 64 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_8",
+	.event = "event=0xB,umask=0x10,period=20000,ldlat=0x8",
+	.desc = "Memory instructions retired above 8 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_8192",
+	.event = "event=0xB,umask=0x10,period=10,ldlat=0x2000",
+	.desc = "Memory instructions retired above 8192 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5011",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f11",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff11",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8011",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x111",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x211",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x411",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x711",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1011",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x811",
+	.desc = "REQUEST = ANY_DATA read and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5044",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f44",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff44",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8044",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x144",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x244",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x444",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x744",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1044",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x844",
+	.desc = "REQUEST = ANY IFETCH and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x50ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7fff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xffff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x80ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x10ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8ff",
+	.desc = "REQUEST = ANY_REQUEST and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5022",
+	.desc = "REQUEST = ANY RFO and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f22",
+	.desc = "REQUEST = ANY RFO and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff22",
+	.desc = "REQUEST = ANY RFO and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8022",
+	.desc = "REQUEST = ANY RFO and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x122",
+	.desc = "REQUEST = ANY RFO and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x222",
+	.desc = "REQUEST = ANY RFO and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x422",
+	.desc = "REQUEST = ANY RFO and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x722",
+	.desc = "REQUEST = ANY RFO and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1022",
+	.desc = "REQUEST = ANY RFO and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x822",
+	.desc = "REQUEST = ANY RFO and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5008",
+	.desc = "REQUEST = CORE_WB and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f08",
+	.desc = "REQUEST = CORE_WB and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff08",
+	.desc = "REQUEST = CORE_WB and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8008",
+	.desc = "REQUEST = CORE_WB and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x108",
+	.desc = "REQUEST = CORE_WB and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x208",
+	.desc = "REQUEST = CORE_WB and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x408",
+	.desc = "REQUEST = CORE_WB and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x708",
+	.desc = "REQUEST = CORE_WB and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1008",
+	.desc = "REQUEST = CORE_WB and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x808",
+	.desc = "REQUEST = CORE_WB and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5077",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f77",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff77",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8077",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x177",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x277",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x477",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x777",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1077",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x877",
+	.desc = "REQUEST = DATA_IFETCH and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5033",
+	.desc = "REQUEST = DATA_IN and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f33",
+	.desc = "REQUEST = DATA_IN and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff33",
+	.desc = "REQUEST = DATA_IN and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8033",
+	.desc = "REQUEST = DATA_IN and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x133",
+	.desc = "REQUEST = DATA_IN and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x233",
+	.desc = "REQUEST = DATA_IN and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x433",
+	.desc = "REQUEST = DATA_IN and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x733",
+	.desc = "REQUEST = DATA_IN and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1033",
+	.desc = "REQUEST = DATA_IN and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x833",
+	.desc = "REQUEST = DATA_IN and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5003",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f03",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff03",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8003",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x103",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x203",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x403",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x703",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1003",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x803",
+	.desc = "REQUEST = DEMAND_DATA and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5001",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f01",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff01",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8001",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x101",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x201",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x401",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x701",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1001",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x801",
+	.desc = "REQUEST = DEMAND_DATA_RD and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5004",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f04",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff04",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8004",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x104",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x204",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x404",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x704",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1004",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x804",
+	.desc = "REQUEST = DEMAND_IFETCH and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5002",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f02",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff02",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8002",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x102",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x202",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x402",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x702",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1002",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x802",
+	.desc = "REQUEST = DEMAND_RFO and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5080",
+	.desc = "REQUEST = OTHER and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f80",
+	.desc = "REQUEST = OTHER and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff80",
+	.desc = "REQUEST = OTHER and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8080",
+	.desc = "REQUEST = OTHER and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x180",
+	.desc = "REQUEST = OTHER and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x280",
+	.desc = "REQUEST = OTHER and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x480",
+	.desc = "REQUEST = OTHER and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x780",
+	.desc = "REQUEST = OTHER and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1080",
+	.desc = "REQUEST = OTHER and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x880",
+	.desc = "REQUEST = OTHER and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5050",
+	.desc = "REQUEST = PF_DATA and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f50",
+	.desc = "REQUEST = PF_DATA and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff50",
+	.desc = "REQUEST = PF_DATA and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8050",
+	.desc = "REQUEST = PF_DATA and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x150",
+	.desc = "REQUEST = PF_DATA and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x250",
+	.desc = "REQUEST = PF_DATA and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x450",
+	.desc = "REQUEST = PF_DATA and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x750",
+	.desc = "REQUEST = PF_DATA and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1050",
+	.desc = "REQUEST = PF_DATA and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x850",
+	.desc = "REQUEST = PF_DATA and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5010",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f10",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff10",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8010",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x110",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x210",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x410",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x710",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1010",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x810",
+	.desc = "REQUEST = PF_DATA_RD and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5040",
+	.desc = "REQUEST = PF_RFO and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f40",
+	.desc = "REQUEST = PF_RFO and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff40",
+	.desc = "REQUEST = PF_RFO and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8040",
+	.desc = "REQUEST = PF_RFO and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x140",
+	.desc = "REQUEST = PF_RFO and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x240",
+	.desc = "REQUEST = PF_RFO and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x440",
+	.desc = "REQUEST = PF_RFO and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x740",
+	.desc = "REQUEST = PF_RFO and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1040",
+	.desc = "REQUEST = PF_RFO and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x840",
+	.desc = "REQUEST = PF_RFO and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5020",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f20",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff20",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8020",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x120",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x220",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x420",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x720",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1020",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x820",
+	.desc = "REQUEST = PF_IFETCH and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.all_local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5070",
+	.desc = "REQUEST = PREFETCH and RESPONSE = ALL_LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7f70",
+	.desc = "REQUEST = PREFETCH and RESPONSE = ANY_CACHE_DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xff70",
+	.desc = "REQUEST = PREFETCH and RESPONSE = ANY_LOCATION",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8070",
+	.desc = "REQUEST = PREFETCH and RESPONSE = IO_CSR_MMIO",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x170",
+	.desc = "REQUEST = PREFETCH and RESPONSE = LLC_HIT_NO_OTHER_CORE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x270",
+	.desc = "REQUEST = PREFETCH and RESPONSE = LLC_HIT_OTHER_CORE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x470",
+	.desc = "REQUEST = PREFETCH and RESPONSE = LLC_HIT_OTHER_CORE_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x770",
+	.desc = "REQUEST = PREFETCH and RESPONSE = LOCAL_CACHE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.local_dram_and_remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1070",
+	.desc = "REQUEST = PREFETCH and RESPONSE = LOCAL_DRAM AND REMOTE_CACHE_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x870",
+	.desc = "REQUEST = PREFETCH and RESPONSE = REMOTE_CACHE_HITM",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_westmereep_sp[] = {
+{
+	.name = "fp_assist.all",
+	.event = "event=0xF7,umask=0x1,period=20000",
+	.desc = "X87 Floating point assists (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.input",
+	.event = "event=0xF7,umask=0x4,period=20000",
+	.desc = "X87 Floating poiint assists for invalid input value (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.output",
+	.event = "event=0xF7,umask=0x2,period=20000",
+	.desc = "X87 Floating point assists for invalid output value (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.mmx",
+	.event = "event=0x10,umask=0x2,period=2000000",
+	.desc = "MMX Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_double_precision",
+	.event = "event=0x10,umask=0x80,period=2000000",
+	.desc = "SSE* FP double precision Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp",
+	.event = "event=0x10,umask=0x4,period=2000000",
+	.desc = "SSE and SSE2 FP Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp_packed",
+	.event = "event=0x10,umask=0x10,period=2000000",
+	.desc = "SSE FP packed Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp_scalar",
+	.event = "event=0x10,umask=0x20,period=2000000",
+	.desc = "SSE FP scalar Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_single_precision",
+	.event = "event=0x10,umask=0x40,period=2000000",
+	.desc = "SSE* FP single precision Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse2_integer",
+	.event = "event=0x10,umask=0x8,period=2000000",
+	.desc = "SSE2 integer Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.x87",
+	.event = "event=0x10,umask=0x1,period=2000000",
+	.desc = "Computational floating-point operations executed",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.any",
+	.event = "event=0xCC,umask=0x3,period=2000000",
+	.desc = "All Floating Point to and from MMX transitions",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.to_fp",
+	.event = "event=0xCC,umask=0x1,period=2000000",
+	.desc = "Transitions from MMX to Floating Point instructions",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.to_mmx",
+	.event = "event=0xCC,umask=0x2,period=2000000",
+	.desc = "Transitions from Floating Point to MMX instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.pack",
+	.event = "event=0x12,umask=0x4,period=200000",
+	.desc = "128 bit SIMD integer pack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_arith",
+	.event = "event=0x12,umask=0x20,period=200000",
+	.desc = "128 bit SIMD integer arithmetic operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_logical",
+	.event = "event=0x12,umask=0x10,period=200000",
+	.desc = "128 bit SIMD integer logical operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_mpy",
+	.event = "event=0x12,umask=0x1,period=200000",
+	.desc = "128 bit SIMD integer multiply operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_shift",
+	.event = "event=0x12,umask=0x2,period=200000",
+	.desc = "128 bit SIMD integer shift operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.shuffle_move",
+	.event = "event=0x12,umask=0x40,period=200000",
+	.desc = "128 bit SIMD integer shuffle/move operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.unpack",
+	.event = "event=0x12,umask=0x8,period=200000",
+	.desc = "128 bit SIMD integer unpack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.pack",
+	.event = "event=0xFD,umask=0x4,period=200000",
+	.desc = "SIMD integer 64 bit pack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_arith",
+	.event = "event=0xFD,umask=0x20,period=200000",
+	.desc = "SIMD integer 64 bit arithmetic operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_logical",
+	.event = "event=0xFD,umask=0x10,period=200000",
+	.desc = "SIMD integer 64 bit logical operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_mpy",
+	.event = "event=0xFD,umask=0x1,period=200000",
+	.desc = "SIMD integer 64 bit packed multiply operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_shift",
+	.event = "event=0xFD,umask=0x2,period=200000",
+	.desc = "SIMD integer 64 bit shift operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.shuffle_move",
+	.event = "event=0xFD,umask=0x40,period=200000",
+	.desc = "SIMD integer 64 bit shuffle/move operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.unpack",
+	.event = "event=0xFD,umask=0x8,period=200000",
+	.desc = "SIMD integer 64 bit unpack operations",
+	.topic = "floating point",
+},
+{
+	.name = "arith.cycles_div_busy",
+	.event = "event=0x14,umask=0x1,period=2000000",
+	.desc = "Cycles the divider is busy",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.div",
+	.event = "event=0x14,inv=1,umask=0x1,period=2000000,cmask=1,edge=1",
+	.desc = "Divide Operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.mul",
+	.event = "event=0x14,umask=0x2,period=2000000",
+	.desc = "Multiply operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear.bad_target",
+	.event = "event=0xE6,umask=0x2,period=2000000",
+	.desc = "BACLEAR asserted with bad target address",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear.clear",
+	.event = "event=0xE6,umask=0x1,period=2000000",
+	.desc = "BACLEAR asserted, regardless of cause ",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear_force_iq",
+	.event = "event=0xA7,umask=0x1,period=2000000",
+	.desc = "Instruction queue forced BACLEAR",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_decoded",
+	.event = "event=0xE0,umask=0x1,period=2000000",
+	.desc = "Branch instructions decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.any",
+	.event = "event=0x88,umask=0x7f,period=200000",
+	.desc = "Branch instructions executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.cond",
+	.event = "event=0x88,umask=0x1,period=200000",
+	.desc = "Conditional branch instructions executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.direct",
+	.event = "event=0x88,umask=0x2,period=200000",
+	.desc = "Unconditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.direct_near_call",
+	.event = "event=0x88,umask=0x10,period=20000",
+	.desc = "Unconditional call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.indirect_near_call",
+	.event = "event=0x88,umask=0x20,period=20000",
+	.desc = "Indirect call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.indirect_non_call",
+	.event = "event=0x88,umask=0x4,period=20000",
+	.desc = "Indirect non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.near_calls",
+	.event = "event=0x88,umask=0x30,period=20000",
+	.desc = "Call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.non_calls",
+	.event = "event=0x88,umask=0x7,period=200000",
+	.desc = "All non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.return_near",
+	.event = "event=0x88,umask=0x8,period=20000",
+	.desc = "Indirect return branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken",
+	.event = "event=0x88,umask=0x40,period=200000",
+	.desc = "Taken branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x4,period=200000",
+	.desc = "Retired branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=200000",
+	.desc = "Retired conditional branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=20000",
+	.desc = "Retired near call instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.any",
+	.event = "event=0x89,umask=0x7f,period=20000",
+	.desc = "Mispredicted branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.cond",
+	.event = "event=0x89,umask=0x1,period=20000",
+	.desc = "Mispredicted conditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.direct",
+	.event = "event=0x89,umask=0x2,period=20000",
+	.desc = "Mispredicted unconditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.direct_near_call",
+	.event = "event=0x89,umask=0x10,period=2000",
+	.desc = "Mispredicted non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.indirect_near_call",
+	.event = "event=0x89,umask=0x20,period=2000",
+	.desc = "Mispredicted indirect call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.indirect_non_call",
+	.event = "event=0x89,umask=0x4,period=2000",
+	.desc = "Mispredicted indirect non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.near_calls",
+	.event = "event=0x89,umask=0x30,period=2000",
+	.desc = "Mispredicted call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.non_calls",
+	.event = "event=0x89,umask=0x7,period=20000",
+	.desc = "Mispredicted non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.return_near",
+	.event = "event=0x89,umask=0x8,period=2000",
+	.desc = "Mispredicted return branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken",
+	.event = "event=0x89,umask=0x40,period=20000",
+	.desc = "Mispredicted taken branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x4,period=20000",
+	.desc = "Mispredicted retired branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=20000",
+	.desc = "Mispredicted conditional retired branches (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_call",
+	.event = "event=0xC5,umask=0x2,period=2000",
+	.desc = "Mispredicted near retired calls (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref",
+	.event = "event=0x0,umask=0x03",
+	.desc = "Reference cycles when thread is not halted (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_p",
+	.event = "event=0x3C,umask=0x1,period=100000",
+	.desc = "Reference base clock (133 Mhz) cycles when thread is not halted (programmable counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Cycles when thread is not halted (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000000",
+	.desc = "Cycles when thread is not halted (programmable counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.total_cycles",
+	.event = "event=0x3C,inv=1,umask=0x0,period=2000000,cmask=2",
+	.desc = "Total CPU cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.any",
+	.event = "event=0x87,umask=0xf,period=2000000",
+	.desc = "Any Instruction Length Decoder stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.iq_full",
+	.event = "event=0x87,umask=0x4,period=2000000",
+	.desc = "Instruction Queue full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000000",
+	.desc = "Length Change Prefix stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.mru",
+	.event = "event=0x87,umask=0x2,period=2000000",
+	.desc = "Stall cycles due to BPU MRU bypass",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.regen",
+	.event = "event=0x87,umask=0x8,period=2000000",
+	.desc = "Regen stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_decoded.dec0",
+	.event = "event=0x18,umask=0x1,period=2000000",
+	.desc = "Instructions that must be decoded by decoder 0",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_queue_write_cycles",
+	.event = "event=0x1E,umask=0x1,period=2000000",
+	.desc = "Cycles instructions are written to the instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_queue_writes",
+	.event = "event=0x17,umask=0x1,period=2000000",
+	.desc = "Instructions written to instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (Programmable counter and Precise Event) (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.mmx",
+	.event = "event=0xC0,umask=0x4,period=2000000",
+	.desc = "Retired MMX instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.total_cycles",
+	.event = "event=0xC0,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.x87",
+	.event = "event=0xC0,umask=0x2,period=2000000",
+	.desc = "Retired floating-point operations (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre",
+	.event = "event=0x4C,umask=0x1,period=200000",
+	.desc = "Load operations conflicting with software prefetches",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.active",
+	.event = "event=0xA8,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles when uops were delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.inactive",
+	.event = "event=0xA8,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no uops were delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd_overflow",
+	.event = "event=0x20,umask=0x1,period=2000000",
+	.desc = "Loops that can't stream from the instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.cycles",
+	.event = "event=0xC3,umask=0x1,period=20000",
+	.desc = "Cycles machine clear asserted",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.mem_order",
+	.event = "event=0xC3,umask=0x2,period=20000",
+	.desc = "Execution pipeline restart due to Memory ordering conflicts",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=20000",
+	.desc = "Self-Modifying Code detected",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000000",
+	.desc = "Resource related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.fpcw",
+	.event = "event=0xA2,umask=0x20,period=2000000",
+	.desc = "FPU control word write stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.load",
+	.event = "event=0xA2,umask=0x2,period=2000000",
+	.desc = "Load buffer stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.mxcsr",
+	.event = "event=0xA2,umask=0x40,period=2000000",
+	.desc = "MXCSR rename stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.other",
+	.event = "event=0xA2,umask=0x80,period=2000000",
+	.desc = "Other Resource related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rob_full",
+	.event = "event=0xA2,umask=0x10,period=2000000",
+	.desc = "ROB full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rs_full",
+	.event = "event=0xA2,umask=0x4,period=2000000",
+	.desc = "Reservation Station full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.store",
+	.event = "event=0xA2,umask=0x8,period=2000000",
+	.desc = "Store buffer stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.packed_double",
+	.event = "event=0xC7,umask=0x4,period=200000",
+	.desc = "SIMD Packed-Double Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.packed_single",
+	.event = "event=0xC7,umask=0x1,period=200000",
+	.desc = "SIMD Packed-Single Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.scalar_double",
+	.event = "event=0xC7,umask=0x8,period=200000",
+	.desc = "SIMD Scalar-Double Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.scalar_single",
+	.event = "event=0xC7,umask=0x2,period=200000",
+	.desc = "SIMD Scalar-Single Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.vector_integer",
+	.event = "event=0xC7,umask=0x10,period=200000",
+	.desc = "SIMD Vector Integer Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uop_unfusion",
+	.event = "event=0xDB,umask=0x1,period=2000000",
+	.desc = "Uop unfusions due to FP exceptions",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.esp_folding",
+	.event = "event=0xD1,umask=0x4,period=2000000",
+	.desc = "Stack pointer instructions decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.esp_sync",
+	.event = "event=0xD1,umask=0x8,period=2000000",
+	.desc = "Stack pointer sync operations",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.ms_cycles_active",
+	.event = "event=0xD1,umask=0x2,period=2000000,cmask=1",
+	.desc = "Uops decoded by Microcode Sequencer",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.stall_cycles",
+	.event = "event=0xD1,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops are decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_active_cycles",
+	.event = "event=0xB1,umask=0x3f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops executed on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_active_cycles_no_port5",
+	.event = "event=0xB1,umask=0x1f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops executed on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_count",
+	.event = "event=0xB1,inv=1,umask=0x3f,any=1,period=2000000,cmask=1,edge=1",
+	.desc = "Uops executed on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_count_no_port5",
+	.event = "event=0xB1,inv=1,umask=0x1f,any=1,period=2000000,cmask=1,edge=1",
+	.desc = "Uops executed on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x3f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_cycles_no_port5",
+	.event = "event=0xB1,inv=1,umask=0x1f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port0",
+	.event = "event=0xB1,umask=0x1,period=2000000",
+	.desc = "Uops executed on port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port015",
+	.event = "event=0xB1,umask=0x40,period=2000000",
+	.desc = "Uops issued on ports 0, 1 or 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port015_stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x40,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on ports 0, 1 or 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port1",
+	.event = "event=0xB1,umask=0x2,period=2000000",
+	.desc = "Uops executed on port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port2_core",
+	.event = "event=0xB1,umask=0x4,any=1,period=2000000",
+	.desc = "Uops executed on port 2 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port234_core",
+	.event = "event=0xB1,umask=0x80,any=1,period=2000000",
+	.desc = "Uops issued on ports 2, 3 or 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port3_core",
+	.event = "event=0xB1,umask=0x8,any=1,period=2000000",
+	.desc = "Uops executed on port 3 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port4_core",
+	.event = "event=0xB1,umask=0x10,any=1,period=2000000",
+	.desc = "Uops executed on port 4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port5",
+	.event = "event=0xB1,umask=0x20,period=2000000",
+	.desc = "Uops executed on port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0xE,umask=0x1,period=2000000",
+	.desc = "Uops issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.core_stall_cycles",
+	.event = "event=0xE,inv=1,umask=0x1,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops were issued on any thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.cycles_all_threads",
+	.event = "event=0xE,umask=0x1,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops were issued on either thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.fused",
+	.event = "event=0xE,umask=0x2,period=2000000",
+	.desc = "Fused Uops issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0xE,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops were issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.active_cycles",
+	.event = "event=0xC2,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles Uops are being retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.any",
+	.event = "event=0xC2,umask=0x1,period=2000000",
+	.desc = "Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.macro_fused",
+	.event = "event=0xC2,umask=0x4,period=2000000",
+	.desc = "Macro-fused Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000000",
+	.desc = "Retirement slots used (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles Uops are not retiring (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles using precise uop retired event (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.total_cycles_ps",
+	.event = "event=0xC0,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "macro_insts.decoded",
+	.event = "event=0xD0,umask=0x1,period=2000000",
+	.desc = "Instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "macro_insts.fusions_decoded",
+	.event = "event=0xA6,umask=0x1,period=2000000",
+	.desc = "Macro-fused instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "two_uop_insts_decoded",
+	.event = "event=0x19,umask=0x1,period=2000000",
+	.desc = "Two Uop instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "bpu_clears.early",
+	.event = "event=0xE8,umask=0x1,period=2000000",
+	.desc = "Early Branch Prediciton Unit clears",
+	.topic = "other",
+},
+{
+	.name = "bpu_clears.late",
+	.event = "event=0xE8,umask=0x2,period=2000000",
+	.desc = "Late Branch Prediction Unit clears",
+	.topic = "other",
+},
+{
+	.name = "bpu_missed_call_ret",
+	.event = "event=0xE5,umask=0x1,period=2000000",
+	.desc = "Branch prediction unit missed call or return",
+	.topic = "other",
+},
+{
+	.name = "es_reg_renames",
+	.event = "event=0xD5,umask=0x1,period=2000000",
+	.desc = "ES segment renames",
+	.topic = "other",
+},
+{
+	.name = "io_transactions",
+	.event = "event=0x6C,umask=0x1,period=2000000",
+	.desc = "I/O transactions",
+	.topic = "other",
+},
+{
+	.name = "l1i.cycles_stalled",
+	.event = "event=0x80,umask=0x4,period=2000000",
+	.desc = "L1I instruction fetch stall cycles",
+	.topic = "other",
+},
+{
+	.name = "l1i.hits",
+	.event = "event=0x80,umask=0x1,period=2000000",
+	.desc = "L1I instruction fetch hits",
+	.topic = "other",
+},
+{
+	.name = "l1i.misses",
+	.event = "event=0x80,umask=0x2,period=2000000",
+	.desc = "L1I instruction fetch misses",
+	.topic = "other",
+},
+{
+	.name = "l1i.reads",
+	.event = "event=0x80,umask=0x3,period=2000000",
+	.desc = "L1I Instruction fetches",
+	.topic = "other",
+},
+{
+	.name = "large_itlb.hit",
+	.event = "event=0x82,umask=0x1,period=200000",
+	.desc = "Large ITLB hit",
+	.topic = "other",
+},
+{
+	.name = "load_block.overlap_store",
+	.event = "event=0x3,umask=0x2,period=200000",
+	.desc = "Loads that partially overlap an earlier store",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.any",
+	.event = "event=0x13,umask=0x7,period=2000000",
+	.desc = "All loads dispatched",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.mob",
+	.event = "event=0x13,umask=0x4,period=2000000",
+	.desc = "Loads dispatched from the MOB",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.rs",
+	.event = "event=0x13,umask=0x1,period=2000000",
+	.desc = "Loads dispatched that bypass the MOB",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.rs_delayed",
+	.event = "event=0x13,umask=0x2,period=2000000",
+	.desc = "Loads dispatched from stage 305",
+	.topic = "other",
+},
+{
+	.name = "partial_address_alias",
+	.event = "event=0x7,umask=0x1,period=200000",
+	.desc = "False dependencies due to partial address aliasing",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.any",
+	.event = "event=0xD2,umask=0xf,period=2000000",
+	.desc = "All RAT stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.flags",
+	.event = "event=0xD2,umask=0x1,period=2000000",
+	.desc = "Flag stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.registers",
+	.event = "event=0xD2,umask=0x2,period=2000000",
+	.desc = "Partial register stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.rob_read_port",
+	.event = "event=0xD2,umask=0x4,period=2000000",
+	.desc = "ROB read port stalls cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.scoreboard",
+	.event = "event=0xD2,umask=0x8,period=2000000",
+	.desc = "Scoreboard stall cycles",
+	.topic = "other",
+},
+{
+	.name = "sb_drain.any",
+	.event = "event=0x4,umask=0x7,period=200000",
+	.desc = "All Store buffer stall cycles",
+	.topic = "other",
+},
+{
+	.name = "seg_rename_stalls",
+	.event = "event=0xD4,umask=0x1,period=2000000",
+	.desc = "Segment rename stall cycles",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hit",
+	.event = "event=0xB8,umask=0x1,period=100000",
+	.desc = "Thread responded HIT to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hite",
+	.event = "event=0xB8,umask=0x2,period=100000",
+	.desc = "Thread responded HITE to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hitm",
+	.event = "event=0xB8,umask=0x4,period=100000",
+	.desc = "Thread responded HITM to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests.code",
+	.event = "event=0xB4,umask=0x4,period=100000",
+	.desc = "Snoop code requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests.data",
+	.event = "event=0xB4,umask=0x1,period=100000",
+	.desc = "Snoop data requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests.invalidate",
+	.event = "event=0xB4,umask=0x2,period=100000",
+	.desc = "Snoop invalidate requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.code",
+	.event = "event=0xB3,umask=0x4,period=2000000",
+	.desc = "Outstanding snoop code requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.code_not_empty",
+	.event = "event=0xB3,umask=0x4,period=2000000,cmask=1",
+	.desc = "Cycles snoop code requests queued",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.data",
+	.event = "event=0xB3,umask=0x1,period=2000000",
+	.desc = "Outstanding snoop data requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.data_not_empty",
+	.event = "event=0xB3,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles snoop data requests queued",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.invalidate",
+	.event = "event=0xB3,umask=0x2,period=2000000",
+	.desc = "Outstanding snoop invalidate requests",
+	.topic = "other",
+},
+{
+	.name = "snoopq_requests_outstanding.invalidate_not_empty",
+	.event = "event=0xB3,umask=0x2,period=2000000,cmask=1",
+	.desc = "Cycles snoop invalidate requests queued",
+	.topic = "other",
+},
+{
+	.name = "sq_full_stall_cycles",
+	.event = "event=0xF6,umask=0x1,period=2000000",
+	.desc = "Super Queue full stall cycles",
+	.topic = "other",
+},
+{
+	.name = "offcore_response.any_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6011",
+	.desc = "Offcore data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF811",
+	.desc = "Offcore data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2011",
+	.desc = "Offcore data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4011",
+	.desc = "Offcore data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6044",
+	.desc = "Offcore code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF844",
+	.desc = "Offcore code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2044",
+	.desc = "Offcore code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4044",
+	.desc = "Offcore code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x60FF",
+	.desc = "Offcore requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF8FF",
+	.desc = "Offcore requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x20FF",
+	.desc = "Offcore requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x40FF",
+	.desc = "Offcore requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6022",
+	.desc = "Offcore RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF822",
+	.desc = "Offcore RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2022",
+	.desc = "Offcore RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4022",
+	.desc = "Offcore RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6008",
+	.desc = "Offcore writebacks to any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF808",
+	.desc = "Offcore writebacks that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2008",
+	.desc = "Offcore writebacks to the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4008",
+	.desc = "Offcore writebacks to a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6077",
+	.desc = "Offcore code or data read requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF877",
+	.desc = "Offcore code or data read requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2077",
+	.desc = "Offcore code or data read requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4077",
+	.desc = "Offcore code or data read requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6033",
+	.desc = "Offcore request = all data, response = any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF833",
+	.desc = "Offcore request = all data, response = any LLC miss",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2033",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4033",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6003",
+	.desc = "Offcore demand data requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF803",
+	.desc = "Offcore demand data requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2003",
+	.desc = "Offcore demand data requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4003",
+	.desc = "Offcore demand data requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6001",
+	.desc = "Offcore demand data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF801",
+	.desc = "Offcore demand data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2001",
+	.desc = "Offcore demand data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4001",
+	.desc = "Offcore demand data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6004",
+	.desc = "Offcore demand code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF804",
+	.desc = "Offcore demand code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2004",
+	.desc = "Offcore demand code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4004",
+	.desc = "Offcore demand code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6002",
+	.desc = "Offcore demand RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF802",
+	.desc = "Offcore demand RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2002",
+	.desc = "Offcore demand RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4002",
+	.desc = "Offcore demand RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6080",
+	.desc = "Offcore other requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF880",
+	.desc = "Offcore other requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4080",
+	.desc = "Offcore other requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6050",
+	.desc = "Offcore prefetch data requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF850",
+	.desc = "Offcore prefetch data requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2050",
+	.desc = "Offcore prefetch data requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4050",
+	.desc = "Offcore prefetch data requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6010",
+	.desc = "Offcore prefetch data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF810",
+	.desc = "Offcore prefetch data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2010",
+	.desc = "Offcore prefetch data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4010",
+	.desc = "Offcore prefetch data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6040",
+	.desc = "Offcore prefetch code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF840",
+	.desc = "Offcore prefetch code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2040",
+	.desc = "Offcore prefetch code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4040",
+	.desc = "Offcore prefetch code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6020",
+	.desc = "Offcore prefetch RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF820",
+	.desc = "Offcore prefetch RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2020",
+	.desc = "Offcore prefetch RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4020",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6070",
+	.desc = "Offcore prefetch requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF870",
+	.desc = "Offcore prefetch requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2070",
+	.desc = "Offcore prefetch requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4070",
+	.desc = "Offcore prefetch requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "dtlb_load_misses.any",
+	.event = "event=0x8,umask=0x1,period=200000",
+	.desc = "DTLB load misses",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.pde_miss",
+	.event = "event=0x8,umask=0x20,period=200000",
+	.desc = "DTLB load miss caused by low part of address",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x8,umask=0x10,period=2000000",
+	.desc = "DTLB second level hit",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x8,umask=0x2,period=200000",
+	.desc = "DTLB load miss page walks complete",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_cycles",
+	.event = "event=0x8,umask=0x4,period=200000",
+	.desc = "DTLB load miss page walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.any",
+	.event = "event=0x49,umask=0x1,period=200000",
+	.desc = "DTLB misses",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.large_walk_completed",
+	.event = "event=0x49,umask=0x80,period=200000",
+	.desc = "DTLB miss large page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.stlb_hit",
+	.event = "event=0x49,umask=0x10,period=200000",
+	.desc = "DTLB first level misses but second level hit",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.walk_completed",
+	.event = "event=0x49,umask=0x2,period=200000",
+	.desc = "DTLB miss page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.walk_cycles",
+	.event = "event=0x49,umask=0x4,period=2000000",
+	.desc = "DTLB miss page walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4F,umask=0x10,period=2000000",
+	.desc = "Extended Page Table walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=2000000",
+	.desc = "ITLB flushes",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_miss_retired",
+	.event = "event=0xC8,umask=0x20,period=200000",
+	.desc = "Retired instructions that missed the ITLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.any",
+	.event = "event=0x85,umask=0x1,period=200000",
+	.desc = "ITLB miss",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0x2,period=200000",
+	.desc = "ITLB miss page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_cycles",
+	.event = "event=0x85,umask=0x4,period=2000000",
+	.desc = "ITLB miss page walk cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_load_retired.dtlb_miss",
+	.event = "event=0xCB,umask=0x80,period=200000",
+	.desc = "Retired loads that miss the DTLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_store_retired.dtlb_miss",
+	.event = "event=0xC,umask=0x1,period=200000",
+	.desc = "Retired stores that miss the DTLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "cache_lock_cycles.l1d",
+	.event = "event=0x63,umask=0x2,period=2000000",
+	.desc = "Cycles L1D locked",
+	.topic = "cache",
+},
+{
+	.name = "cache_lock_cycles.l1d_l2",
+	.event = "event=0x63,umask=0x1,period=2000000",
+	.desc = "Cycles L1D and L2 locked",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_evict",
+	.event = "event=0x51,umask=0x4,period=2000000",
+	.desc = "L1D cache lines replaced in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_repl",
+	.event = "event=0x51,umask=0x2,period=2000000",
+	.desc = "L1D cache lines allocated in the M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_snoop_evict",
+	.event = "event=0x51,umask=0x8,period=2000000",
+	.desc = "L1D snoop eviction of cache lines in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.repl",
+	.event = "event=0x51,umask=0x1,period=2000000",
+	.desc = "L1 data cache lines allocated",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_prefetch_lock_fb_hit",
+	.event = "event=0x52,umask=0x1,period=2000000",
+	.desc = "L1D prefetch load lock accepted in fill buffer",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.miss",
+	.event = "event=0x4E,umask=0x2,period=200000",
+	.desc = "L1D hardware prefetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.requests",
+	.event = "event=0x4E,umask=0x1,period=200000",
+	.desc = "L1D hardware prefetch requests",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.triggers",
+	.event = "event=0x4E,umask=0x4,period=200000",
+	.desc = "L1D hardware prefetch requests triggered",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.e_state",
+	.event = "event=0x28,umask=0x4,period=100000",
+	.desc = "L1 writebacks to L2 in E state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.i_state",
+	.event = "event=0x28,umask=0x1,period=100000",
+	.desc = "L1 writebacks to L2 in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.m_state",
+	.event = "event=0x28,umask=0x8,period=100000",
+	.desc = "L1 writebacks to L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.mesi",
+	.event = "event=0x28,umask=0xf,period=100000",
+	.desc = "All L1 writebacks to L2",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.s_state",
+	.event = "event=0x28,umask=0x2,period=100000",
+	.desc = "L1 writebacks to L2 in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.any",
+	.event = "event=0x26,umask=0xff,period=200000",
+	.desc = "All L2 data requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.e_state",
+	.event = "event=0x26,umask=0x4,period=200000",
+	.desc = "L2 data demand loads in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.i_state",
+	.event = "event=0x26,umask=0x1,period=200000",
+	.desc = "L2 data demand loads in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.m_state",
+	.event = "event=0x26,umask=0x8,period=200000",
+	.desc = "L2 data demand loads in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.mesi",
+	.event = "event=0x26,umask=0xf,period=200000",
+	.desc = "L2 data demand requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.s_state",
+	.event = "event=0x26,umask=0x2,period=200000",
+	.desc = "L2 data demand loads in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.e_state",
+	.event = "event=0x26,umask=0x40,period=200000",
+	.desc = "L2 data prefetches in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.i_state",
+	.event = "event=0x26,umask=0x10,period=200000",
+	.desc = "L2 data prefetches in the I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.m_state",
+	.event = "event=0x26,umask=0x80,period=200000",
+	.desc = "L2 data prefetches in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.mesi",
+	.event = "event=0x26,umask=0xf0,period=200000",
+	.desc = "All L2 data prefetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.s_state",
+	.event = "event=0x26,umask=0x20,period=200000",
+	.desc = "L2 data prefetches in the S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.any",
+	.event = "event=0xF1,umask=0x7,period=100000",
+	.desc = "L2 lines alloacated",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.e_state",
+	.event = "event=0xF1,umask=0x4,period=100000",
+	.desc = "L2 lines allocated in the E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.s_state",
+	.event = "event=0xF1,umask=0x2,period=100000",
+	.desc = "L2 lines allocated in the S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.any",
+	.event = "event=0xF2,umask=0xf,period=100000",
+	.desc = "L2 lines evicted",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x1,period=100000",
+	.desc = "L2 lines evicted by a demand request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_dirty",
+	.event = "event=0xF2,umask=0x2,period=100000",
+	.desc = "L2 modified lines evicted by a demand request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.prefetch_clean",
+	.event = "event=0xF2,umask=0x4,period=100000",
+	.desc = "L2 lines evicted by a prefetch request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.prefetch_dirty",
+	.event = "event=0xF2,umask=0x8,period=100000",
+	.desc = "L2 modified lines evicted by a prefetch request",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetch_hit",
+	.event = "event=0x24,umask=0x10,period=200000",
+	.desc = "L2 instruction fetch hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetch_miss",
+	.event = "event=0x24,umask=0x20,period=200000",
+	.desc = "L2 instruction fetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetches",
+	.event = "event=0x24,umask=0x30,period=200000",
+	.desc = "L2 instruction fetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ld_hit",
+	.event = "event=0x24,umask=0x1,period=200000",
+	.desc = "L2 load hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ld_miss",
+	.event = "event=0x24,umask=0x2,period=200000",
+	.desc = "L2 load misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.loads",
+	.event = "event=0x24,umask=0x3,period=200000",
+	.desc = "L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.miss",
+	.event = "event=0x24,umask=0xaa,period=200000",
+	.desc = "All L2 misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetch_hit",
+	.event = "event=0x24,umask=0x40,period=200000",
+	.desc = "L2 prefetch hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetch_miss",
+	.event = "event=0x24,umask=0x80,period=200000",
+	.desc = "L2 prefetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetches",
+	.event = "event=0x24,umask=0xc0,period=200000",
+	.desc = "All L2 prefetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.references",
+	.event = "event=0x24,umask=0xff,period=200000",
+	.desc = "All L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x4,period=200000",
+	.desc = "L2 RFO hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x8,period=200000",
+	.desc = "L2 RFO misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfos",
+	.event = "event=0x24,umask=0xc,period=200000",
+	.desc = "L2 RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.any",
+	.event = "event=0xF0,umask=0x80,period=200000",
+	.desc = "All L2 transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.fill",
+	.event = "event=0xF0,umask=0x20,period=200000",
+	.desc = "L2 fill transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.ifetch",
+	.event = "event=0xF0,umask=0x4,period=200000",
+	.desc = "L2 instruction fetch transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200000",
+	.desc = "L1D writeback to L2 transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.load",
+	.event = "event=0xF0,umask=0x1,period=200000",
+	.desc = "L2 Load transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.prefetch",
+	.event = "event=0xF0,umask=0x8,period=200000",
+	.desc = "L2 prefetch transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.rfo",
+	.event = "event=0xF0,umask=0x2,period=200000",
+	.desc = "L2 RFO transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.wb",
+	.event = "event=0xF0,umask=0x40,period=200000",
+	.desc = "L2 writeback to LLC transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.e_state",
+	.event = "event=0x27,umask=0x40,period=100000",
+	.desc = "L2 demand lock RFOs in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.hit",
+	.event = "event=0x27,umask=0xe0,period=100000",
+	.desc = "All demand L2 lock RFOs that hit the cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.i_state",
+	.event = "event=0x27,umask=0x10,period=100000",
+	.desc = "L2 demand lock RFOs in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.m_state",
+	.event = "event=0x27,umask=0x80,period=100000",
+	.desc = "L2 demand lock RFOs in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.mesi",
+	.event = "event=0x27,umask=0xf0,period=100000",
+	.desc = "All demand L2 lock RFOs",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.s_state",
+	.event = "event=0x27,umask=0x20,period=100000",
+	.desc = "L2 demand lock RFOs in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.hit",
+	.event = "event=0x27,umask=0xe,period=100000",
+	.desc = "All L2 demand store RFOs that hit the cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.i_state",
+	.event = "event=0x27,umask=0x1,period=100000",
+	.desc = "L2 demand store RFOs in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.m_state",
+	.event = "event=0x27,umask=0x8,period=100000",
+	.desc = "L2 demand store RFOs in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.mesi",
+	.event = "event=0x27,umask=0xf,period=100000",
+	.desc = "All L2 demand store RFOs",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.s_state",
+	.event = "event=0x27,umask=0x2,period=100000",
+	.desc = "L2 demand store RFOs in S state",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100000",
+	.desc = "Longest latency cache miss",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=200000",
+	.desc = "Longest latency cache reference",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.loads",
+	.event = "event=0xB,umask=0x1,period=2000000",
+	.desc = "Instructions retired which contains a load (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.stores",
+	.event = "event=0xB,umask=0x2,period=2000000",
+	.desc = "Instructions retired which contains a store (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.hit_lfb",
+	.event = "event=0xCB,umask=0x40,period=200000",
+	.desc = "Retired loads that miss L1D and hit an previously allocated LFB (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l1d_hit",
+	.event = "event=0xCB,umask=0x1,period=2000000",
+	.desc = "Retired loads that hit the L1 data cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l2_hit",
+	.event = "event=0xCB,umask=0x2,period=200000",
+	.desc = "Retired loads that hit the L2 cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.llc_miss",
+	.event = "event=0xCB,umask=0x10,period=10000",
+	.desc = "Retired loads that miss the LLC cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.llc_unshared_hit",
+	.event = "event=0xCB,umask=0x4,period=40000",
+	.desc = "Retired loads that hit valid versions in the LLC cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.other_core_l2_hit_hitm",
+	.event = "event=0xCB,umask=0x8,period=40000",
+	.desc = "Retired loads that hit sibling core's L2 in modified or unmodified states (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.local_dram",
+	.event = "event=0xF,umask=0x10,period=10000",
+	.desc = "Load instructions retired with a data source of local DRAM or locally homed remote hitm (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.other_core_l2_hitm",
+	.event = "event=0xF,umask=0x2,period=40000",
+	.desc = "Load instructions retired that HIT modified data in sibling core (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.remote_cache_local_home_hit",
+	.event = "event=0xF,umask=0x8,period=20000",
+	.desc = "Load instructions retired remote cache HIT data source (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.remote_dram",
+	.event = "event=0xF,umask=0x20,period=10000",
+	.desc = "Load instructions retired remote DRAM and remote home-remote cache HITM (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uncore_retired.uncacheable",
+	.event = "event=0xF,umask=0x80,period=4000",
+	.desc = "Load instructions retired IO (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.any",
+	.event = "event=0xB0,umask=0x80,period=100000",
+	.desc = "All offcore requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.any.read",
+	.event = "event=0xB0,umask=0x8,period=100000",
+	.desc = "Offcore read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.any.rfo",
+	.event = "event=0xB0,umask=0x10,period=100000",
+	.desc = "Offcore RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand.read_code",
+	.event = "event=0xB0,umask=0x2,period=100000",
+	.desc = "Offcore demand code read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand.read_data",
+	.event = "event=0xB0,umask=0x1,period=100000",
+	.desc = "Offcore demand data read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand.rfo",
+	.event = "event=0xB0,umask=0x4,period=100000",
+	.desc = "Offcore demand RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.l1d_writeback",
+	.event = "event=0xB0,umask=0x40,period=100000",
+	.desc = "Offcore L1 data cache writebacks",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.uncached_mem",
+	.event = "event=0xB0,umask=0x20,period=100000",
+	.desc = "Offcore uncached memory accesses",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.any.read",
+	.event = "event=0x60,umask=0x8,period=2000000",
+	.desc = "Outstanding offcore reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.any.read_not_empty",
+	.event = "event=0x60,umask=0x8,period=2000000,cmask=1",
+	.desc = "Cycles offcore reads busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_code",
+	.event = "event=0x60,umask=0x2,period=2000000",
+	.desc = "Outstanding offcore demand code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_code_not_empty",
+	.event = "event=0x60,umask=0x2,period=2000000,cmask=1",
+	.desc = "Cycles offcore demand code read busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_data",
+	.event = "event=0x60,umask=0x1,period=2000000",
+	.desc = "Outstanding offcore demand data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.read_data_not_empty",
+	.event = "event=0x60,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles offcore demand data read busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.rfo",
+	.event = "event=0x60,umask=0x4,period=2000000",
+	.desc = "Outstanding offcore demand RFOs",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand.rfo_not_empty",
+	.event = "event=0x60,umask=0x4,period=2000000,cmask=1",
+	.desc = "Cycles offcore demand RFOs busy",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_sq_full",
+	.event = "event=0xB2,umask=0x1,period=100000",
+	.desc = "Offcore requests blocked due to Super Queue full",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.lru_hints",
+	.event = "event=0xF4,umask=0x4,period=2000000",
+	.desc = "Super Queue LRU hints sent to LLC",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xF4,umask=0x10,period=2000000",
+	.desc = "Super Queue lock splits across a cache line",
+	.topic = "cache",
+},
+{
+	.name = "store_blocks.at_ret",
+	.event = "event=0x6,umask=0x4,period=200000",
+	.desc = "Loads delayed with at-Retirement block code",
+	.topic = "cache",
+},
+{
+	.name = "store_blocks.l1d_block",
+	.event = "event=0x6,umask=0x8,period=200000",
+	.desc = "Cacheable loads delayed with L1D block code",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_0",
+	.event = "event=0xB,umask=0x10,period=2000000,ldlat=0x0",
+	.desc = "Memory instructions retired above 0 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_1024",
+	.event = "event=0xB,umask=0x10,period=100,ldlat=0x400",
+	.desc = "Memory instructions retired above 1024 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_128",
+	.event = "event=0xB,umask=0x10,period=1000,ldlat=0x80",
+	.desc = "Memory instructions retired above 128 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_16",
+	.event = "event=0xB,umask=0x10,period=10000,ldlat=0x10",
+	.desc = "Memory instructions retired above 16 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_16384",
+	.event = "event=0xB,umask=0x10,period=5,ldlat=0x4000",
+	.desc = "Memory instructions retired above 16384 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_2048",
+	.event = "event=0xB,umask=0x10,period=50,ldlat=0x800",
+	.desc = "Memory instructions retired above 2048 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_256",
+	.event = "event=0xB,umask=0x10,period=500,ldlat=0x100",
+	.desc = "Memory instructions retired above 256 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_32",
+	.event = "event=0xB,umask=0x10,period=5000,ldlat=0x20",
+	.desc = "Memory instructions retired above 32 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_32768",
+	.event = "event=0xB,umask=0x10,period=3,ldlat=0x8000",
+	.desc = "Memory instructions retired above 32768 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_4",
+	.event = "event=0xB,umask=0x10,period=50000,ldlat=0x4",
+	.desc = "Memory instructions retired above 4 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_4096",
+	.event = "event=0xB,umask=0x10,period=20,ldlat=0x1000",
+	.desc = "Memory instructions retired above 4096 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_512",
+	.event = "event=0xB,umask=0x10,period=200,ldlat=0x200",
+	.desc = "Memory instructions retired above 512 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_64",
+	.event = "event=0xB,umask=0x10,period=2000,ldlat=0x40",
+	.desc = "Memory instructions retired above 64 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_8",
+	.event = "event=0xB,umask=0x10,period=20000,ldlat=0x8",
+	.desc = "Memory instructions retired above 8 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_8192",
+	.event = "event=0xB,umask=0x10,period=10,ldlat=0x2000",
+	.desc = "Memory instructions retired above 8192 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F11",
+	.desc = "Offcore data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF11",
+	.desc = "All offcore data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8011",
+	.desc = "Offcore data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x111",
+	.desc = "Offcore data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x211",
+	.desc = "Offcore data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x411",
+	.desc = "Offcore data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x711",
+	.desc = "Offcore data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2711",
+	.desc = "Offcore data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1811",
+	.desc = "Offcore data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5811",
+	.desc = "Offcore data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1011",
+	.desc = "Offcore data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x811",
+	.desc = "Offcore data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F44",
+	.desc = "Offcore code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF44",
+	.desc = "All offcore code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8044",
+	.desc = "Offcore code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x144",
+	.desc = "Offcore code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x244",
+	.desc = "Offcore code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x444",
+	.desc = "Offcore code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x744",
+	.desc = "Offcore code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2744",
+	.desc = "Offcore code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1844",
+	.desc = "Offcore code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5844",
+	.desc = "Offcore code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1044",
+	.desc = "Offcore code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x844",
+	.desc = "Offcore code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7FFF",
+	.desc = "Offcore requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFFFF",
+	.desc = "All offcore requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x80FF",
+	.desc = "Offcore requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1FF",
+	.desc = "Offcore requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2FF",
+	.desc = "Offcore requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4FF",
+	.desc = "Offcore requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7FF",
+	.desc = "Offcore requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x27FF",
+	.desc = "Offcore requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x18FF",
+	.desc = "Offcore requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x58FF",
+	.desc = "Offcore requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x10FF",
+	.desc = "Offcore requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8FF",
+	.desc = "Offcore requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F22",
+	.desc = "Offcore RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF22",
+	.desc = "All offcore RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8022",
+	.desc = "Offcore RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x122",
+	.desc = "Offcore RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x222",
+	.desc = "Offcore RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x422",
+	.desc = "Offcore RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x722",
+	.desc = "Offcore RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2722",
+	.desc = "Offcore RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1822",
+	.desc = "Offcore RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5822",
+	.desc = "Offcore RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1022",
+	.desc = "Offcore RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x822",
+	.desc = "Offcore RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F08",
+	.desc = "Offcore writebacks to any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF08",
+	.desc = "All offcore writebacks",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8008",
+	.desc = "Offcore writebacks to the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x108",
+	.desc = "Offcore writebacks to the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x408",
+	.desc = "Offcore writebacks to the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x708",
+	.desc = "Offcore writebacks to the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2708",
+	.desc = "Offcore writebacks to the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1808",
+	.desc = "Offcore writebacks to a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5808",
+	.desc = "Offcore writebacks to a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1008",
+	.desc = "Offcore writebacks that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x808",
+	.desc = "Offcore writebacks that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F77",
+	.desc = "Offcore code or data read requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF77",
+	.desc = "All offcore code or data read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8077",
+	.desc = "Offcore code or data read requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x177",
+	.desc = "Offcore code or data read requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x277",
+	.desc = "Offcore code or data read requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x477",
+	.desc = "Offcore code or data read requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x777",
+	.desc = "Offcore code or data read requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2777",
+	.desc = "Offcore code or data read requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1877",
+	.desc = "Offcore code or data read requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5877",
+	.desc = "Offcore code or data read requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1077",
+	.desc = "Offcore code or data read requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x877",
+	.desc = "Offcore code or data read requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F33",
+	.desc = "Offcore request = all data, response = any cache_dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF33",
+	.desc = "Offcore request = all data, response = any location",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8033",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x133",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x233",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x433",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x733",
+	.desc = "Offcore request = all data, response = local cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2733",
+	.desc = "Offcore request = all data, response = local cache or dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1833",
+	.desc = "Offcore request = all data, response = remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5833",
+	.desc = "Offcore request = all data, response = remote cache or dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1033",
+	.desc = "Offcore data reads, RFO's and prefetches that HIT in a remote cache ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x833",
+	.desc = "Offcore data reads, RFO's and prefetches that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F03",
+	.desc = "Offcore demand data requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF03",
+	.desc = "All offcore demand data requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8003",
+	.desc = "Offcore demand data requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x103",
+	.desc = "Offcore demand data requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x203",
+	.desc = "Offcore demand data requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x403",
+	.desc = "Offcore demand data requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x703",
+	.desc = "Offcore demand data requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2703",
+	.desc = "Offcore demand data requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1803",
+	.desc = "Offcore demand data requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5803",
+	.desc = "Offcore demand data requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1003",
+	.desc = "Offcore demand data requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x803",
+	.desc = "Offcore demand data requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F01",
+	.desc = "Offcore demand data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF01",
+	.desc = "All offcore demand data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8001",
+	.desc = "Offcore demand data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x101",
+	.desc = "Offcore demand data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x201",
+	.desc = "Offcore demand data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x401",
+	.desc = "Offcore demand data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x701",
+	.desc = "Offcore demand data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2701",
+	.desc = "Offcore demand data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1801",
+	.desc = "Offcore demand data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5801",
+	.desc = "Offcore demand data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1001",
+	.desc = "Offcore demand data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x801",
+	.desc = "Offcore demand data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F04",
+	.desc = "Offcore demand code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF04",
+	.desc = "All offcore demand code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8004",
+	.desc = "Offcore demand code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x104",
+	.desc = "Offcore demand code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x204",
+	.desc = "Offcore demand code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x404",
+	.desc = "Offcore demand code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x704",
+	.desc = "Offcore demand code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2704",
+	.desc = "Offcore demand code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1804",
+	.desc = "Offcore demand code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5804",
+	.desc = "Offcore demand code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1004",
+	.desc = "Offcore demand code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x804",
+	.desc = "Offcore demand code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F02",
+	.desc = "Offcore demand RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF02",
+	.desc = "All offcore demand RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8002",
+	.desc = "Offcore demand RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x102",
+	.desc = "Offcore demand RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x202",
+	.desc = "Offcore demand RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x402",
+	.desc = "Offcore demand RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x702",
+	.desc = "Offcore demand RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2702",
+	.desc = "Offcore demand RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1802",
+	.desc = "Offcore demand RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5802",
+	.desc = "Offcore demand RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1002",
+	.desc = "Offcore demand RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x802",
+	.desc = "Offcore demand RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F80",
+	.desc = "Offcore other requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF80",
+	.desc = "All offcore other requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8080",
+	.desc = "Offcore other requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x180",
+	.desc = "Offcore other requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x280",
+	.desc = "Offcore other requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x480",
+	.desc = "Offcore other requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x780",
+	.desc = "Offcore other requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2780",
+	.desc = "Offcore other requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1880",
+	.desc = "Offcore other requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5880",
+	.desc = "Offcore other requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1080",
+	.desc = "Offcore other requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x880",
+	.desc = "Offcore other requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F50",
+	.desc = "Offcore prefetch data requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF50",
+	.desc = "All offcore prefetch data requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8050",
+	.desc = "Offcore prefetch data requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x150",
+	.desc = "Offcore prefetch data requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x250",
+	.desc = "Offcore prefetch data requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x450",
+	.desc = "Offcore prefetch data requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x750",
+	.desc = "Offcore prefetch data requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2750",
+	.desc = "Offcore prefetch data requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1850",
+	.desc = "Offcore prefetch data requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5850",
+	.desc = "Offcore prefetch data requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1050",
+	.desc = "Offcore prefetch data requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x850",
+	.desc = "Offcore prefetch data requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F10",
+	.desc = "Offcore prefetch data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF10",
+	.desc = "All offcore prefetch data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8010",
+	.desc = "Offcore prefetch data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x110",
+	.desc = "Offcore prefetch data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x210",
+	.desc = "Offcore prefetch data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x410",
+	.desc = "Offcore prefetch data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x710",
+	.desc = "Offcore prefetch data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2710",
+	.desc = "Offcore prefetch data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1810",
+	.desc = "Offcore prefetch data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5810",
+	.desc = "Offcore prefetch data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1010",
+	.desc = "Offcore prefetch data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x810",
+	.desc = "Offcore prefetch data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F40",
+	.desc = "Offcore prefetch code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF40",
+	.desc = "All offcore prefetch code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8040",
+	.desc = "Offcore prefetch code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x140",
+	.desc = "Offcore prefetch code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x240",
+	.desc = "Offcore prefetch code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x440",
+	.desc = "Offcore prefetch code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x740",
+	.desc = "Offcore prefetch code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2740",
+	.desc = "Offcore prefetch code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1840",
+	.desc = "Offcore prefetch code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5840",
+	.desc = "Offcore prefetch code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1040",
+	.desc = "Offcore prefetch code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x840",
+	.desc = "Offcore prefetch code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F20",
+	.desc = "Offcore prefetch RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF20",
+	.desc = "All offcore prefetch RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8020",
+	.desc = "Offcore prefetch RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x120",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x220",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x420",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x720",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2720",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1820",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5820",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1020",
+	.desc = "Offcore prefetch RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x820",
+	.desc = "Offcore prefetch RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F70",
+	.desc = "Offcore prefetch requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF70",
+	.desc = "All offcore prefetch requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8070",
+	.desc = "Offcore prefetch requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x170",
+	.desc = "Offcore prefetch requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x270",
+	.desc = "Offcore prefetch requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x470",
+	.desc = "Offcore prefetch requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x770",
+	.desc = "Offcore prefetch requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2770",
+	.desc = "Offcore prefetch requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1870",
+	.desc = "Offcore prefetch requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x5870",
+	.desc = "Offcore prefetch requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1070",
+	.desc = "Offcore prefetch requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x870",
+	.desc = "Offcore prefetch requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_knightslanding[] = {
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=200003",
+	.desc = "Counts the number of branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.jcc",
+	.event = "event=0xC4,umask=0x7e,period=200003",
+	.desc = "Counts the number of branch instructions retired that were conditional jumps (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.taken_jcc",
+	.event = "event=0xC4,umask=0xfe,period=200003",
+	.desc = "Counts the number of branch instructions retired that were conditional jumps and predicted taken (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.call",
+	.event = "event=0xC4,umask=0xf9,period=200003",
+	.desc = "Counts the number of near CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.rel_call",
+	.event = "event=0xC4,umask=0xfd,period=200003",
+	.desc = "Counts the number of near relative CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.ind_call",
+	.event = "event=0xC4,umask=0xfb,period=200003",
+	.desc = "Counts the number of near indirect CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.return",
+	.event = "event=0xC4,umask=0xf7,period=200003",
+	.desc = "Counts the number of near RET branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.non_return_ind",
+	.event = "event=0xC4,umask=0xeb,period=200003",
+	.desc = "Counts the number of branch instructions retired that were near indirect CALL or near indirect JMP (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0xbf,period=200003",
+	.desc = "Counts the number of far branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=200003",
+	.desc = "Counts the number of mispredicted branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.jcc",
+	.event = "event=0xC5,umask=0x7e,period=200003",
+	.desc = "Counts the number of mispredicted branch instructions retired that were conditional jumps (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.taken_jcc",
+	.event = "event=0xC5,umask=0xfe,period=200003",
+	.desc = "Counts the number of mispredicted branch instructions retired that were conditional jumps and predicted taken (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.ind_call",
+	.event = "event=0xC5,umask=0xfb,period=200003",
+	.desc = "Counts the number of mispredicted near indirect CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.return",
+	.event = "event=0xC5,umask=0xf7,period=200003",
+	.desc = "Counts the number of mispredicted near RET branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.non_return_ind",
+	.event = "event=0xC5,umask=0xeb,period=200003",
+	.desc = "Counts the number of mispredicted branch instructions retired that were near indirect CALL or near indirect JMP (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.ms",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "Counts the number of micro-ops retired that are from the complex flows issued by the micro-sequencer (MS)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of micro-ops retired that were supplied from MSROM",
+},
+{
+	.name = "uops_retired.all",
+	.event = "event=0xC2,umask=0x10,period=2000003",
+	.desc = "Counts the number of micro-ops retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of micro-ops (uops) retired. The processor decodes complex macro instructions into a sequence of simpler uops. Most instructions are composed of one or two uops. Some instructions are decoded into longer sequences such as repeat instructions, floating point transcendental instructions, and assists",
+},
+{
+	.name = "uops_retired.scalar_simd",
+	.event = "event=0xC2,umask=0x20,period=200003",
+	.desc = "Counts the number of scalar SSE, AVX, AVX2, AVX-512 micro-ops retired. More specifically, it counts scalar SSE, AVX, AVX2, AVX-512 micro-ops except for loads (memory-to-register mov-type micro ops), division, sqrt",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of scalar SSE, AVX, AVX2, AVX-512 micro-ops retired (floating point, integer and store) except for loads (memory-to-register mov-type micro ops), division, sqrt",
+},
+{
+	.name = "uops_retired.packed_simd",
+	.event = "event=0xC2,umask=0x40,period=200003",
+	.desc = "Counts the number of vector SSE, AVX, AVX2, AVX-512 micro-ops retired. More specifically, it counts packed SSE, AVX, AVX2, AVX-512 micro-ops (both floating point and integer) except for loads (memory-to-register mov-type micro-ops), packed byte and word multiplies",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of packed vector SSE, AVX, AVX2, and AVX-512 micro-ops retired (floating point, integer and store) except for loads (memory-to-register mov-type micro-ops), packed byte and word multiplies",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x1,period=200003",
+	.desc = "Counts the number of times that the machine clears due to program modifying data within 1K of a recently fetched code page",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.fp_assist",
+	.event = "event=0xC3,umask=0x4,period=200003",
+	.desc = "Counts the number of floating operations retired that required microcode assists",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of times that the pipeline stalled due to FP operations needing assists",
+},
+{
+	.name = "machine_clears.all",
+	.event = "event=0xC3,umask=0x8,period=200003",
+	.desc = "Counts all nukes",
+	.topic = "pipeline",
+},
+{
+	.name = "no_alloc_cycles.rob_full",
+	.event = "event=0xCA,umask=0x1,period=200003",
+	.desc = "Counts the number of core cycles when no micro-ops are allocated and the ROB is full",
+	.topic = "pipeline",
+},
+{
+	.name = "no_alloc_cycles.mispredicts",
+	.event = "event=0xCA,umask=0x4,period=200003",
+	.desc = "Counts the number of core cycles when no micro-ops are allocated and the alloc pipe is stalled waiting for a mispredicted branch to retire",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of core cycles when no uops are allocated and the alloc pipe is stalled waiting for a mispredicted branch to retire",
+},
+{
+	.name = "no_alloc_cycles.rat_stall",
+	.event = "event=0xCA,umask=0x20,period=200003",
+	.desc = "Counts the number of core cycles when no micro-ops are allocated and a RATstall (caused by reservation station full) is asserted",
+	.topic = "pipeline",
+},
+{
+	.name = "no_alloc_cycles.not_delivered",
+	.event = "event=0xCA,umask=0x90,period=200003",
+	.desc = "Counts the number of core cycles when no micro-ops are allocated, the IQ is empty, and no other condition is blocking allocation",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of core cycles when no uops are allocated, the instruction queue is empty and the alloc pipe is stalled waiting for instructions to be fetched",
+},
+{
+	.name = "no_alloc_cycles.all",
+	.event = "event=0xCA,umask=0x7f,period=200003",
+	.desc = "Counts the total number of core cycles when no micro-ops are allocated for any reason",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_full_stall.mec",
+	.event = "event=0xCB,umask=0x1,period=200003",
+	.desc = "Counts the number of core cycles when allocation pipeline is stalled and is waiting for a free MEC reservation station entry",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_full_stall.all",
+	.event = "event=0xCB,umask=0x1f,period=200003",
+	.desc = "Counts the total number of core cycles the Alloc pipeline is stalled when any one of the reservation stations is full",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Counts the total number of instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "cycles_div_busy.all",
+	.event = "event=0xCD,umask=0x1,period=2000003",
+	.desc = "Cycles the number of core cycles when divider is busy.  Does not imply a stall waiting for the divider",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles when the divider is busy. More specifically cycles when the divide unit is unable to accept a new divide uop because it is busy processing a previously dispatched uop. The cycles will be counted irrespective of whether or not another divide uop is waiting to enter the divide unit (from the RS). This event counts integer divides, x87 divides, divss, divsd, sqrtss, sqrtsd event and does not count vector divides",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Fixed Counter: Counts the number of instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions that retire.  For instructions that consist of multiple micro-ops, this event counts exactly once, as the last micro-op of the instruction retires.  The event continues counting while instructions retire, including during interrupt service routines caused by hardware interrupts, faults or traps",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Counts the number of unhalted core clock cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref",
+	.event = "event=0x0,umask=0x03",
+	.desc = "Counts the number of unhalted reference clock cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Fixed Counter: Counts the number of unhalted core clock cycles",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of core cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. This event is a component in many key event ratios. The core frequency may change from time to time due to transitions associated with Enhanced Intel SpeedStep Technology or TM2. For this reason this event may have a changing ratio with regards to time. When the core frequency is constant, this event can approximate elapsed time while the core was not in the halt state. It is counted on a dedicated fixed counter\r\n",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Fixed Counter: Counts the number of unhalted reference clock cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "baclears.all",
+	.event = "event=0xE6,umask=0x1,period=200003",
+	.desc = "Counts the number of times the front end resteers for any branch as a result of another branch handling mechanism in the front end",
+	.topic = "pipeline",
+},
+{
+	.name = "baclears.return",
+	.event = "event=0xE6,umask=0x8,period=200003",
+	.desc = "Counts the number of times the front end resteers for RET branches as a result of another branch handling mechanism in the front end",
+	.topic = "pipeline",
+},
+{
+	.name = "baclears.cond",
+	.event = "event=0xE6,umask=0x10,period=200003",
+	.desc = "Counts the number of times the front end resteers for conditional branches as a result of another branch handling mechanism in the front end",
+	.topic = "pipeline",
+},
+{
+	.name = "recycleq.ld_block_st_forward",
+	.event = "event=0x03,umask=0x1,period=200003",
+	.desc = "Counts the number of occurences a retired load gets blocked because its address partially overlaps with a store   Supports address when precise (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "recycleq.ld_block_std_notready",
+	.event = "event=0x03,umask=0x2,period=200003",
+	.desc = "Counts the number of occurences a retired load gets blocked because its address overlaps with a store whose data is not ready",
+	.topic = "pipeline",
+},
+{
+	.name = "recycleq.st_splits",
+	.event = "event=0x03,umask=0x4,period=200003",
+	.desc = "Counts the number of occurences a retired store that is a cache line split. Each split should be counted only once",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of retired store that experienced a cache line boundary split(Precise Event). Note that each spilt should be counted only once",
+},
+{
+	.name = "recycleq.ld_splits",
+	.event = "event=0x03,umask=0x8,period=200003",
+	.desc = "Counts the number of occurences a retired load that is a cache line split. Each split should be counted only once  Supports address when precise (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "recycleq.lock",
+	.event = "event=0x03,umask=0x10,period=200003",
+	.desc = "Counts all the retired locked loads. It does not include stores because we would double count if we count stores",
+	.topic = "pipeline",
+},
+{
+	.name = "recycleq.sta_full",
+	.event = "event=0x03,umask=0x20,period=200003",
+	.desc = "Counts the store micro-ops retired that were pushed in the rehad queue because the store address buffer is full",
+	.topic = "pipeline",
+},
+{
+	.name = "recycleq.any_ld",
+	.event = "event=0x03,umask=0x40,period=200003",
+	.desc = "Counts any retired load that was pushed into the recycle queue for any reason",
+	.topic = "pipeline",
+},
+{
+	.name = "recycleq.any_st",
+	.event = "event=0x03,umask=0x80,period=200003",
+	.desc = "Counts any retired store that was pushed into the recycle queue for any reason",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.call",
+	.event = "event=0xC5,umask=0xf9,period=200003",
+	.desc = "Counts the number of mispredicted near CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.rel_call",
+	.event = "event=0xC5,umask=0xfd,period=200003",
+	.desc = "Counts the number of mispredicted near relative CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.far_branch",
+	.event = "event=0xC5,umask=0xbf,period=200003",
+	.desc = "Counts the number of mispredicted far branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "icache.accesses",
+	.event = "event=0x80,umask=0x3,period=200003",
+	.desc = "Counts all instruction fetches, including uncacheable fetches",
+	.topic = "frontend",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=200003",
+	.desc = "Counts all instruction fetches that hit the instruction cache",
+	.topic = "frontend",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "Counts all instruction fetches that miss the instruction cache or produce memory requests. An instruction fetch miss is counted only once and not once for every cycle it is outstanding",
+	.topic = "frontend",
+},
+{
+	.name = "ms_decoded.ms_entry",
+	.event = "event=0xE7,umask=0x1,period=200003",
+	.desc = "Counts the number of times the MSROM starts a flow of uops",
+	.topic = "frontend",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=200003",
+	.desc = "Counts the number of times the machine clears due to memory ordering hazards",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_pf_l2.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400070 ",
+	.desc = "Counts any Prefetch requests that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_pf_l2.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200070 ",
+	.desc = "Counts any Prefetch requests that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_pf_l2.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000070 ",
+	.desc = "Counts any Prefetch requests that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_pf_l2.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800070 ",
+	.desc = "Counts any Prefetch requests that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_read.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x01004032f7 ",
+	.desc = "Counts any Read request  that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_read.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x00802032f7 ",
+	.desc = "Counts any Read request  that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_read.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x01010032f7 ",
+	.desc = "Counts any Read request  that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_read.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x00808032f7 ",
+	.desc = "Counts any Read request  that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_code_rd.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_code_rd.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_code_rd.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_code_rd.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data_rd.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100403091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data_rd.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080203091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data_rd.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101003091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data_rd.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080803091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100408000 ",
+	.desc = "Counts any request that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080208000 ",
+	.desc = "Counts any request that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101008000 ",
+	.desc = "Counts any request that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080808000 ",
+	.desc = "Counts any request that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100402000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080202000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101002000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080802000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_software.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100401000 ",
+	.desc = "Counts Software Prefetches that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_software.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080201000 ",
+	.desc = "Counts Software Prefetches that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_software.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101001000 ",
+	.desc = "Counts Software Prefetches that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_software.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080801000 ",
+	.desc = "Counts Software Prefetches that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.bus_locks.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.bus_locks.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.bus_locks.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.bus_locks.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.uc_code_reads.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.uc_code_reads.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.uc_code_reads.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.uc_code_reads.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_writes.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_writes.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_writes.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_writes.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_reads.non_dram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x2000020080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for responses from any NON_DRAM system address. This includes MMIO transactions",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_reads.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_reads.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_reads.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_reads.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.non_dram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x2000020020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for responses from any NON_DRAM system address. This includes MMIO transactions",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.mcdram_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0100400001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for data responses from MCDRAM Far or Other tile L2 hit far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.mcdram_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080200001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for data responses from MCDRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.ddr_far",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0101000001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for data responses from DRAM Far",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.ddr_near",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080800001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for data responses from DRAM Local",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180600001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180600002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180600004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180600020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_reads.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180600080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_writes.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180600100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.uc_code_reads.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180600200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.bus_locks.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180600400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_software.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180601000 ",
+	.desc = "Counts Software Prefetches that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180608000 ",
+	.desc = "Counts any request that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data_rd.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180603091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180600022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_code_rd.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180600044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_read.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x01806032f7 ",
+	.desc = "Counts any Read request  that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_pf_l2.mcdram",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0180600070 ",
+	.desc = "Counts any Prefetch requests that accounts for responses from MCDRAM (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181800001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181800002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181800004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181800020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181800040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.partial_reads.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181800080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.uc_code_reads.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181800200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.bus_locks.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181800400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_software.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181801000 ",
+	.desc = "Counts Software Prefetches that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181802000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181808000 ",
+	.desc = "Counts any request that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data_rd.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181803091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181800022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_code_rd.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0181800044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_read.ddr",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x01818032f7 ",
+	.desc = "Counts any Read request  that accounts for responses from DDR (local and far)",
+	.topic = "memory",
+},
+{
+	.name = "mem_uops_retired.dtlb_miss_loads",
+	.event = "event=0x04,umask=0x8,period=200003",
+	.desc = "Counts the number of load micro-ops retired that cause a DTLB miss  Supports address when precise (Precise event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walks.d_side_walks",
+	.event = "event=0x05,umask=0x1,period=100003,edge=1",
+	.desc = "Counts the total D-side page walks that are completed or started. The page walks started in the speculative path will also be counted",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walks.d_side_cycles",
+	.event = "event=0x05,umask=0x1,period=200003",
+	.desc = "Counts the total number of core cycles for all the D-side page walks. The cycles for page walks started in speculative path will also be included",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walks.i_side_walks",
+	.event = "event=0x05,umask=0x2,period=100003,edge=1",
+	.desc = "Counts the total I-side page walks that are completed",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walks.i_side_cycles",
+	.event = "event=0x05,umask=0x2,period=200003",
+	.desc = "Counts the total number of core cycles for all the I-side page walks. The cycles for page walks started in speculative path will also be included",
+	.topic = "virtual memory",
+	.long_desc = "This event counts every cycle when an I-side (walks due to an instruction fetch) page walk is in progress",
+},
+{
+	.name = "page_walks.walks",
+	.event = "event=0x05,umask=0x3,period=100003,edge=1",
+	.desc = "Counts the total page walks that are completed (I-side and D-side)",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walks.cycles",
+	.event = "event=0x05,umask=0x3,period=200003",
+	.desc = "Counts the total number of core cycles for all the page walks. The cycles for page walks started in speculative path will also be included",
+	.topic = "virtual memory",
+	.long_desc = "This event counts every cycle when a data (D) page walk or instruction (I) page walk is in progress",
+},
+{
+	.name = "l2_requests_reject.all",
+	.event = "event=0x30,umask=0x0,period=200003",
+	.desc = "Counts the number of MEC requests from the L2Q that reference a cache line (cacheable requests) exlcuding SW prefetches filling only to L2 cache and L1 evictions (automatically exlcudes L2HWP, UC, WC) that were rejected - Multiple repeated rejects should be counted multiple times",
+	.topic = "cache",
+},
+{
+	.name = "core_reject_l2q.all",
+	.event = "event=0x31,umask=0x0,period=200003",
+	.desc = "Counts the number of MEC requests that were not accepted into the L2Q because of any L2  queue reject condition. There is no concept of at-ret here. It might include requests due to instructions in the speculative path",
+	.topic = "cache",
+},
+{
+	.name = "l2_requests.reference",
+	.event = "event=0x2E,umask=0x4f,period=200003",
+	.desc = "Counts the total number of L2 cache references",
+	.topic = "cache",
+},
+{
+	.name = "l2_requests.miss",
+	.event = "event=0x2E,umask=0x41,period=200003",
+	.desc = "Counts the number of L2 cache misses",
+	.topic = "cache",
+},
+{
+	.name = "fetch_stall.icache_fill_pending_cycles",
+	.event = "event=0x86,umask=0x4,period=200003",
+	.desc = "Counts the number of core cycles the fetch stalls because of an icache miss. This is a cummulative count of core cycles the fetch stalled for all icache misses",
+	.topic = "cache",
+	.long_desc = "This event counts the number of core cycles the fetch stalls because of an icache miss. This is a cumulative count of cycles the NIP stalled for all icache misses",
+},
+{
+	.name = "mem_uops_retired.l1_miss_loads",
+	.event = "event=0x04,umask=0x1,period=200003",
+	.desc = "Counts the number of load micro-ops retired that miss in L1 D cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of load micro-ops retired that miss in L1 Data cache. Note that prefetch misses will not be counted",
+},
+{
+	.name = "mem_uops_retired.l2_hit_loads",
+	.event = "event=0x04,umask=0x2,period=200003",
+	.desc = "Counts the number of load micro-ops retired that hit in the L2  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.l2_miss_loads",
+	.event = "event=0x04,umask=0x4,period=100007",
+	.desc = "Counts the number of load micro-ops retired that miss in the L2  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.utlb_miss_loads",
+	.event = "event=0x04,umask=0x10,period=200003",
+	.desc = "Counts the number of load micro-ops retired that caused micro TLB miss",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.hitm",
+	.event = "event=0x04,umask=0x20,period=200003",
+	.desc = "Counts the loads retired that get the data from the other core in the same tile in M state  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0x04,umask=0x40,period=200003",
+	.desc = "Counts all the load micro-ops retired",
+	.topic = "cache",
+	.long_desc = "This event counts the number of load micro-ops retired",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0x04,umask=0x80,period=200003",
+	.desc = "Counts all the store micro-ops retired",
+	.topic = "cache",
+	.long_desc = "This event counts the number of store micro-ops retired",
+},
+{
+	.name = "offcore_response",
+	.event = "event=0xB7,umask=0x1,period=100007",
+	.desc = "Counts the matrix events specified by MSR_OFFCORE_RESPx",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_l2.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000070 ",
+	.desc = "Counts any Prefetch requests that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_l2.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400070 ",
+	.desc = "Counts any Prefetch requests that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_l2.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400070 ",
+	.desc = "Counts any Prefetch requests that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_l2.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080070 ",
+	.desc = "Counts any Prefetch requests that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_l2.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080070 ",
+	.desc = "Counts any Prefetch requests that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_l2.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010070 ",
+	.desc = "Counts any Prefetch requests that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x40000032f7 ",
+	.desc = "Counts any Read request  that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x10004032f7 ",
+	.desc = "Counts any Read request  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x08004032f7 ",
+	.desc = "Counts any Read request  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x10000832f7 ",
+	.desc = "Counts any Read request  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x08000832f7 ",
+	.desc = "Counts any Read request  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x00000132f7 ",
+	.desc = "Counts any Read request  that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000022 ",
+	.desc = "Counts Demand cacheable data write requests  that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000003091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000403091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800403091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000083091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800083091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000013091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000008000 ",
+	.desc = "Counts any request that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000408000 ",
+	.desc = "Counts any request that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800408000 ",
+	.desc = "Counts any request that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000088000 ",
+	.desc = "Counts any request that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800088000 ",
+	.desc = "Counts any request that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000018000 ",
+	.desc = "Counts any request that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000014800 ",
+	.desc = "Counts all streaming stores (WC and should be programmed on PMC1) that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_streaming_stores.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000014000 ",
+	.desc = "Counts Partial streaming stores (WC and should be programmed on PMC1) that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000002000 ",
+	.desc = "Counts L1 data HW prefetches that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000402000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800402000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000082000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800082000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000012000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000001000 ",
+	.desc = "Counts Software Prefetches that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000401000 ",
+	.desc = "Counts Software Prefetches that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800401000 ",
+	.desc = "Counts Software Prefetches that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000081000 ",
+	.desc = "Counts Software Prefetches that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800081000 ",
+	.desc = "Counts Software Prefetches that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000011000 ",
+	.desc = "Counts Software Prefetches that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.full_streaming_stores.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010800 ",
+	.desc = "Counts Full streaming stores (WC and should be programmed on PMC1) that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000400 ",
+	.desc = "Counts Bus locks and split lock requests that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.uc_code_reads.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.uc_code_reads.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.uc_code_reads.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.uc_code_reads.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.uc_code_reads.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.uc_code_reads.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000040 ",
+	.desc = "Counts L2 code HW prefetches that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.supplier_none",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000020020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that provides no supplier details",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000004 ",
+	.desc = "Counts demand code reads and prefetch code reads that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000002 ",
+	.desc = "Counts Demand cacheable data writes that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that are outstanding, per weighted cycle, from the time of the request to when any response is received. The oustanding response should be programmed only on PMC0",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_hit_far_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000400001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_hit_far_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800400001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for responses from a snoop request hit with data forwarded from its Far(not in the same quadrant as the request)-other tile's L2 in E/F state. Valid only for SNC4 cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_hit_near_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000080001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_hit_near_tile_e_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0800080001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for responses from a snoop request hit with data forwarded from its Near-other tile's L2 in E/F state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for any response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002000001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002000002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002000004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002000020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002000080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002000100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.uc_code_reads.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002000200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002000400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002001000 ",
+	.desc = "Counts Software Prefetches that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002002000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002008000 ",
+	.desc = "Counts any request that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002003091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002000022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002000044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x00020032f7 ",
+	.desc = "Counts any Read request  that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_l2.l2_hit_this_tile_m",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0002000070 ",
+	.desc = "Counts any Prefetch requests that accounts for responses which hit its own tile's L2 with data in M state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.uc_code_reads.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004001000 ",
+	.desc = "Counts Software Prefetches that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004002000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004008000 ",
+	.desc = "Counts any request that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004003091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x00040032f7 ",
+	.desc = "Counts any Read request  that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_l2.l2_hit_this_tile_e",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0004000070 ",
+	.desc = "Counts any Prefetch requests that accounts for responses which hit its own tile's L2 with data in E state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008000001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008000002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008000004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008000020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008000080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008000100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.uc_code_reads.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008000200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008000400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008001000 ",
+	.desc = "Counts Software Prefetches that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008002000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008008000 ",
+	.desc = "Counts any request that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008003091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008000022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0008000044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_hit_this_tile_s",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x00080032f7 ",
+	.desc = "Counts any Read request  that accounts for responses which hit its own tile's L2 with data in S state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000001 ",
+	.desc = "Counts demand cacheable data and L1 prefetch data reads that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.uc_code_reads.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010001000 ",
+	.desc = "Counts Software Prefetches that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010002000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010008000 ",
+	.desc = "Counts any request that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010003091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x00100032f7 ",
+	.desc = "Counts any Read request  that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_l2.l2_hit_this_tile_f",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0010000070 ",
+	.desc = "Counts any Prefetch requests that accounts for responses which hit its own tile's L2 with data in F state ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800180002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800180004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800180020 ",
+	.desc = "Counts L2 data RFO prefetches (includes PREFETCHW instruction) that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800180040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800180080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800180100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.uc_code_reads.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800180200 ",
+	.desc = "Counts UC code reads (valid only for Outstanding response type)  that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800180400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800181000 ",
+	.desc = "Counts Software Prefetches that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800182000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800188000 ",
+	.desc = "Counts any request that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800183091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800180022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800180044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x18001832f7 ",
+	.desc = "Counts any Read request  that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_l2.l2_hit_near_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800180070 ",
+	.desc = "Counts any Prefetch requests that accounts for reponses from snoop request hit with data forwarded from its Near-other tile L2 in E/F/M state",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800400002 ",
+	.desc = "Counts Demand cacheable data writes that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800400004 ",
+	.desc = "Counts demand code reads and prefetch code reads that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800400040 ",
+	.desc = "Counts L2 code HW prefetches that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800400080 ",
+	.desc = "Counts Partial reads (UC or WC and is valid only for Outstanding response type).  that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800400100 ",
+	.desc = "Counts Partial writes (UC or WT or WP and should be programmed on PMC1) that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.bus_locks.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800400400 ",
+	.desc = "Counts Bus locks and split lock requests that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_software.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800401000 ",
+	.desc = "Counts Software Prefetches that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800402000 ",
+	.desc = "Counts L1 data HW prefetches that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800408000 ",
+	.desc = "Counts any request that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800403091 ",
+	.desc = "Counts Demand cacheable data and L1 prefetch data read requests  that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800400022 ",
+	.desc = "Counts Demand cacheable data write requests  that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800400044 ",
+	.desc = "Counts Demand code reads and prefetch code read requests  that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_read.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x18004032f7 ",
+	.desc = "Counts any Read request  that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_pf_l2.l2_hit_far_tile",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1800400070 ",
+	.desc = "Counts any Prefetch requests that accounts for reponses from snoop request hit with data forwarded from it Far(not in the same quadrant as the request)-other tile L2 in E/F/M state. Valid only in SNC4 Cluster mode",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_bonnell[] = {
+{
+	.name = "x87_comp_ops_exe.any.s",
+	.event = "event=0x10,umask=0x1,period=2000000",
+	.desc = "Floating point computational micro-ops executed",
+	.topic = "floating point",
+},
+{
+	.name = "x87_comp_ops_exe.any.ar",
+	.event = "event=0x10,umask=0x81,period=2000000",
+	.desc = "Floating point computational micro-ops retired (Must be precise)",
+	.topic = "floating point",
+},
+{
+	.name = "x87_comp_ops_exe.fxch.s",
+	.event = "event=0x10,umask=0x2,period=2000000",
+	.desc = "FXCH uops executed",
+	.topic = "floating point",
+},
+{
+	.name = "x87_comp_ops_exe.fxch.ar",
+	.event = "event=0x10,umask=0x82,period=2000000",
+	.desc = "FXCH uops retired (Must be precise)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.s",
+	.event = "event=0x11,umask=0x1,period=10000",
+	.desc = "Floating point assists",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.ar",
+	.event = "event=0x11,umask=0x81,period=10000",
+	.desc = "Floating point assists for retired operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uops_exec.s",
+	.event = "event=0xB0,umask=0x0,period=2000000",
+	.desc = "SIMD micro-ops executed (excluding stores)",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uops_exec.ar",
+	.event = "event=0xB0,umask=0x80,period=2000000",
+	.desc = "SIMD micro-ops retired (excluding stores) (Must be precise)",
+	.topic = "floating point",
+},
+{
+	.name = "simd_sat_uop_exec.s",
+	.event = "event=0xB1,umask=0x0,period=2000000",
+	.desc = "SIMD saturated arithmetic micro-ops executed",
+	.topic = "floating point",
+},
+{
+	.name = "simd_sat_uop_exec.ar",
+	.event = "event=0xB1,umask=0x80,period=2000000",
+	.desc = "SIMD saturated arithmetic micro-ops retired",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.mul.s",
+	.event = "event=0xB3,umask=0x1,period=2000000",
+	.desc = "SIMD packed multiply micro-ops executed",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.mul.ar",
+	.event = "event=0xB3,umask=0x81,period=2000000",
+	.desc = "SIMD packed multiply micro-ops retired",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.shift.s",
+	.event = "event=0xB3,umask=0x2,period=2000000",
+	.desc = "SIMD packed shift micro-ops executed",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.shift.ar",
+	.event = "event=0xB3,umask=0x82,period=2000000",
+	.desc = "SIMD packed shift micro-ops retired",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.pack.s",
+	.event = "event=0xB3,umask=0x4,period=2000000",
+	.desc = "SIMD packed micro-ops executed",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.pack.ar",
+	.event = "event=0xB3,umask=0x84,period=2000000",
+	.desc = "SIMD packed micro-ops retired",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.unpack.s",
+	.event = "event=0xB3,umask=0x8,period=2000000",
+	.desc = "SIMD unpacked micro-ops executed",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.unpack.ar",
+	.event = "event=0xB3,umask=0x88,period=2000000",
+	.desc = "SIMD unpacked micro-ops retired",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.logical.s",
+	.event = "event=0xB3,umask=0x10,period=2000000",
+	.desc = "SIMD packed logical micro-ops executed",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.logical.ar",
+	.event = "event=0xB3,umask=0x90,period=2000000",
+	.desc = "SIMD packed logical micro-ops retired",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.arithmetic.s",
+	.event = "event=0xB3,umask=0x20,period=2000000",
+	.desc = "SIMD packed arithmetic micro-ops executed",
+	.topic = "floating point",
+},
+{
+	.name = "simd_uop_type_exec.arithmetic.ar",
+	.event = "event=0xB3,umask=0xa0,period=2000000",
+	.desc = "SIMD packed arithmetic micro-ops retired",
+	.topic = "floating point",
+},
+{
+	.name = "simd_inst_retired.packed_single",
+	.event = "event=0xC7,umask=0x1,period=2000000",
+	.desc = "Retired Streaming SIMD Extensions (SSE) packed-single instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_inst_retired.scalar_single",
+	.event = "event=0xC7,umask=0x2,period=2000000",
+	.desc = "Retired Streaming SIMD Extensions (SSE) scalar-single instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_inst_retired.scalar_double",
+	.event = "event=0xC7,umask=0x8,period=2000000",
+	.desc = "Retired Streaming SIMD Extensions 2 (SSE2) scalar-double instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_inst_retired.vector",
+	.event = "event=0xC7,umask=0x10,period=2000000",
+	.desc = "Retired Streaming SIMD Extensions 2 (SSE2) vector instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_comp_inst_retired.packed_single",
+	.event = "event=0xCA,umask=0x1,period=2000000",
+	.desc = "Retired computational Streaming SIMD Extensions (SSE) packed-single instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_comp_inst_retired.scalar_single",
+	.event = "event=0xCA,umask=0x2,period=2000000",
+	.desc = "Retired computational Streaming SIMD Extensions (SSE) scalar-single instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_comp_inst_retired.scalar_double",
+	.event = "event=0xCA,umask=0x8,period=2000000",
+	.desc = "Retired computational Streaming SIMD Extensions 2 (SSE2) scalar-double instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_assist",
+	.event = "event=0xCD,umask=0x0,period=100000",
+	.desc = "SIMD assists invoked",
+	.topic = "floating point",
+},
+{
+	.name = "simd_instr_retired",
+	.event = "event=0xCE,umask=0x0,period=2000000",
+	.desc = "SIMD Instructions retired",
+	.topic = "floating point",
+},
+{
+	.name = "simd_sat_instr_retired",
+	.event = "event=0xCF,umask=0x0,period=2000000",
+	.desc = "Saturated arithmetic instructions retired",
+	.topic = "floating point",
+},
+{
+	.name = "store_forwards.any",
+	.event = "event=0x2,umask=0x83,period=200000",
+	.desc = "All store forwards",
+	.topic = "pipeline",
+},
+{
+	.name = "store_forwards.good",
+	.event = "event=0x2,umask=0x81,period=200000",
+	.desc = "Good store forwards",
+	.topic = "pipeline",
+},
+{
+	.name = "reissue.any",
+	.event = "event=0x3,umask=0x7f,period=200000",
+	.desc = "Micro-op reissues for any cause",
+	.topic = "pipeline",
+},
+{
+	.name = "reissue.any.ar",
+	.event = "event=0x3,umask=0xff,period=200000",
+	.desc = "Micro-op reissues for any cause (At Retirement)",
+	.topic = "pipeline",
+},
+{
+	.name = "mul.s",
+	.event = "event=0x12,umask=0x1,period=2000000",
+	.desc = "Multiply operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "mul.ar",
+	.event = "event=0x12,umask=0x81,period=2000000",
+	.desc = "Multiply operations retired",
+	.topic = "pipeline",
+},
+{
+	.name = "div.s",
+	.event = "event=0x13,umask=0x1,period=2000000",
+	.desc = "Divide operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "div.ar",
+	.event = "event=0x13,umask=0x81,period=2000000",
+	.desc = "Divide operations retired",
+	.topic = "pipeline",
+},
+{
+	.name = "cycles_div_busy",
+	.event = "event=0x14,umask=0x1,period=2000000",
+	.desc = "Cycles the divider is busy",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.core_p",
+	.event = "event=0x3C,umask=0x0,period=2000000",
+	.desc = "Core cycles when core is not halted",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.bus",
+	.event = "event=0x3C,umask=0x1,period=200000",
+	.desc = "Bus cycles when core is not halted",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.core",
+	.event = "event=0xA,umask=0x0,period=2000000",
+	.desc = "Core cycles when core is not halted",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref",
+	.event = "event=0x0,umask=0x03",
+	.desc = "Reference cycles when core is not halted",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_type_retired.cond",
+	.event = "event=0x88,umask=0x1,period=2000000",
+	.desc = "All macro conditional branch instructions",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_type_retired.uncond",
+	.event = "event=0x88,umask=0x2,period=2000000",
+	.desc = "All macro unconditional branch instructions, excluding calls and indirects",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_type_retired.ind",
+	.event = "event=0x88,umask=0x4,period=2000000",
+	.desc = "All indirect branches that are not calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_type_retired.ret",
+	.event = "event=0x88,umask=0x8,period=2000000",
+	.desc = "All indirect branches that have a return mnemonic",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_type_retired.dir_call",
+	.event = "event=0x88,umask=0x10,period=2000000",
+	.desc = "All non-indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_type_retired.ind_call",
+	.event = "event=0x88,umask=0x20,period=2000000",
+	.desc = "All indirect calls, including both register and memory indirect",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_type_retired.cond_taken",
+	.event = "event=0x88,umask=0x41,period=2000000",
+	.desc = "Only taken macro conditional branch instructions",
+	.topic = "pipeline",
+},
+{
+	.name = "br_missp_type_retired.cond",
+	.event = "event=0x89,umask=0x1,period=200000",
+	.desc = "Mispredicted cond branch instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "br_missp_type_retired.ind",
+	.event = "event=0x89,umask=0x2,period=200000",
+	.desc = "Mispredicted ind branches that are not calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_missp_type_retired.return",
+	.event = "event=0x89,umask=0x4,period=200000",
+	.desc = "Mispredicted return branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_missp_type_retired.ind_call",
+	.event = "event=0x89,umask=0x8,period=200000",
+	.desc = "Mispredicted indirect calls, including both register and memory indirect",
+	.topic = "pipeline",
+},
+{
+	.name = "br_missp_type_retired.cond_taken",
+	.event = "event=0x89,umask=0x11,period=200000",
+	.desc = "Mispredicted and taken cond branch instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (precise event) (Must be precise)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.any",
+	.event = "event=0xC2,umask=0x10,period=2000000",
+	.desc = "Micro-ops retired",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.stalled_cycles",
+	.event = "event=0xC2,umask=0x10,period=2000000",
+	.desc = "Cycles no micro-ops retired",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.stalls",
+	.event = "event=0xC2,umask=0x10,period=2000000",
+	.desc = "Periods no micro-ops retired",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x1,period=200000",
+	.desc = "Self-Modifying Code detected",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.any",
+	.event = "event=0xC4,umask=0x0,period=2000000",
+	.desc = "Retired branch instructions",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.pred_not_taken",
+	.event = "event=0xC4,umask=0x1,period=2000000",
+	.desc = "Retired branch instructions that were predicted not-taken",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.mispred_not_taken",
+	.event = "event=0xC4,umask=0x2,period=200000",
+	.desc = "Retired branch instructions that were mispredicted not-taken",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.pred_taken",
+	.event = "event=0xC4,umask=0x4,period=2000000",
+	.desc = "Retired branch instructions that were predicted taken",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.mispred_taken",
+	.event = "event=0xC4,umask=0x8,period=200000",
+	.desc = "Retired branch instructions that were mispredicted taken",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.taken",
+	.event = "event=0xC4,umask=0xc,period=2000000",
+	.desc = "Retired taken branch instructions",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.any1",
+	.event = "event=0xC4,umask=0xf,period=2000000",
+	.desc = "Retired branch instructions",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.mispred",
+	.event = "event=0xC5,umask=0x0,period=200000",
+	.desc = "Retired mispredicted branch instructions (precise event) (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.div_busy",
+	.event = "event=0xDC,umask=0x2,period=2000000",
+	.desc = "Cycles issue is stalled due to div busy",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_decoded",
+	.event = "event=0xE0,umask=0x1,period=2000000",
+	.desc = "Branch instructions decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "bogus_br",
+	.event = "event=0xE4,umask=0x1,period=2000000",
+	.desc = "Bogus branches",
+	.topic = "pipeline",
+},
+{
+	.name = "baclears.any",
+	.event = "event=0xE6,umask=0x1,period=2000000",
+	.desc = "BACLEARS asserted",
+	.topic = "pipeline",
+},
+{
+	.name = "reissue.overlap_store",
+	.event = "event=0x3,umask=0x1,period=200000",
+	.desc = "Micro-op reissues on a store-load collision",
+	.topic = "pipeline",
+},
+{
+	.name = "reissue.overlap_store.ar",
+	.event = "event=0x3,umask=0x81,period=200000",
+	.desc = "Micro-op reissues on a store-load collision (At Retirement)",
+	.topic = "pipeline",
+},
+{
+	.name = "icache.accesses",
+	.event = "event=0x80,umask=0x3,period=200000",
+	.desc = "Instruction fetches",
+	.topic = "frontend",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=200000",
+	.desc = "Icache hit",
+	.topic = "frontend",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200000",
+	.desc = "Icache miss",
+	.topic = "frontend",
+},
+{
+	.name = "cycles_icache_mem_stalled.icache_mem_stalled",
+	.event = "event=0x86,umask=0x1,period=2000000",
+	.desc = "Cycles during which instruction fetches are  stalled",
+	.topic = "frontend",
+},
+{
+	.name = "decode_stall.pfb_empty",
+	.event = "event=0x87,umask=0x1,period=2000000",
+	.desc = "Decode stall due to PFB empty",
+	.topic = "frontend",
+},
+{
+	.name = "decode_stall.iq_full",
+	.event = "event=0x87,umask=0x2,period=2000000",
+	.desc = "Decode stall due to IQ full",
+	.topic = "frontend",
+},
+{
+	.name = "macro_insts.non_cisc_decoded",
+	.event = "event=0xAA,umask=0x1,period=2000000",
+	.desc = "Non-CISC nacro instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "macro_insts.cisc_decoded",
+	.event = "event=0xAA,umask=0x2,period=2000000",
+	.desc = "CISC macro instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "macro_insts.all_decoded",
+	.event = "event=0xAA,umask=0x3,period=2000000",
+	.desc = "All Instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "uops.ms_cycles",
+	.event = "event=0xA9,umask=0x1,period=2000000,cmask=1",
+	.desc = "This event counts the cycles where 1 or more uops are issued by the micro-sequencer (MS), including microcode assists and inserted flows, and written to the IQ",
+	.topic = "frontend",
+},
+{
+	.name = "segment_reg_loads.any",
+	.event = "event=0x6,umask=0x80,period=200000",
+	.desc = "Number of segment register loads",
+	.topic = "other",
+},
+{
+	.name = "dispatch_blocked.any",
+	.event = "event=0x9,umask=0x20,period=200000",
+	.desc = "Memory cluster signals to block micro-op dispatch for any reason",
+	.topic = "other",
+},
+{
+	.name = "eist_trans",
+	.event = "event=0x3A,umask=0x0,period=200000",
+	.desc = "Number of Enhanced Intel SpeedStep(R) Technology (EIST) transitions",
+	.topic = "other",
+},
+{
+	.name = "thermal_trip",
+	.event = "event=0x3B,umask=0xc0,period=200000",
+	.desc = "Number of thermal trips",
+	.topic = "other",
+},
+{
+	.name = "bus_request_outstanding.all_agents",
+	.event = "event=0x60,umask=0xe0,period=200000",
+	.desc = "Outstanding cacheable data read bus requests duration",
+	.topic = "other",
+},
+{
+	.name = "bus_request_outstanding.self",
+	.event = "event=0x60,umask=0x40,period=200000",
+	.desc = "Outstanding cacheable data read bus requests duration",
+	.topic = "other",
+},
+{
+	.name = "bus_bnr_drv.all_agents",
+	.event = "event=0x61,umask=0x20,period=200000",
+	.desc = "Number of Bus Not Ready signals asserted",
+	.topic = "other",
+},
+{
+	.name = "bus_bnr_drv.this_agent",
+	.event = "event=0x61,umask=0x0,period=200000",
+	.desc = "Number of Bus Not Ready signals asserted",
+	.topic = "other",
+},
+{
+	.name = "bus_drdy_clocks.all_agents",
+	.event = "event=0x62,umask=0x20,period=200000",
+	.desc = "Bus cycles when data is sent on the bus",
+	.topic = "other",
+},
+{
+	.name = "bus_drdy_clocks.this_agent",
+	.event = "event=0x62,umask=0x0,period=200000",
+	.desc = "Bus cycles when data is sent on the bus",
+	.topic = "other",
+},
+{
+	.name = "bus_lock_clocks.all_agents",
+	.event = "event=0x63,umask=0xe0,period=200000",
+	.desc = "Bus cycles when a LOCK signal is asserted",
+	.topic = "other",
+},
+{
+	.name = "bus_lock_clocks.self",
+	.event = "event=0x63,umask=0x40,period=200000",
+	.desc = "Bus cycles when a LOCK signal is asserted",
+	.topic = "other",
+},
+{
+	.name = "bus_data_rcv.self",
+	.event = "event=0x64,umask=0x40,period=200000",
+	.desc = "Bus cycles while processor receives data",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_brd.all_agents",
+	.event = "event=0x65,umask=0xe0,period=200000",
+	.desc = "Burst read bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_brd.self",
+	.event = "event=0x65,umask=0x40,period=200000",
+	.desc = "Burst read bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_rfo.all_agents",
+	.event = "event=0x66,umask=0xe0,period=200000",
+	.desc = "RFO bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_rfo.self",
+	.event = "event=0x66,umask=0x40,period=200000",
+	.desc = "RFO bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_wb.all_agents",
+	.event = "event=0x67,umask=0xe0,period=200000",
+	.desc = "Explicit writeback bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_wb.self",
+	.event = "event=0x67,umask=0x40,period=200000",
+	.desc = "Explicit writeback bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_ifetch.all_agents",
+	.event = "event=0x68,umask=0xe0,period=200000",
+	.desc = "Instruction-fetch bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_ifetch.self",
+	.event = "event=0x68,umask=0x40,period=200000",
+	.desc = "Instruction-fetch bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_inval.all_agents",
+	.event = "event=0x69,umask=0xe0,period=200000",
+	.desc = "Invalidate bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_inval.self",
+	.event = "event=0x69,umask=0x40,period=200000",
+	.desc = "Invalidate bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_pwr.all_agents",
+	.event = "event=0x6A,umask=0xe0,period=200000",
+	.desc = "Partial write bus transaction",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_pwr.self",
+	.event = "event=0x6A,umask=0x40,period=200000",
+	.desc = "Partial write bus transaction",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_p.all_agents",
+	.event = "event=0x6B,umask=0xe0,period=200000",
+	.desc = "Partial bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_p.self",
+	.event = "event=0x6B,umask=0x40,period=200000",
+	.desc = "Partial bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_io.all_agents",
+	.event = "event=0x6C,umask=0xe0,period=200000",
+	.desc = "IO bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_io.self",
+	.event = "event=0x6C,umask=0x40,period=200000",
+	.desc = "IO bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_def.all_agents",
+	.event = "event=0x6D,umask=0xe0,period=200000",
+	.desc = "Deferred bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_def.self",
+	.event = "event=0x6D,umask=0x40,period=200000",
+	.desc = "Deferred bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_burst.all_agents",
+	.event = "event=0x6E,umask=0xe0,period=200000",
+	.desc = "Burst (full cache-line) bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_burst.self",
+	.event = "event=0x6E,umask=0x40,period=200000",
+	.desc = "Burst (full cache-line) bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_mem.all_agents",
+	.event = "event=0x6F,umask=0xe0,period=200000",
+	.desc = "Memory bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_mem.self",
+	.event = "event=0x6F,umask=0x40,period=200000",
+	.desc = "Memory bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_any.all_agents",
+	.event = "event=0x70,umask=0xe0,period=200000",
+	.desc = "All bus transactions",
+	.topic = "other",
+},
+{
+	.name = "bus_trans_any.self",
+	.event = "event=0x70,umask=0x40,period=200000",
+	.desc = "All bus transactions",
+	.topic = "other",
+},
+{
+	.name = "ext_snoop.this_agent.any",
+	.event = "event=0x77,umask=0xb,period=200000",
+	.desc = "External snoops",
+	.topic = "other",
+},
+{
+	.name = "ext_snoop.this_agent.clean",
+	.event = "event=0x77,umask=0x1,period=200000",
+	.desc = "External snoops",
+	.topic = "other",
+},
+{
+	.name = "ext_snoop.this_agent.hit",
+	.event = "event=0x77,umask=0x2,period=200000",
+	.desc = "External snoops",
+	.topic = "other",
+},
+{
+	.name = "ext_snoop.this_agent.hitm",
+	.event = "event=0x77,umask=0x8,period=200000",
+	.desc = "External snoops",
+	.topic = "other",
+},
+{
+	.name = "ext_snoop.all_agents.any",
+	.event = "event=0x77,umask=0x2b,period=200000",
+	.desc = "External snoops",
+	.topic = "other",
+},
+{
+	.name = "ext_snoop.all_agents.clean",
+	.event = "event=0x77,umask=0x21,period=200000",
+	.desc = "External snoops",
+	.topic = "other",
+},
+{
+	.name = "ext_snoop.all_agents.hit",
+	.event = "event=0x77,umask=0x22,period=200000",
+	.desc = "External snoops",
+	.topic = "other",
+},
+{
+	.name = "ext_snoop.all_agents.hitm",
+	.event = "event=0x77,umask=0x28,period=200000",
+	.desc = "External snoops",
+	.topic = "other",
+},
+{
+	.name = "bus_hit_drv.all_agents",
+	.event = "event=0x7A,umask=0x20,period=200000",
+	.desc = "HIT signal asserted",
+	.topic = "other",
+},
+{
+	.name = "bus_hit_drv.this_agent",
+	.event = "event=0x7A,umask=0x0,period=200000",
+	.desc = "HIT signal asserted",
+	.topic = "other",
+},
+{
+	.name = "bus_hitm_drv.all_agents",
+	.event = "event=0x7B,umask=0x20,period=200000",
+	.desc = "HITM signal asserted",
+	.topic = "other",
+},
+{
+	.name = "bus_hitm_drv.this_agent",
+	.event = "event=0x7B,umask=0x0,period=200000",
+	.desc = "HITM signal asserted",
+	.topic = "other",
+},
+{
+	.name = "busq_empty.self",
+	.event = "event=0x7D,umask=0x40,period=200000",
+	.desc = "Bus queue is empty",
+	.topic = "other",
+},
+{
+	.name = "snoop_stall_drv.all_agents",
+	.event = "event=0x7E,umask=0xe0,period=200000",
+	.desc = "Bus stalled for snoops",
+	.topic = "other",
+},
+{
+	.name = "snoop_stall_drv.self",
+	.event = "event=0x7E,umask=0x40,period=200000",
+	.desc = "Bus stalled for snoops",
+	.topic = "other",
+},
+{
+	.name = "bus_io_wait.self",
+	.event = "event=0x7F,umask=0x40,period=200000",
+	.desc = "IO requests waiting in the bus queue",
+	.topic = "other",
+},
+{
+	.name = "cycles_int_masked.cycles_int_masked",
+	.event = "event=0xC6,umask=0x1,period=2000000",
+	.desc = "Cycles during which interrupts are disabled",
+	.topic = "other",
+},
+{
+	.name = "cycles_int_masked.cycles_int_pending_and_masked",
+	.event = "event=0xC6,umask=0x2,period=2000000",
+	.desc = "Cycles during which interrupts are pending and disabled",
+	.topic = "other",
+},
+{
+	.name = "hw_int_rcv",
+	.event = "event=0xC8,umask=0x0,period=200000",
+	.desc = "Hardware interrupts received",
+	.topic = "other",
+},
+{
+	.name = "misalign_mem_ref.split",
+	.event = "event=0x5,umask=0xf,period=200000",
+	.desc = "Memory references that cross an 8-byte boundary",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.ld_split",
+	.event = "event=0x5,umask=0x9,period=200000",
+	.desc = "Load splits",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.st_split",
+	.event = "event=0x5,umask=0xa,period=200000",
+	.desc = "Store splits",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.split.ar",
+	.event = "event=0x5,umask=0x8f,period=200000",
+	.desc = "Memory references that cross an 8-byte boundary (At Retirement)",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.ld_split.ar",
+	.event = "event=0x5,umask=0x89,period=200000",
+	.desc = "Load splits (At Retirement)",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.st_split.ar",
+	.event = "event=0x5,umask=0x8a,period=200000",
+	.desc = "Store splits (Ar Retirement)",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.rmw_split",
+	.event = "event=0x5,umask=0x8c,period=200000",
+	.desc = "ld-op-st splits",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.bubble",
+	.event = "event=0x5,umask=0x97,period=200000",
+	.desc = "Nonzero segbase 1 bubble",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.ld_bubble",
+	.event = "event=0x5,umask=0x91,period=200000",
+	.desc = "Nonzero segbase load 1 bubble",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.st_bubble",
+	.event = "event=0x5,umask=0x92,period=200000",
+	.desc = "Nonzero segbase store 1 bubble",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.rmw_bubble",
+	.event = "event=0x5,umask=0x94,period=200000",
+	.desc = "Nonzero segbase ld-op-st 1 bubble",
+	.topic = "memory",
+},
+{
+	.name = "prefetch.prefetcht0",
+	.event = "event=0x7,umask=0x81,period=200000",
+	.desc = "Streaming SIMD Extensions (SSE) PrefetchT0 instructions executed",
+	.topic = "memory",
+},
+{
+	.name = "prefetch.prefetcht1",
+	.event = "event=0x7,umask=0x82,period=200000",
+	.desc = "Streaming SIMD Extensions (SSE) PrefetchT1 instructions executed",
+	.topic = "memory",
+},
+{
+	.name = "prefetch.prefetcht2",
+	.event = "event=0x7,umask=0x84,period=200000",
+	.desc = "Streaming SIMD Extensions (SSE) PrefetchT2 instructions executed",
+	.topic = "memory",
+},
+{
+	.name = "prefetch.sw_l2",
+	.event = "event=0x7,umask=0x86,period=200000",
+	.desc = "Streaming SIMD Extensions (SSE) PrefetchT1 and PrefetchT2 instructions executed",
+	.topic = "memory",
+},
+{
+	.name = "prefetch.prefetchnta",
+	.event = "event=0x7,umask=0x88,period=200000",
+	.desc = "Streaming SIMD Extensions (SSE) Prefetch NTA instructions executed",
+	.topic = "memory",
+},
+{
+	.name = "prefetch.hw_prefetch",
+	.event = "event=0x7,umask=0x10,period=2000000",
+	.desc = "L1 hardware prefetch request",
+	.topic = "memory",
+},
+{
+	.name = "prefetch.software_prefetch",
+	.event = "event=0x7,umask=0xf,period=200000",
+	.desc = "Any Software prefetch",
+	.topic = "memory",
+},
+{
+	.name = "prefetch.software_prefetch.ar",
+	.event = "event=0x7,umask=0x8f,period=200000",
+	.desc = "Any Software prefetch",
+	.topic = "memory",
+},
+{
+	.name = "data_tlb_misses.dtlb_miss",
+	.event = "event=0x8,umask=0x7,period=200000",
+	.desc = "Memory accesses that missed the DTLB",
+	.topic = "virtual memory",
+},
+{
+	.name = "data_tlb_misses.dtlb_miss_ld",
+	.event = "event=0x8,umask=0x5,period=200000",
+	.desc = "DTLB misses due to load operations",
+	.topic = "virtual memory",
+},
+{
+	.name = "data_tlb_misses.l0_dtlb_miss_ld",
+	.event = "event=0x8,umask=0x9,period=200000",
+	.desc = "L0 DTLB misses due to load operations",
+	.topic = "virtual memory",
+},
+{
+	.name = "data_tlb_misses.dtlb_miss_st",
+	.event = "event=0x8,umask=0x6,period=200000",
+	.desc = "DTLB misses due to store operations",
+	.topic = "virtual memory",
+},
+{
+	.name = "data_tlb_misses.l0_dtlb_miss_st",
+	.event = "event=0x8,umask=0xa,period=200000",
+	.desc = "L0 DTLB misses due to store operations",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walks.walks",
+	.event = "event=0xC,umask=0x3,period=200000",
+	.desc = "Number of page-walks executed",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walks.cycles",
+	.event = "event=0xC,umask=0x3,period=2000000",
+	.desc = "Duration of page-walks in core cycles",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walks.d_side_walks",
+	.event = "event=0xC,umask=0x1,period=200000",
+	.desc = "Number of D-side only page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walks.d_side_cycles",
+	.event = "event=0xC,umask=0x1,period=2000000",
+	.desc = "Duration of D-side only page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walks.i_side_walks",
+	.event = "event=0xC,umask=0x2,period=200000",
+	.desc = "Number of I-Side page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walks.i_side_cycles",
+	.event = "event=0xC,umask=0x2,period=2000000",
+	.desc = "Duration of I-Side page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb.hit",
+	.event = "event=0x82,umask=0x1,period=200000",
+	.desc = "ITLB hits",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb.flush",
+	.event = "event=0x82,umask=0x4,period=200000",
+	.desc = "ITLB flushes",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb.misses",
+	.event = "event=0x82,umask=0x2,period=200000",
+	.desc = "ITLB misses (Must be precise)",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_load_retired.dtlb_miss",
+	.event = "event=0xCB,umask=0x4,period=200000",
+	.desc = "Retired loads that miss the DTLB (precise event) (Precise event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "l2_ads.self",
+	.event = "event=0x21,umask=0x40,period=200000",
+	.desc = "Cycles L2 address bus is in use",
+	.topic = "cache",
+},
+{
+	.name = "l2_dbus_busy.self",
+	.event = "event=0x22,umask=0x40,period=200000",
+	.desc = "Cycles the L2 cache data bus is busy",
+	.topic = "cache",
+},
+{
+	.name = "l2_dbus_busy_rd.self",
+	.event = "event=0x23,umask=0x40,period=200000",
+	.desc = "Cycles the L2 transfers data to the core",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.self.any",
+	.event = "event=0x24,umask=0x70,period=200000",
+	.desc = "L2 cache misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.self.demand",
+	.event = "event=0x24,umask=0x40,period=200000",
+	.desc = "L2 cache misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.self.prefetch",
+	.event = "event=0x24,umask=0x50,period=200000",
+	.desc = "L2 cache misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_m_lines_in.self",
+	.event = "event=0x25,umask=0x40,period=200000",
+	.desc = "L2 cache line modifications",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.self.any",
+	.event = "event=0x26,umask=0x70,period=200000",
+	.desc = "L2 cache lines evicted",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.self.demand",
+	.event = "event=0x26,umask=0x40,period=200000",
+	.desc = "L2 cache lines evicted",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.self.prefetch",
+	.event = "event=0x26,umask=0x50,period=200000",
+	.desc = "L2 cache lines evicted",
+	.topic = "cache",
+},
+{
+	.name = "l2_m_lines_out.self.any",
+	.event = "event=0x27,umask=0x70,period=200000",
+	.desc = "Modified lines evicted from the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_m_lines_out.self.demand",
+	.event = "event=0x27,umask=0x40,period=200000",
+	.desc = "Modified lines evicted from the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_m_lines_out.self.prefetch",
+	.event = "event=0x27,umask=0x50,period=200000",
+	.desc = "Modified lines evicted from the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_ifetch.self.e_state",
+	.event = "event=0x28,umask=0x44,period=200000",
+	.desc = "L2 cacheable instruction fetch requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_ifetch.self.i_state",
+	.event = "event=0x28,umask=0x41,period=200000",
+	.desc = "L2 cacheable instruction fetch requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_ifetch.self.m_state",
+	.event = "event=0x28,umask=0x48,period=200000",
+	.desc = "L2 cacheable instruction fetch requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_ifetch.self.s_state",
+	.event = "event=0x28,umask=0x42,period=200000",
+	.desc = "L2 cacheable instruction fetch requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_ifetch.self.mesi",
+	.event = "event=0x28,umask=0x4f,period=200000",
+	.desc = "L2 cacheable instruction fetch requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.any.e_state",
+	.event = "event=0x29,umask=0x74,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.any.i_state",
+	.event = "event=0x29,umask=0x71,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.any.m_state",
+	.event = "event=0x29,umask=0x78,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.any.s_state",
+	.event = "event=0x29,umask=0x72,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.any.mesi",
+	.event = "event=0x29,umask=0x7f,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.demand.e_state",
+	.event = "event=0x29,umask=0x44,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.demand.i_state",
+	.event = "event=0x29,umask=0x41,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.demand.m_state",
+	.event = "event=0x29,umask=0x48,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.demand.s_state",
+	.event = "event=0x29,umask=0x42,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.demand.mesi",
+	.event = "event=0x29,umask=0x4f,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.prefetch.e_state",
+	.event = "event=0x29,umask=0x54,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.prefetch.i_state",
+	.event = "event=0x29,umask=0x51,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.prefetch.m_state",
+	.event = "event=0x29,umask=0x58,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.prefetch.s_state",
+	.event = "event=0x29,umask=0x52,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld.self.prefetch.mesi",
+	.event = "event=0x29,umask=0x5f,period=200000",
+	.desc = "L2 cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_st.self.e_state",
+	.event = "event=0x2A,umask=0x44,period=200000",
+	.desc = "L2 store requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_st.self.i_state",
+	.event = "event=0x2A,umask=0x41,period=200000",
+	.desc = "L2 store requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_st.self.m_state",
+	.event = "event=0x2A,umask=0x48,period=200000",
+	.desc = "L2 store requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_st.self.s_state",
+	.event = "event=0x2A,umask=0x42,period=200000",
+	.desc = "L2 store requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_st.self.mesi",
+	.event = "event=0x2A,umask=0x4f,period=200000",
+	.desc = "L2 store requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_lock.self.e_state",
+	.event = "event=0x2B,umask=0x44,period=200000",
+	.desc = "L2 locked accesses",
+	.topic = "cache",
+},
+{
+	.name = "l2_lock.self.i_state",
+	.event = "event=0x2B,umask=0x41,period=200000",
+	.desc = "L2 locked accesses",
+	.topic = "cache",
+},
+{
+	.name = "l2_lock.self.m_state",
+	.event = "event=0x2B,umask=0x48,period=200000",
+	.desc = "L2 locked accesses",
+	.topic = "cache",
+},
+{
+	.name = "l2_lock.self.s_state",
+	.event = "event=0x2B,umask=0x42,period=200000",
+	.desc = "L2 locked accesses",
+	.topic = "cache",
+},
+{
+	.name = "l2_lock.self.mesi",
+	.event = "event=0x2B,umask=0x4f,period=200000",
+	.desc = "L2 locked accesses",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.self.e_state",
+	.event = "event=0x2C,umask=0x44,period=200000",
+	.desc = "All data requests from the L1 data cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.self.i_state",
+	.event = "event=0x2C,umask=0x41,period=200000",
+	.desc = "All data requests from the L1 data cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.self.m_state",
+	.event = "event=0x2C,umask=0x48,period=200000",
+	.desc = "All data requests from the L1 data cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.self.s_state",
+	.event = "event=0x2C,umask=0x42,period=200000",
+	.desc = "All data requests from the L1 data cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.self.mesi",
+	.event = "event=0x2C,umask=0x4f,period=200000",
+	.desc = "All data requests from the L1 data cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld_ifetch.self.e_state",
+	.event = "event=0x2D,umask=0x44,period=200000",
+	.desc = "All read requests from L1 instruction and data caches",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld_ifetch.self.i_state",
+	.event = "event=0x2D,umask=0x41,period=200000",
+	.desc = "All read requests from L1 instruction and data caches",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld_ifetch.self.m_state",
+	.event = "event=0x2D,umask=0x48,period=200000",
+	.desc = "All read requests from L1 instruction and data caches",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld_ifetch.self.s_state",
+	.event = "event=0x2D,umask=0x42,period=200000",
+	.desc = "All read requests from L1 instruction and data caches",
+	.topic = "cache",
+},
+{
+	.name = "l2_ld_ifetch.self.mesi",
+	.event = "event=0x2D,umask=0x4f,period=200000",
+	.desc = "All read requests from L1 instruction and data caches",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.any.e_state",
+	.event = "event=0x2E,umask=0x74,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.any.i_state",
+	.event = "event=0x2E,umask=0x71,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.any.m_state",
+	.event = "event=0x2E,umask=0x78,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.any.s_state",
+	.event = "event=0x2E,umask=0x72,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.any.mesi",
+	.event = "event=0x2E,umask=0x7f,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.demand.e_state",
+	.event = "event=0x2E,umask=0x44,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.demand.m_state",
+	.event = "event=0x2E,umask=0x48,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.demand.s_state",
+	.event = "event=0x2E,umask=0x42,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.prefetch.e_state",
+	.event = "event=0x2E,umask=0x54,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.prefetch.i_state",
+	.event = "event=0x2E,umask=0x51,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.prefetch.m_state",
+	.event = "event=0x2E,umask=0x58,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.prefetch.s_state",
+	.event = "event=0x2E,umask=0x52,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.prefetch.mesi",
+	.event = "event=0x2E,umask=0x5f,period=200000",
+	.desc = "L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.demand.i_state",
+	.event = "event=0x2E,umask=0x41,period=200000",
+	.desc = "L2 cache demand requests from this core that missed the L2",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.self.demand.mesi",
+	.event = "event=0x2E,umask=0x4f,period=200000",
+	.desc = "L2 cache demand requests from this core",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.any.e_state",
+	.event = "event=0x30,umask=0x74,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.any.i_state",
+	.event = "event=0x30,umask=0x71,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.any.m_state",
+	.event = "event=0x30,umask=0x78,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.any.s_state",
+	.event = "event=0x30,umask=0x72,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.any.mesi",
+	.event = "event=0x30,umask=0x7f,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.demand.e_state",
+	.event = "event=0x30,umask=0x44,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.demand.i_state",
+	.event = "event=0x30,umask=0x41,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.demand.m_state",
+	.event = "event=0x30,umask=0x48,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.demand.s_state",
+	.event = "event=0x30,umask=0x42,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.demand.mesi",
+	.event = "event=0x30,umask=0x4f,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.prefetch.e_state",
+	.event = "event=0x30,umask=0x54,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.prefetch.i_state",
+	.event = "event=0x30,umask=0x51,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.prefetch.m_state",
+	.event = "event=0x30,umask=0x58,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.prefetch.s_state",
+	.event = "event=0x30,umask=0x52,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_reject_busq.self.prefetch.mesi",
+	.event = "event=0x30,umask=0x5f,period=200000",
+	.desc = "Rejected L2 cache requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_no_req.self",
+	.event = "event=0x32,umask=0x40,period=200000",
+	.desc = "Cycles no L2 cache requests are pending",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache.ld",
+	.event = "event=0x40,umask=0xa1,period=2000000",
+	.desc = "L1 Cacheable Data Reads",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache.st",
+	.event = "event=0x40,umask=0xa2,period=2000000",
+	.desc = "L1 Cacheable Data Writes",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache.all_ref",
+	.event = "event=0x40,umask=0x83,period=2000000",
+	.desc = "L1 Data reads and writes",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache.all_cache_ref",
+	.event = "event=0x40,umask=0xa3,period=2000000",
+	.desc = "L1 Data Cacheable reads and writes",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache.repl",
+	.event = "event=0x40,umask=0x8,period=200000",
+	.desc = "L1 Data line replacements",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache.replm",
+	.event = "event=0x40,umask=0x48,period=200000",
+	.desc = "Modified cache lines allocated in the L1 data cache",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache.evict",
+	.event = "event=0x40,umask=0x10,period=200000",
+	.desc = "Modified cache lines evicted from the L1 data cache",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l2_hit",
+	.event = "event=0xCB,umask=0x1,period=200000",
+	.desc = "Retired loads that hit the L2 cache (precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l2_miss",
+	.event = "event=0xCB,umask=0x2,period=10000",
+	.desc = "Retired loads that miss the L2 cache",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_jaketown[] = {
+{
+	.name = "other_assists.avx_store",
+	.event = "event=0xC1,umask=0x8,period=100003",
+	.desc = "Number of GSSE memory assist for stores. GSSE microcode assist is being invoked whenever the hardware is unable to properly handle GSSE-256b operations",
+	.topic = "floating point",
+},
+{
+	.name = "other_assists.avx_to_sse",
+	.event = "event=0xC1,umask=0x10,period=100003",
+	.desc = "Number of transitions from AVX-256 to legacy SSE when penalty applicable",
+	.topic = "floating point",
+},
+{
+	.name = "other_assists.sse_to_avx",
+	.event = "event=0xC1,umask=0x20,period=100003",
+	.desc = "Number of transitions from SSE to AVX-256 when penalty applicable",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.x87_output",
+	.event = "event=0xCA,umask=0x2,period=100003",
+	.desc = "Number of X87 assists due to output value",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.x87_input",
+	.event = "event=0xCA,umask=0x4,period=100003",
+	.desc = "Number of X87 assists due to input value",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.simd_output",
+	.event = "event=0xCA,umask=0x8,period=100003",
+	.desc = "Number of SIMD FP assists due to Output values",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.simd_input",
+	.event = "event=0xCA,umask=0x10,period=100003",
+	.desc = "Number of SIMD FP assists due to input values",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.x87",
+	.event = "event=0x10,umask=0x1,period=2000003",
+	.desc = "Number of FP Computational Uops Executed this cycle. The number of FADD, FSUB, FCOM, FMULs, integer MULsand IMULs, FDIVs, FPREMs, FSQRTS, integer DIVs, and IDIVs. This event does not distinguish an FADD used in the middle of a transcendental flow from a s",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_packed_double",
+	.event = "event=0x10,umask=0x10,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational packed double-precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_scalar_single",
+	.event = "event=0x10,umask=0x20,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational scalar single-precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_packed_single",
+	.event = "event=0x10,umask=0x40,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational packed single-precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_scalar_double",
+	.event = "event=0x10,umask=0x80,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational scalar double-precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "simd_fp_256.packed_single",
+	.event = "event=0x11,umask=0x1,period=2000003",
+	.desc = "Number of GSSE-256 Computational FP single precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "simd_fp_256.packed_double",
+	.event = "event=0x11,umask=0x2,period=2000003",
+	.desc = "Number of AVX-256 Computational FP double precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.any",
+	.event = "event=0xCA,umask=0x1e,period=100003,cmask=1",
+	.desc = "Cycles with any input/output SSE or FP assist",
+	.topic = "floating point",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired from execution",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions retired from execution. For instructions that consist of multiple micro-ops, this event counts the retirement of the last micro-op of the instruction. Counting continues during hardware interrupts, traps, and inside interrupt handlers",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Core cycles when the thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of core cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. This event is a component in many key event ratios. The core frequency may change from time to time due to transitions associated with Enhanced Intel SpeedStep Technology or TM2. For this reason this event may have a changing ratio with regards to time. When the core frequency is constant, this event can approximate elapsed time while the core was not in the halt state. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Reference cycles when the core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of reference cycles when the core is not in a halt state. The core enters the halt state when it is running the HLT instruction or the MWAIT instruction. This event is not affected by core frequency changes (for example, P states, TM2 transitions) but has the same incrementing frequency as the time stamp counter. This event can approximate elapsed time while the core was not in a halt state. This event has a constant ratio with the CPU_CLK_UNHALTED.REF_XCLK event. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events",
+},
+{
+	.name = "br_inst_exec.nontaken_conditional",
+	.event = "event=0x88,umask=0x41,period=200003",
+	.desc = "Not taken macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_conditional",
+	.event = "event=0x88,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_direct_jump",
+	.event = "event=0x88,umask=0x82,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branch instructions excluding calls and indirects",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_return",
+	.event = "event=0x88,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired indirect branches with return mnemonic",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_direct_near_call",
+	.event = "event=0x88,umask=0x90,period=200003",
+	.desc = "Taken speculative and retired direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_call",
+	.event = "event=0x88,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_conditional",
+	.event = "event=0x88,umask=0xc1,period=200003",
+	.desc = "Speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_direct_jmp",
+	.event = "event=0x88,umask=0xc2,period=200003",
+	.desc = "Speculative and retired macro-unconditional branches excluding calls and indirects",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0xc4,period=200003",
+	.desc = "Speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_indirect_near_return",
+	.event = "event=0x88,umask=0xc8,period=200003",
+	.desc = "Speculative and retired indirect return branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_direct_near_call",
+	.event = "event=0x88,umask=0xd0,period=200003",
+	.desc = "Speculative and retired direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.nontaken_conditional",
+	.event = "event=0x89,umask=0x41,period=200003",
+	.desc = "Not taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_conditional",
+	.event = "event=0x89,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_return_near",
+	.event = "event=0x89,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches with return mnemonic",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_direct_near_call",
+	.event = "event=0x89,umask=0x90,period=200003",
+	.desc = "Taken speculative and retired mispredicted direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_indirect_near_call",
+	.event = "event=0x89,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_conditional",
+	.event = "event=0x89,umask=0xc1,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0xc4,period=200003",
+	.desc = "Mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_direct_near_call",
+	.event = "event=0x89,umask=0xd0,period=200003",
+	.desc = "Speculative and retired mispredicted direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Thread cycles when thread is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.uops",
+	.event = "event=0xA8,umask=0x1,period=2000003",
+	.desc = "Number of Uops delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_active",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000003",
+	.desc = "Stalls caused by changing prefix length of the instruction",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.iq_full",
+	.event = "event=0x87,umask=0x4,period=2000003",
+	.desc = "Stall cycles because IQ is full",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.rat_stall_cycles",
+	.event = "event=0x0D,umask=0x40,period=2000003",
+	.desc = "Cycles when Resource Allocation Table (RAT) external stall is sent to Instruction Decode Queue (IDQ) for the thread",
+	.topic = "pipeline",
+},
+{
+	.name = "partial_rat_stalls.flags_merge_uop",
+	.event = "event=0x59,umask=0x20,period=2000003",
+	.desc = "Increments the number of flags-merge uops in flight each cycle",
+	.topic = "pipeline",
+},
+{
+	.name = "partial_rat_stalls.slow_lea_window",
+	.event = "event=0x59,umask=0x40,period=2000003",
+	.desc = "Cycles with at least one slow LEA uop being allocated",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of cycles with at least one slow LEA uop being allocated. A uop is generally considered as slow LEA if it has three sources (for example, two sources and immediate) regardless of whether it is a result of LEA instruction or not. Examples of the slow LEA uop are or uops with base, index, and offset source operands using base and index reqisters, where base is EBR/RBP/R13, using RIP relative or 16-bit addressing modes. See the Intel? 64 and IA-32 Architectures Optimization Reference Manual for more details about slow LEA instructions",
+},
+{
+	.name = "partial_rat_stalls.mul_single_uop",
+	.event = "event=0x59,umask=0x80,period=2000003",
+	.desc = "Multiply packed/scalar single precision uops allocated",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000003",
+	.desc = "Resource-related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.lb",
+	.event = "event=0xA2,umask=0x2,period=2000003",
+	.desc = "Counts the cycles of stall due to lack of load buffers",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rs",
+	.event = "event=0xA2,umask=0x4,period=2000003",
+	.desc = "Cycles stalled due to no eligible RS entry available",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.sb",
+	.event = "event=0xA2,umask=0x8,period=2000003",
+	.desc = "Cycles stalled due to no store buffers available. (not including draining form sync)",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rob",
+	.event = "event=0xA2,umask=0x10,period=2000003",
+	.desc = "Cycles stalled due to re-order buffer full",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls2.bob_full",
+	.event = "event=0x5B,umask=0x40,period=2000003",
+	.desc = "Cycles when Allocator is stalled if BOB is full and new branch needs it",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0x0E,umask=0x1,period=2000003",
+	.desc = "Uops that Resource Allocation Table (RAT) issues to Reservation Station (RS)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of Uops issued by the front-end of the pipeilne to the back-end",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.core_stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for all threads",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_cycles",
+	.event = "event=0x5E,umask=0x1,period=2000003",
+	.desc = "Cycles when Reservation Station (RS) is empty for the thread",
+	.topic = "pipeline",
+},
+{
+	.name = "rob_misc_events.lbr_inserts",
+	.event = "event=0xCC,umask=0x20,period=2000003",
+	.desc = "Count cases of saving new LBR",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=100003",
+	.desc = "Self-modifying code (SMC) detected",
+	.topic = "pipeline",
+	.long_desc = "This event is incremented when self-modifying code (SMC) is detected, which causes a machine clear.  Machine clears can have a significant performance impact if they are happening frequently",
+},
+{
+	.name = "machine_clears.maskmov",
+	.event = "event=0xC3,umask=0x20,period=100003",
+	.desc = "This event counts the number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0",
+	.topic = "pipeline",
+	.long_desc = "Maskmov false fault - counts number of time ucode passes through Maskmov flow due to instruction's mask being 0 while the flow was completed without raising a fault",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Number of instructions retired. General Counter   - architectural event",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.all",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "Actually retired uops (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of micro-ops retired (Precise event)",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000003",
+	.desc = "Retirement slots used (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of retirement slots used each cycle.  There are potentially 4 slots that can be used each cycle - meaning, 4 micro-ops or 4 instructions could retire each cycle.  This event is used in determining the 'Retiring' category of the Top-Down pipeline slots characterization (Precise event)",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=10",
+	.desc = "Cycles with less than 10 actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=400009",
+	.desc = "Conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=100007",
+	.desc = "Direct and indirect near call instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=400009",
+	.desc = "All (macro) branch instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.near_return",
+	.event = "event=0xC4,umask=0x8,period=100007",
+	.desc = "Return instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.not_taken",
+	.event = "event=0xC4,umask=0x10,period=400009",
+	.desc = "Not taken branch instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.near_taken",
+	.event = "event=0xC4,umask=0x20,period=400009",
+	.desc = "Taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0x40,period=100007",
+	.desc = "Far branch instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.all_branches_pebs",
+	.event = "event=0xC4,umask=0x4,period=400009",
+	.desc = "All (macro) branch instructions retired. (Precise Event - PEBS) (Must be precise)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=400009",
+	.desc = "Mispredicted conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_call",
+	.event = "event=0xC5,umask=0x2,period=100007",
+	.desc = "Direct and indirect mispredicted near call instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=400009",
+	.desc = "All mispredicted macro branch instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.not_taken",
+	.event = "event=0xC5,umask=0x10,period=400009",
+	.desc = "Mispredicted not taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.taken",
+	.event = "event=0xC5,umask=0x20,period=400009",
+	.desc = "Mispredicted taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.all_branches_pebs",
+	.event = "event=0xC5,umask=0x4,period=400009",
+	.desc = "Mispredicted macro branch instructions retired. (Precise Event - PEBS) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted macro branch instructions retired. (Precise Event - PEBS) (Must be precise)",
+},
+{
+	.name = "other_assists.itlb_miss_retired",
+	.event = "event=0xC1,umask=0x2,period=100003",
+	.desc = "Retired instructions experiencing ITLB misses",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.fpu_div_active",
+	.event = "event=0x14,umask=0x1,period=2000003",
+	.desc = "Cycles when divider is busy executing divide operations",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.fpu_div",
+	.event = "event=0x14,umask=0x1,edge=1,period=100003,cmask=1",
+	.desc = "Divide operations executed",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of the divide operations executed",
+},
+{
+	.name = "uops_dispatched.thread",
+	.event = "event=0xB1,umask=0x1,period=2000003",
+	.desc = "Uops dispatched per thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched.core",
+	.event = "event=0xB1,umask=0x2,period=2000003",
+	.desc = "Uops dispatched from any thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_4",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_5",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_no_dispatch",
+	.event = "event=0xA3,umask=0x4,period=2000003,cmask=4",
+	.desc = "Each cycle there was no dispatch for this thread, increment by 1. Note this is connect to Umask 2. No dispatch can be deduced from the UOPS_EXECUTED event",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l1d_pending",
+	.event = "event=0xA3,umask=0x2,period=2000003,cmask=2",
+	.desc = "Each cycle there was a miss-pending demand load this thread, increment by 1. Note this is in DCU and connected to Umask 1. Miss Pending demand load should be deduced by OR-ing increment bits of DCACHE_MISS_PEND.PENDING",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l2_pending",
+	.event = "event=0xA3,umask=0x1,period=2000003,cmask=1",
+	.desc = "Each cycle there was a MLC-miss pending demand load this thread (i.e. Non-completed valid SQ entry allocated for demand load and waiting for Uncore), increment by 1. Note this is in MLC and connected to Umask 0",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l1d_pending",
+	.event = "event=0xA3,umask=0x6,period=2000003,cmask=6",
+	.desc = "Each cycle there was a miss-pending demand load this thread and no uops dispatched, increment by 1. Note this is in DCU and connected to Umask 1 and 2. Miss Pending demand load should be deduced by OR-ing increment bits of DCACHE_MISS_PEND.PENDING",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l2_pending",
+	.event = "event=0xA3,umask=0x5,period=2000003,cmask=5",
+	.desc = "Each cycle there was a MLC-miss pending demand load and no uops dispatched on this thread (i.e. Non-completed valid SQ entry allocated for demand load and waiting for Uncore), increment by 1. Note this is in MLC and connected to Umask 0 and 2",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.sw_pf",
+	.event = "event=0x4C,umask=0x1,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for software prefetch",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.hw_pf",
+	.event = "event=0x4C,umask=0x2,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for hardware prefetch",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks.data_unknown",
+	.event = "event=0x03,umask=0x1,period=100003",
+	.desc = "Loads delayed due to SB blocks, preceding store operations with known addresses but unknown data",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks.store_forward",
+	.event = "event=0x03,umask=0x2,period=100003",
+	.desc = "Cases when loads get true Block-on-Store blocking code preventing store forwarding",
+	.topic = "pipeline",
+	.long_desc = "This event counts loads that followed a store to the same address, where the data could not be forwarded inside the pipeline from the store to the load.  The most common reason why store forwarding would be blocked is when a load's address range overlaps with a preceeding smaller uncompleted store.  See the table of not supported store forwards in the Intel? 64 and IA-32 Architectures Optimization Reference Manual.  The penalty for blocked store forwarding is that the load must wait for the store to complete before it can be issued",
+},
+{
+	.name = "ld_blocks.no_sr",
+	.event = "event=0x03,umask=0x8,period=100003",
+	.desc = "This event counts the number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks.all_block",
+	.event = "event=0x03,umask=0x10,period=100003",
+	.desc = "Number of cases where any load ends up with a valid block-code written to the load buffer (including blocks due to Memory Order Buffer (MOB), Data Cache Unit (DCU), TLB, but load has no DCU miss)",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks_partial.address_alias",
+	.event = "event=0x07,umask=0x1,period=100003",
+	.desc = "False dependencies in MOB due to partial compare",
+	.topic = "pipeline",
+	.long_desc = "Aliasing occurs when a load is issued after a store and their memory addresses are offset by 4K.  This event counts the number of loads that aliased with a preceding store, resulting in an extended address check in the pipeline.  The enhanced address check typically has a performance penalty of 5 cycles",
+},
+{
+	.name = "ld_blocks_partial.all_sta_block",
+	.event = "event=0x07,umask=0x8,period=100003",
+	.desc = "This event counts the number of times that load operations are temporarily blocked because of older stores, with addresses that are not yet known. A load operation may incur more than one block of this type",
+	.topic = "pipeline",
+},
+{
+	.name = "agu_bypass_cancel.count",
+	.event = "event=0xB6,umask=0x1,period=100003",
+	.desc = "This event counts executed load operations with all the following traits: 1. addressing of the format [base + offset], 2. the offset is between 1 and 2047, 3. the address specified in the base register is in one page and the address [base+offset] is in an",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_0_core",
+	.event = "event=0xA1,umask=0x1,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_1_core",
+	.event = "event=0xA1,umask=0x2,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_4_core",
+	.event = "event=0xA1,umask=0x40,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_5_core",
+	.event = "event=0xA1,umask=0x80,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_2",
+	.event = "event=0xA1,umask=0xc,period=2000003",
+	.desc = "Cycles per thread when load or STA uops are dispatched to port 2",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_3",
+	.event = "event=0xA1,umask=0x30,period=2000003",
+	.desc = "Cycles per thread when load or STA uops are dispatched to port 3",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_2_core",
+	.event = "event=0xA1,umask=0xc,any=1,period=2000003",
+	.desc = "Cycles per core when load or STA uops are dispatched to port 2",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_3_core",
+	.event = "event=0xA1,umask=0x30,any=1,period=2000003",
+	.desc = "Cycles per core when load or STA uops are dispatched to port 3",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.prec_dist",
+	.event = "event=0xC0,umask=0x1,period=2000003",
+	.desc = "Instructions retired. (Precise Event - PEBS) (Must be precise)",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls2.all_prf_control",
+	.event = "event=0x5B,umask=0xf,period=2000003",
+	.desc = "Resource stalls2 control structures full for physical registers",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls2.all_fl_empty",
+	.event = "event=0x5B,umask=0xc,period=2000003",
+	.desc = "Cycles with either free list is empty",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.mem_rs",
+	.event = "event=0xA2,umask=0xe,period=2000003",
+	.desc = "Resource stalls due to memory buffers or Reservation Station (RS) being fully utilized",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.ooo_rsrc",
+	.event = "event=0xA2,umask=0xf0,period=2000003",
+	.desc = "Resource stalls due to Rob being full, FCSW, MXCSR and OTHER",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls2.ooo_rsrc",
+	.event = "event=0x5B,umask=0x4f,period=2000003",
+	.desc = "Resource stalls out of order resources full",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.lb_sb",
+	.event = "event=0xA2,umask=0xa,period=2000003",
+	.desc = "Resource stalls due to load or store buffers all being in use",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_cycles",
+	.event = "event=0x0D,umask=0x3,period=2000003,cmask=1",
+	.desc = "Number of cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc...)",
+	.topic = "pipeline",
+},
+{
+	.name = "partial_rat_stalls.flags_merge_uop_cycles",
+	.event = "event=0x59,umask=0x20,period=2000003,cmask=1",
+	.desc = "Performance sensitive flags-merging uops added by Sandy Bridge u-arch",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of cycles spent executing performance-sensitive flags-merging uops. For example, shift CL (merge_arith_flags). For more details, See the Intel? 64 and IA-32 Architectures Optimization Reference Manual",
+},
+{
+	.name = "int_misc.recovery_stalls_count",
+	.event = "event=0x0D,umask=0x3,edge=1,period=2000003,cmask=1",
+	.desc = "Number of occurences waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc...)",
+	.topic = "pipeline",
+},
+{
+	.name = "baclears.any",
+	.event = "event=0xE6,umask=0x1f,period=100003",
+	.desc = "Counts the total number when the front end is resteered, mainly when the BPU cannot provide a correct prediction and this is corrected by other branch handling mechanisms at the front end",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_branches",
+	.event = "event=0x88,umask=0xff,period=200003",
+	.desc = "Speculative and retired  branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_branches",
+	.event = "event=0x89,umask=0xff,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.core_stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_4_uops",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.count",
+	.event = "event=0xc3,umask=0x1,edge=1,period=100003,cmask=1",
+	.desc = "Number of machine clears (nukes) of any type",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_end",
+	.event = "event=0x5E,inv=1,umask=0x1,edge=1,period=2000003,cmask=1",
+	.desc = "Counts end of periods where the Reservation Station (RS) was empty. Could be useful to precisely locate Frontend Latency Bound issues",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_any",
+	.event = "event=0x3c,any=1",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p_any",
+	.event = "event=0x3C,umask=0x0,any=1,period=2000003",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_cycles_any",
+	.event = "event=0x0D,umask=0x3,any=1,period=2000003,cmask=1",
+	.desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_1",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles at least 1 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_2",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles at least 2 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_3",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=3",
+	.desc = "Cycles at least 3 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_4",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=4",
+	.desc = "Cycles at least 4 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_none",
+	.event = "event=0xB1,inv=1,umask=0x2,period=2000003",
+	.desc = "Cycles with no micro-ops executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=2000003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Reads. both cacheable and noncacheable, including UC fetches",
+	.topic = "frontend",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "Instruction cache, streaming buffer and victim cache misses",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of instruction cache, streaming buffer and victim cache misses. Counting includes unchacheable accesses",
+},
+{
+	.name = "idq.empty",
+	.event = "event=0x79,umask=0x2,period=2000003",
+	.desc = "Instruction Decode Queue (IDQ) empty cycles",
+	.topic = "frontend",
+},
+{
+	.name = "idq.mite_uops",
+	.event = "event=0x79,umask=0x4,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.dsb_uops",
+	.event = "event=0x79,umask=0x8,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_dsb_uops",
+	.event = "event=0x79,umask=0x10,period=2000003",
+	.desc = "Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_mite_uops",
+	.event = "event=0x79,umask=0x20,period=2000003",
+	.desc = "Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_uops",
+	.event = "event=0x79,umask=0x30,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_cycles",
+	.event = "event=0x79,umask=0x30,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which the microcode sequencer assisted the front-end in delivering uops.  Microcode assists are used for complex instructions or scenarios that can't be handled by the standard decoder.  Using other instructions, if possible, will usually improve performance.  See the Intel? 64 and IA-32 Architectures Optimization Reference Manual for more information",
+},
+{
+	.name = "idq_uops_not_delivered.core",
+	.event = "event=0x9C,umask=0x1,period=2000003",
+	.desc = "Uops not delivered to Resource Allocation Table (RAT) per thread when backend of the machine is not stalled ",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops not delivered to the back-end per cycle, per thread, when the back-end was not stalled.  In the ideal case 4 uops can be delivered each cycle.  The event counts the undelivered uops - so if 3 were delivered in one cycle, the counter would be incremented by 1 for that cycle (4 - 3). If the back-end is stalled, the count for this event is not incremented even when uops were not delivered, because the back-end would not have been able to accept them.  This event is used in determining the front-end bound category of the top-down pipeline slots characterization",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_0_uops_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles per thread when 4 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_1_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles per thread when 3 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.count",
+	.event = "event=0xAB,umask=0x1,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switches",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.penalty_cycles",
+	.event = "event=0xAB,umask=0x2,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles",
+	.topic = "frontend",
+	.long_desc = "This event counts the cycles attributed to a switch from the Decoded Stream Buffer (DSB), which holds decoded instructions, to the legacy decode pipeline.  It excludes cycles when the back-end cannot  accept new micro-ops.  The penalty for these switches is potentially several cycles of instruction starvation, where no micro-ops are delivered to the back-end",
+},
+{
+	.name = "dsb_fill.other_cancel",
+	.event = "event=0xAC,umask=0x2,period=2000003",
+	.desc = "Cases of cancelling valid DSB fill not because of exceeding way limit",
+	.topic = "frontend",
+},
+{
+	.name = "dsb_fill.exceed_dsb_lines",
+	.event = "event=0xAC,umask=0x8,period=2000003",
+	.desc = "Cycles when Decode Stream Buffer (DSB) fill encounter more than 3 Decode Stream Buffer (DSB) lines",
+	.topic = "frontend",
+},
+{
+	.name = "idq.mite_cycles",
+	.event = "event=0x79,umask=0x4,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.dsb_cycles",
+	.event = "event=0x79,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_dsb_cycles",
+	.event = "event=0x79,umask=0x10,period=2000003,cmask=1",
+	.desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_dsb_occur",
+	.event = "event=0x79,umask=0x10,edge=1,period=2000003,cmask=1",
+	.desc = "Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_2_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles with less than 2 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_3_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with less than 3 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_ge_1_uop_deliv.core",
+	.event = "event=0x9C,inv=1,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles when 1 or more uops were delivered to the by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq.all_dsb_cycles_4_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=4",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering 4 Uops",
+	.topic = "frontend",
+},
+{
+	.name = "idq.all_dsb_cycles_any_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=1",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering any Uop",
+	.topic = "frontend",
+},
+{
+	.name = "idq.all_mite_cycles_4_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=4",
+	.desc = "Cycles MITE is delivering 4 Uops",
+	.topic = "frontend",
+},
+{
+	.name = "idq.all_mite_cycles_any_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=1",
+	.desc = "Cycles MITE is delivering any Uop",
+	.topic = "frontend",
+},
+{
+	.name = "dsb_fill.all_cancel",
+	.event = "event=0xAC,umask=0xa,period=2000003",
+	.desc = "Cases of cancelling valid Decode Stream Buffer (DSB) fill not because of exceeding way limit",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_fe_was_ok",
+	.event = "event=0x9C,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts cycles FE delivered 4 uops or Resource Allocation Table (RAT) was stalling FE",
+	.topic = "frontend",
+},
+{
+	.name = "idq.mite_all_uops",
+	.event = "event=0x79,umask=0x3c,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_switches",
+	.event = "event=0x79,umask=0x30,edge=1,period=2000003,cmask=1",
+	.desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+	.topic = "frontend",
+},
+{
+	.name = "insts_written_to_iq.insts",
+	.event = "event=0x17,umask=0x1,period=2000003",
+	.desc = "Valid instructions written to IQ per cycle",
+	.topic = "other",
+},
+{
+	.name = "cpl_cycles.ring0",
+	.event = "event=0x5C,umask=0x1,period=2000003",
+	.desc = "Unhalted core cycles when the thread is in ring 0",
+	.topic = "other",
+},
+{
+	.name = "cpl_cycles.ring0_trans",
+	.event = "event=0x5C,umask=0x1,edge=1,period=100007,cmask=1",
+	.desc = "Number of intervals between processor halts while thread is in ring 0",
+	.topic = "other",
+},
+{
+	.name = "cpl_cycles.ring123",
+	.event = "event=0x5C,umask=0x2,period=2000003",
+	.desc = "Unhalted core cycles when thread is in rings 1, 2, or 3",
+	.topic = "other",
+},
+{
+	.name = "hw_pre_req.dl1_miss",
+	.event = "event=0x4E,umask=0x2,period=2000003",
+	.desc = "Hardware Prefetch requests that miss the L1D cache. This accounts for both L1 streamer and IP-based (IPP) HW prefetchers. A request is being counted each time it access the cache & miss it, including if a block is applicable or if hit the Fill Buffer for ",
+	.topic = "other",
+},
+{
+	.name = "lock_cycles.split_lock_uc_lock_duration",
+	.event = "event=0x63,umask=0x1,period=2000003",
+	.desc = "Cycles when L1 and L2 are locked due to UC or split lock",
+	.topic = "other",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=100003",
+	.desc = "Counts the number of machine clears due to memory order conflicts",
+	.topic = "memory",
+	.long_desc = "This event counts the number of memory ordering Machine Clears detected. Memory Ordering Machine Clears can result from memory disambiguation, external snoops, or cross SMT-HW-thread snoop (stores) hitting load buffers.  Machine clears can have a significant performance impact if they are happening frequently",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_4",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x4",
+	.desc = "Loads with latency value being above 4  (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_8",
+	.event = "event=0xCD,umask=0x1,period=50021,ldlat=0x8",
+	.desc = "Loads with latency value being above 8 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_16",
+	.event = "event=0xCD,umask=0x1,period=20011,ldlat=0x10",
+	.desc = "Loads with latency value being above 16 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_32",
+	.event = "event=0xCD,umask=0x1,period=100007,ldlat=0x20",
+	.desc = "Loads with latency value being above 32 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_64",
+	.event = "event=0xCD,umask=0x1,period=2003,ldlat=0x40",
+	.desc = "Loads with latency value being above 64 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_128",
+	.event = "event=0xCD,umask=0x1,period=1009,ldlat=0x80",
+	.desc = "Loads with latency value being above 128 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_256",
+	.event = "event=0xCD,umask=0x1,period=503,ldlat=0x100",
+	.desc = "Loads with latency value being above 256 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_512",
+	.event = "event=0xCD,umask=0x1,period=101,ldlat=0x200",
+	.desc = "Loads with latency value being above 512 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.precise_store",
+	.event = "event=0xCD,umask=0x2,period=2000003",
+	.desc = "Sample stores and collect precise store operation via PEBS record. PMC3 only. (Precise Event - PEBS) (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.loads",
+	.event = "event=0x05,umask=0x1,period=2000003",
+	.desc = "Speculative cache line split load uops dispatched to L1 cache",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.stores",
+	.event = "event=0x05,umask=0x2,period=2000003",
+	.desc = "Speculative cache line split STA uops dispatched to L1 cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20004",
+	.desc = "Counts all demand code reads that miss the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x600400004",
+	.desc = "Counts all demand code reads that miss the LLC  and the data returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x67f800004",
+	.desc = "Counts all demand code reads that miss the LLC  and the data returned from remote dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x87f820004",
+	.desc = "Counts all demand code reads that miss the LLC  and the data forwarded from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x107fc00004",
+	.desc = "Counts all demand code reads that miss the LLC  the data is found in M state in remote cache and forwarded from there",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x67fc00001",
+	.desc = "Counts demand data reads that miss the LLC  and the data returned from remote & local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20001",
+	.desc = "Counts demand data reads that miss in the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x600400001",
+	.desc = "Counts demand data reads that miss the LLC  and the data returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x67f800001",
+	.desc = "Counts demand data reads that miss the LLC  and the data returned from remote dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x87f820001",
+	.desc = "Counts demand data reads that miss the LLC  and the data forwarded from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x107fc00001",
+	.desc = "Counts demand data reads that miss the LLC  the data is found in M state in remote cache and forwarded from there",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20040",
+	.desc = "Counts all prefetch (that bring data to L2) code reads that miss the LLC  and the data returned from remote & local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x67fc00010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss the LLC  and the data returned from remote & local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss in the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x600400010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss the LLC  and the data returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x67f800010",
+	.desc = "Counts prefetch (that bring data to L2) data reads  that miss the LLC  and the data returned from remote dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x87f820010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss the LLC  and the data forwarded from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x107fc00010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss the LLC  the data is found in M state in remote cache and forwarded from there",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20200",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that miss in the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_demand_mlc_pref_reads.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x600400077",
+	.desc = "Counts all local dram accesses for all demand and L2 prefetches. LLC prefetches are excluded",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_demand_mlc_pref_reads.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3FFFC20077",
+	.desc = "This event counts all LLC misses for all demand and L2 prefetches. LLC prefetches are excluded",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_demand_mlc_pref_reads.llc_miss.remote_hitm_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x187FC20077",
+	.desc = "This event counts all remote cache-to-cache transfers (includes HITM and HIT-Forward) for all demand and L2 prefetches. LLC prefetches are excluded",
+	.topic = "memory",
+},
+{
+	.name = "itlb.itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=100007",
+	.desc = "Flushing of the Instruction TLB (ITLB) pages, includes 4k/2M/4M pages",
+	.topic = "virtual memory",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4F,umask=0x10,period=2000003",
+	.desc = "Cycle count for an Extended Page table walk.  The Extended Page Directory cache is used by Virtual Machine operating systems while the guest operating systems use the standard TLB caches",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.miss_causes_a_walk",
+	.event = "event=0x85,umask=0x1,period=100003",
+	.desc = "Misses at all ITLB levels that cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0x2,period=100003",
+	.desc = "Misses in all ITLB levels that cause completed page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_duration",
+	.event = "event=0x85,umask=0x4,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event count cycles when Page Miss Handler (PMH) is servicing page walks caused by ITLB misses",
+},
+{
+	.name = "itlb_misses.stlb_hit",
+	.event = "event=0x85,umask=0x10,period=100003",
+	.desc = "Operations that miss the first ITLB level but hit the second and do not cause any page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.miss_causes_a_walk",
+	.event = "event=0x08,umask=0x1,period=100003",
+	.desc = "Load misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x08,umask=0x2,period=100003",
+	.desc = "Load misses at all DTLB levels that cause completed page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_duration",
+	.event = "event=0x08,umask=0x4,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts cycles when the  page miss handler (PMH) is servicing page walks caused by DTLB load misses",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x08,umask=0x10,period=100003",
+	.desc = "Load operations that miss the first DTLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load operations that miss the first DTLB level but hit the second and do not cause any page walks. The penalty in this case is approximately 7 cycles",
+},
+{
+	.name = "dtlb_store_misses.miss_causes_a_walk",
+	.event = "event=0x49,umask=0x1,period=100003",
+	.desc = "Store misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.walk_completed",
+	.event = "event=0x49,umask=0x2,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.walk_duration",
+	.event = "event=0x49,umask=0x4,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit",
+	.event = "event=0x49,umask=0x10,period=100003",
+	.desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "tlb_flush.dtlb_thread",
+	.event = "event=0xBD,umask=0x1,period=100007",
+	.desc = "DTLB flush attempts of the thread-specific entries",
+	.topic = "virtual memory",
+},
+{
+	.name = "tlb_flush.stlb_any",
+	.event = "event=0xBD,umask=0x20,period=100007",
+	.desc = "STLB flush attempts",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_loads",
+	.event = "event=0xD0,umask=0x11,period=100003",
+	.desc = "Retired load uops that miss the STLB (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_stores",
+	.event = "event=0xD0,umask=0x12,period=100003",
+	.desc = "Retired store uops that miss the STLB (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.lock_loads",
+	.event = "event=0xD0,umask=0x21,period=100007",
+	.desc = "Retired load uops with locked access (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.split_loads",
+	.event = "event=0xD0,umask=0x41,period=100003",
+	.desc = "Retired load uops that split across a cacheline boundary (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts line-splitted load uops retired to the architected path. A line split is across 64B cache-line which includes a page split (4K) (Precise event)",
+},
+{
+	.name = "mem_uops_retired.split_stores",
+	.event = "event=0xD0,umask=0x42,period=100003",
+	.desc = "Retired store uops that split across a cacheline boundary (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts line-splitted store uops retired to the architected path. A line split is across 64B cache-line which includes a page split (4K) (Precise event)",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0xD0,umask=0x81,period=2000003",
+	.desc = "All retired load uops (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts the number of load uops retired (Precise event)",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0xD0,umask=0x82,period=2000003",
+	.desc = "All retired store uops (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts the number of store uops retired (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_hit",
+	.event = "event=0xD1,umask=0x1,period=2000003",
+	.desc = "Retired load uops with L1 cache hits as data sources (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.l2_hit",
+	.event = "event=0xD1,umask=0x2,period=100003",
+	.desc = "Retired load uops with L2 cache hits as data sources (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.llc_hit",
+	.event = "event=0xD1,umask=0x4,period=50021",
+	.desc = "Retired load uops which data sources were data hits in LLC without snoops required",
+	.topic = "cache",
+	.long_desc = "This event counts retired load uops that hit in the last-level (L3) cache without snoops required",
+},
+{
+	.name = "mem_load_uops_retired.llc_miss",
+	.event = "event=0xD1,umask=0x20,period=100007",
+	.desc = "Miss in last-level (L3) cache. Excludes Unknown data-source",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.hit_lfb",
+	.event = "event=0xD1,umask=0x40,period=100003",
+	.desc = "Retired load uops which data sources were load uops missed L1 but hit FB due to preceding miss to the same cache line with data not ready (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_miss",
+	.event = "event=0xD2,umask=0x1,period=20011",
+	.desc = "Retired load uops which data sources were LLC hit and cross-core snoop missed in on-pkg core cache",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_hit",
+	.event = "event=0xD2,umask=0x2,period=20011",
+	.desc = "Retired load uops which data sources were LLC and cross-core snoop hits in on-pkg core cache",
+	.topic = "cache",
+	.long_desc = "This event counts retired load uops that hit in the last-level cache (L3) and were found in a non-modified state in a neighboring core's private cache (same package).  Since the last level cache is inclusive, hits to the L3 may require snooping the private L2 caches of any cores on the same socket that have the line.  In this case, a snoop was required, and another L2 had the line in a non-modified state",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_hitm",
+	.event = "event=0xD2,umask=0x4,period=20011",
+	.desc = "Retired load uops which data sources were HitM responses from shared LLC",
+	.topic = "cache",
+	.long_desc = "This event counts retired load uops that hit in the last-level cache (L3) and were found in a non-modified state in a neighboring core's private cache (same package).  Since the last level cache is inclusive, hits to the L3 may require snooping the private L2 caches of any cores on the same socket that have the line.  In this case, a snoop was required, and another L2 had the line in a modified state, so the line had to be invalidated in that L2 cache and transferred to the requesting L2",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_none",
+	.event = "event=0xD2,umask=0x8,period=100003",
+	.desc = "Retired load uops which data sources were hits in LLC without snoops required",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_llc_miss_retired.local_dram",
+	.event = "event=0xD3,umask=0x1,period=100007",
+	.desc = "Data from local DRAM either Snoop not needed or Snoop Miss (RspI)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_llc_miss_retired.remote_dram",
+	.event = "event=0xD3,umask=0x4,period=100007",
+	.desc = "Data from remote DRAM either Snoop not needed or Snoop Miss (RspI)",
+	.topic = "cache",
+},
+{
+	.name = "l1d.replacement",
+	.event = "event=0x51,umask=0x1,period=2000003",
+	.desc = "L1D data line replacements",
+	.topic = "cache",
+	.long_desc = "This event counts L1D data line replacements.  Replacements occur when a new line is brought into the cache, causing eviction of a line loaded earlier",
+},
+{
+	.name = "l1d.allocated_in_m",
+	.event = "event=0x51,umask=0x2,period=2000003",
+	.desc = "Allocated L1D data cache lines in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.eviction",
+	.event = "event=0x51,umask=0x4,period=2000003",
+	.desc = "L1D data cache lines in M state evicted due to replacement",
+	.topic = "cache",
+},
+{
+	.name = "l1d.all_m_replacement",
+	.event = "event=0x51,umask=0x8,period=2000003",
+	.desc = "Cache lines in M state evicted out of L1D due to Snoop HitM or dirty line replacement",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending",
+	.event = "event=0x48,umask=0x1,period=2000003",
+	.desc = "L1D miss oustandings duration in cycles",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles",
+	.event = "event=0x48,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding",
+	.topic = "cache",
+},
+{
+	.name = "lock_cycles.cache_lock_duration",
+	.event = "event=0x63,umask=0x2,period=2000003",
+	.desc = "Cycles when L1D is locked",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003",
+	.desc = "Offcore outstanding Demand Data Read transactions in uncore queue",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003",
+	.desc = "Offcore outstanding RFO store transactions in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.all_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003",
+	.desc = "Offcore outstanding cacheable Core Data Read transactions in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand_data_rd",
+	.event = "event=0xB0,umask=0x1,period=100003",
+	.desc = "Demand Data Read requests sent to uncore",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand_code_rd",
+	.event = "event=0xB0,umask=0x2,period=100003",
+	.desc = "Cacheable and noncachaeble code read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand_rfo",
+	.event = "event=0xB0,umask=0x4,period=100003",
+	.desc = "Demand RFO requests including regular RFOs, locks, ItoM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.all_data_rd",
+	.event = "event=0xB0,umask=0x8,period=100003",
+	.desc = "Demand and prefetch data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_buffer.sq_full",
+	.event = "event=0xB2,umask=0x1,period=2000003",
+	.desc = "Cases when offcore requests buffer cannot take more entries for core",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_hit",
+	.event = "event=0x24,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that hit L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x4,period=200003",
+	.desc = "RFO requests that hit L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x8,period=200003",
+	.desc = "RFO requests that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.code_rd_hit",
+	.event = "event=0x24,umask=0x10,period=200003",
+	.desc = "L2 cache hits when fetching instructions, code reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.code_rd_miss",
+	.event = "event=0x24,umask=0x20,period=200003",
+	.desc = "L2 cache misses when fetching instructions",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.pf_hit",
+	.event = "event=0x24,umask=0x40,period=200003",
+	.desc = "Requests from the L2 hardware prefetchers that hit L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.pf_miss",
+	.event = "event=0x24,umask=0x80,period=200003",
+	.desc = "Requests from the L2 hardware prefetchers that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_store_lock_rqsts.miss",
+	.event = "event=0x27,umask=0x1,period=200003",
+	.desc = "RFOs that miss cache lines",
+	.topic = "cache",
+},
+{
+	.name = "l2_store_lock_rqsts.hit_e",
+	.event = "event=0x27,umask=0x4,period=200003",
+	.desc = "RFOs that hit cache lines in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_store_lock_rqsts.hit_m",
+	.event = "event=0x27,umask=0x8,period=200003",
+	.desc = "RFOs that hit cache lines in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_store_lock_rqsts.all",
+	.event = "event=0x27,umask=0xf,period=200003",
+	.desc = "RFOs that access cache lines in any state",
+	.topic = "cache",
+},
+{
+	.name = "l2_l1d_wb_rqsts.miss",
+	.event = "event=0x28,umask=0x1,period=200003",
+	.desc = "Count the number of modified Lines evicted from L1 and missed L2. (Non-rejected WBs from the DCU.)",
+	.topic = "cache",
+},
+{
+	.name = "l2_l1d_wb_rqsts.hit_s",
+	.event = "event=0x28,umask=0x2,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_l1d_wb_rqsts.hit_e",
+	.event = "event=0x28,umask=0x4,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_l1d_wb_rqsts.hit_m",
+	.event = "event=0x28,umask=0x8,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_l1d_wb_rqsts.all",
+	.event = "event=0x28,umask=0xf,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in any state",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.demand_data_rd",
+	.event = "event=0xF0,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.rfo",
+	.event = "event=0xF0,umask=0x2,period=200003",
+	.desc = "RFO requests that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.code_rd",
+	.event = "event=0xF0,umask=0x4,period=200003",
+	.desc = "L2 cache accesses when fetching instructions",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.all_pf",
+	.event = "event=0xF0,umask=0x8,period=200003",
+	.desc = "L2 or LLC HW prefetches that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200003",
+	.desc = "L1D writebacks that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.l2_fill",
+	.event = "event=0xF0,umask=0x20,period=200003",
+	.desc = "L2 fill requests that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.l2_wb",
+	.event = "event=0xF0,umask=0x40,period=200003",
+	.desc = "L2 writebacks that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.all_requests",
+	.event = "event=0xF0,umask=0x80,period=200003",
+	.desc = "Transactions accessing L2 pipe",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.i",
+	.event = "event=0xF1,umask=0x1,period=100003",
+	.desc = "L2 cache lines in I state filling L2",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.s",
+	.event = "event=0xF1,umask=0x2,period=100003",
+	.desc = "L2 cache lines in S state filling L2",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.e",
+	.event = "event=0xF1,umask=0x4,period=100003",
+	.desc = "L2 cache lines in E state filling L2",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.all",
+	.event = "event=0xF1,umask=0x7,period=100003",
+	.desc = "L2 cache lines filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines brought into the L2 cache.  Lines are filled into the L2 cache when there was an L2 miss",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x1,period=100003",
+	.desc = "Clean L2 cache lines evicted by demand",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_dirty",
+	.event = "event=0xF2,umask=0x2,period=100003",
+	.desc = "Dirty L2 cache lines evicted by demand",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.pf_clean",
+	.event = "event=0xF2,umask=0x4,period=100003",
+	.desc = "Clean L2 cache lines evicted by L2 prefetch",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.pf_dirty",
+	.event = "event=0xF2,umask=0x8,period=100003",
+	.desc = "Dirty L2 cache lines evicted by L2 prefetch",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.dirty_all",
+	.event = "event=0xF2,umask=0xa,period=100003",
+	.desc = "Dirty L2 cache lines filling the L2",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100003",
+	.desc = "Core-originated cacheable demand requests missed LLC",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=100003",
+	.desc = "Core-originated cacheable demand requests that refer to LLC",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xF4,umask=0x10,period=100003",
+	.desc = "Split locks in SQ",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_demand_data_rd",
+	.event = "event=0x24,umask=0x3,period=200003",
+	.desc = "Demand Data Read requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_rfo",
+	.event = "event=0x24,umask=0xc,period=200003",
+	.desc = "RFO requests to L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_code_rd",
+	.event = "event=0x24,umask=0x30,period=200003",
+	.desc = "L2 code requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_pf",
+	.event = "event=0x24,umask=0xc0,period=200003",
+	.desc = "Requests from L2 hardware prefetchers",
+	.topic = "cache",
+},
+{
+	.name = "l1d_blocks.bank_conflict_cycles",
+	.event = "event=0xBF,umask=0x5,period=100003,cmask=1",
+	.desc = "Cycles when dispatched loads are cancelled due to L1D bank conflicts with other load ports",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003,cmask=1",
+	.desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd_c6",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=6",
+	.desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles_any",
+	.event = "event=0x48,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.fb_full",
+	.event = "event=0x48,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles a demand request was blocked due to Fill Buffers inavailability",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and sibling core snoop returned a clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0090",
+	.desc = "Counts all prefetch data reads that hit the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and sibling core snoop returned a clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c03f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c03f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c03f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c03f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c03f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the LLC and sibling core snoop returned a clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10008",
+	.desc = "Counts all writebacks from the core to the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0004",
+	.desc = "Counts all demand code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0001",
+	.desc = "Counts all demand data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and sibling core snoop returned a clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.lru_hints",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x803c8000",
+	.desc = "Counts L2 hints sent to LLC to keep a line from being evicted out of the core caches",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.portio_mmio_uc",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x23ffc08000",
+	.desc = "Counts miscellaneous accesses that include port i/o, MMIO and uncacheable memory accesses",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0040",
+	.desc = "Counts all prefetch (that bring data to L2) code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0200",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.split_lock_uc_lock.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10400",
+	.desc = "Counts requests where the address of an atomic lock instruction spans a cache line boundary or the lock instruction is executed on uncacheable address",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10800",
+	.desc = "Counts non-temporal stores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010008",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010001",
+	.desc = "Counts all demand data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010002",
+	.desc = "Counts all demand rfo's",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010004",
+	.desc = "Counts all demand code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010008",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x000105B3",
+	.desc = "Counts all demand & prefetch data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010122",
+	.desc = "Counts all demand & prefetch prefetch RFOs",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x000107F7",
+	.desc = "Counts all data/code/rfo references (demand & prefetch)",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_sandybridge[] = {
+{
+	.name = "other_assists.avx_store",
+	.event = "event=0xC1,umask=0x8,period=100003",
+	.desc = "Number of GSSE memory assist for stores. GSSE microcode assist is being invoked whenever the hardware is unable to properly handle GSSE-256b operations",
+	.topic = "floating point",
+},
+{
+	.name = "other_assists.avx_to_sse",
+	.event = "event=0xC1,umask=0x10,period=100003",
+	.desc = "Number of transitions from AVX-256 to legacy SSE when penalty applicable",
+	.topic = "floating point",
+},
+{
+	.name = "other_assists.sse_to_avx",
+	.event = "event=0xC1,umask=0x20,period=100003",
+	.desc = "Number of transitions from SSE to AVX-256 when penalty applicable",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.x87_output",
+	.event = "event=0xCA,umask=0x2,period=100003",
+	.desc = "Number of X87 assists due to output value",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.x87_input",
+	.event = "event=0xCA,umask=0x4,period=100003",
+	.desc = "Number of X87 assists due to input value",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.simd_output",
+	.event = "event=0xCA,umask=0x8,period=100003",
+	.desc = "Number of SIMD FP assists due to Output values",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.simd_input",
+	.event = "event=0xCA,umask=0x10,period=100003",
+	.desc = "Number of SIMD FP assists due to input values",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.x87",
+	.event = "event=0x10,umask=0x1,period=2000003",
+	.desc = "Number of FP Computational Uops Executed this cycle. The number of FADD, FSUB, FCOM, FMULs, integer MULsand IMULs, FDIVs, FPREMs, FSQRTS, integer DIVs, and IDIVs. This event does not distinguish an FADD used in the middle of a transcendental flow from a s",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_packed_double",
+	.event = "event=0x10,umask=0x10,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational packed double-precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_scalar_single",
+	.event = "event=0x10,umask=0x20,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational scalar single-precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_packed_single",
+	.event = "event=0x10,umask=0x40,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational packed single-precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_scalar_double",
+	.event = "event=0x10,umask=0x80,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational scalar double-precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "simd_fp_256.packed_single",
+	.event = "event=0x11,umask=0x1,period=2000003",
+	.desc = "Number of GSSE-256 Computational FP single precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "simd_fp_256.packed_double",
+	.event = "event=0x11,umask=0x2,period=2000003",
+	.desc = "Number of AVX-256 Computational FP double precision uops issued this cycle",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.any",
+	.event = "event=0xCA,umask=0x1e,period=100003,cmask=1",
+	.desc = "Cycles with any input/output SSE or FP assist",
+	.topic = "floating point",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired from execution",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions retired from execution. For instructions that consist of multiple micro-ops, this event counts the retirement of the last micro-op of the instruction. Counting continues during hardware interrupts, traps, and inside interrupt handlers",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Core cycles when the thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of core cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. This event is a component in many key event ratios. The core frequency may change from time to time due to transitions associated with Enhanced Intel SpeedStep Technology or TM2. For this reason this event may have a changing ratio with regards to time. When the core frequency is constant, this event can approximate elapsed time while the core was not in the halt state. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Reference cycles when the core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of reference cycles when the core is not in a halt state. The core enters the halt state when it is running the HLT instruction or the MWAIT instruction. This event is not affected by core frequency changes (for example, P states, TM2 transitions) but has the same incrementing frequency as the time stamp counter. This event can approximate elapsed time while the core was not in a halt state. This event has a constant ratio with the CPU_CLK_UNHALTED.REF_XCLK event. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events",
+},
+{
+	.name = "br_inst_exec.nontaken_conditional",
+	.event = "event=0x88,umask=0x41,period=200003",
+	.desc = "Not taken macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_conditional",
+	.event = "event=0x88,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_direct_jump",
+	.event = "event=0x88,umask=0x82,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branch instructions excluding calls and indirects",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_return",
+	.event = "event=0x88,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired indirect branches with return mnemonic",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_direct_near_call",
+	.event = "event=0x88,umask=0x90,period=200003",
+	.desc = "Taken speculative and retired direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_call",
+	.event = "event=0x88,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_conditional",
+	.event = "event=0x88,umask=0xc1,period=200003",
+	.desc = "Speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_direct_jmp",
+	.event = "event=0x88,umask=0xc2,period=200003",
+	.desc = "Speculative and retired macro-unconditional branches excluding calls and indirects",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0xc4,period=200003",
+	.desc = "Speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_indirect_near_return",
+	.event = "event=0x88,umask=0xc8,period=200003",
+	.desc = "Speculative and retired indirect return branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_direct_near_call",
+	.event = "event=0x88,umask=0xd0,period=200003",
+	.desc = "Speculative and retired direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.nontaken_conditional",
+	.event = "event=0x89,umask=0x41,period=200003",
+	.desc = "Not taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_conditional",
+	.event = "event=0x89,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_return_near",
+	.event = "event=0x89,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches with return mnemonic",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_direct_near_call",
+	.event = "event=0x89,umask=0x90,period=200003",
+	.desc = "Taken speculative and retired mispredicted direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_indirect_near_call",
+	.event = "event=0x89,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_conditional",
+	.event = "event=0x89,umask=0xc1,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0xc4,period=200003",
+	.desc = "Mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_direct_near_call",
+	.event = "event=0x89,umask=0xd0,period=200003",
+	.desc = "Speculative and retired mispredicted direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Thread cycles when thread is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.uops",
+	.event = "event=0xA8,umask=0x1,period=2000003",
+	.desc = "Number of Uops delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_active",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000003",
+	.desc = "Stalls caused by changing prefix length of the instruction",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.iq_full",
+	.event = "event=0x87,umask=0x4,period=2000003",
+	.desc = "Stall cycles because IQ is full",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.rat_stall_cycles",
+	.event = "event=0x0D,umask=0x40,period=2000003",
+	.desc = "Cycles when Resource Allocation Table (RAT) external stall is sent to Instruction Decode Queue (IDQ) for the thread",
+	.topic = "pipeline",
+},
+{
+	.name = "partial_rat_stalls.flags_merge_uop",
+	.event = "event=0x59,umask=0x20,period=2000003",
+	.desc = "Increments the number of flags-merge uops in flight each cycle",
+	.topic = "pipeline",
+},
+{
+	.name = "partial_rat_stalls.slow_lea_window",
+	.event = "event=0x59,umask=0x40,period=2000003",
+	.desc = "Cycles with at least one slow LEA uop being allocated",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of cycles with at least one slow LEA uop being allocated. A uop is generally considered as slow LEA if it has three sources (for example, two sources and immediate) regardless of whether it is a result of LEA instruction or not. Examples of the slow LEA uop are or uops with base, index, and offset source operands using base and index reqisters, where base is EBR/RBP/R13, using RIP relative or 16-bit addressing modes. See the Intel? 64 and IA-32 Architectures Optimization Reference Manual for more details about slow LEA instructions",
+},
+{
+	.name = "partial_rat_stalls.mul_single_uop",
+	.event = "event=0x59,umask=0x80,period=2000003",
+	.desc = "Multiply packed/scalar single precision uops allocated",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000003",
+	.desc = "Resource-related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.lb",
+	.event = "event=0xA2,umask=0x2,period=2000003",
+	.desc = "Counts the cycles of stall due to lack of load buffers",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rs",
+	.event = "event=0xA2,umask=0x4,period=2000003",
+	.desc = "Cycles stalled due to no eligible RS entry available",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.sb",
+	.event = "event=0xA2,umask=0x8,period=2000003",
+	.desc = "Cycles stalled due to no store buffers available. (not including draining form sync)",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rob",
+	.event = "event=0xA2,umask=0x10,period=2000003",
+	.desc = "Cycles stalled due to re-order buffer full",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls2.bob_full",
+	.event = "event=0x5B,umask=0x40,period=2000003",
+	.desc = "Cycles when Allocator is stalled if BOB is full and new branch needs it",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0x0E,umask=0x1,period=2000003",
+	.desc = "Uops that Resource Allocation Table (RAT) issues to Reservation Station (RS)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of Uops issued by the front-end of the pipeilne to the back-end",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.core_stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for all threads",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_cycles",
+	.event = "event=0x5E,umask=0x1,period=2000003",
+	.desc = "Cycles when Reservation Station (RS) is empty for the thread",
+	.topic = "pipeline",
+},
+{
+	.name = "rob_misc_events.lbr_inserts",
+	.event = "event=0xCC,umask=0x20,period=2000003",
+	.desc = "Count cases of saving new LBR",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=100003",
+	.desc = "Self-modifying code (SMC) detected",
+	.topic = "pipeline",
+	.long_desc = "This event is incremented when self-modifying code (SMC) is detected, which causes a machine clear.  Machine clears can have a significant performance impact if they are happening frequently",
+},
+{
+	.name = "machine_clears.maskmov",
+	.event = "event=0xC3,umask=0x20,period=100003",
+	.desc = "This event counts the number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0",
+	.topic = "pipeline",
+	.long_desc = "Maskmov false fault - counts number of time ucode passes through Maskmov flow due to instruction's mask being 0 while the flow was completed without raising a fault",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Number of instructions retired. General Counter   - architectural event",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.all",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "Actually retired uops (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of micro-ops retired (Precise event)",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000003",
+	.desc = "Retirement slots used (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of retirement slots used each cycle.  There are potentially 4 slots that can be used each cycle - meaning, 4 micro-ops or 4 instructions could retire each cycle.  This event is used in determining the 'Retiring' category of the Top-Down pipeline slots characterization (Precise event)",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=10",
+	.desc = "Cycles with less than 10 actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=400009",
+	.desc = "Conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=100007",
+	.desc = "Direct and indirect near call instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=400009",
+	.desc = "All (macro) branch instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.near_return",
+	.event = "event=0xC4,umask=0x8,period=100007",
+	.desc = "Return instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.not_taken",
+	.event = "event=0xC4,umask=0x10,period=400009",
+	.desc = "Not taken branch instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.near_taken",
+	.event = "event=0xC4,umask=0x20,period=400009",
+	.desc = "Taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0x40,period=100007",
+	.desc = "Far branch instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.all_branches_pebs",
+	.event = "event=0xC4,umask=0x4,period=400009",
+	.desc = "All (macro) branch instructions retired. (Precise Event - PEBS) (Must be precise)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=400009",
+	.desc = "Mispredicted conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_call",
+	.event = "event=0xC5,umask=0x2,period=100007",
+	.desc = "Direct and indirect mispredicted near call instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=400009",
+	.desc = "All mispredicted macro branch instructions retired",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.not_taken",
+	.event = "event=0xC5,umask=0x10,period=400009",
+	.desc = "Mispredicted not taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.taken",
+	.event = "event=0xC5,umask=0x20,period=400009",
+	.desc = "Mispredicted taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.all_branches_pebs",
+	.event = "event=0xC5,umask=0x4,period=400009",
+	.desc = "Mispredicted macro branch instructions retired. (Precise Event - PEBS) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted macro branch instructions retired. (Precise Event - PEBS) (Must be precise)",
+},
+{
+	.name = "other_assists.itlb_miss_retired",
+	.event = "event=0xC1,umask=0x2,period=100003",
+	.desc = "Retired instructions experiencing ITLB misses",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.fpu_div_active",
+	.event = "event=0x14,umask=0x1,period=2000003",
+	.desc = "Cycles when divider is busy executing divide operations",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.fpu_div",
+	.event = "event=0x14,umask=0x1,edge=1,period=100003,cmask=1",
+	.desc = "Divide operations executed",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of the divide operations executed",
+},
+{
+	.name = "uops_dispatched.thread",
+	.event = "event=0xB1,umask=0x1,period=2000003",
+	.desc = "Uops dispatched per thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched.core",
+	.event = "event=0xB1,umask=0x2,period=2000003",
+	.desc = "Uops dispatched from any thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_4",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_5",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_no_dispatch",
+	.event = "event=0xA3,umask=0x4,period=2000003,cmask=4",
+	.desc = "Each cycle there was no dispatch for this thread, increment by 1. Note this is connect to Umask 2. No dispatch can be deduced from the UOPS_EXECUTED event",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l1d_pending",
+	.event = "event=0xA3,umask=0x2,period=2000003,cmask=2",
+	.desc = "Each cycle there was a miss-pending demand load this thread, increment by 1. Note this is in DCU and connected to Umask 1. Miss Pending demand load should be deduced by OR-ing increment bits of DCACHE_MISS_PEND.PENDING",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l2_pending",
+	.event = "event=0xA3,umask=0x1,period=2000003,cmask=1",
+	.desc = "Each cycle there was a MLC-miss pending demand load this thread (i.e. Non-completed valid SQ entry allocated for demand load and waiting for Uncore), increment by 1. Note this is in MLC and connected to Umask 0",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l1d_pending",
+	.event = "event=0xA3,umask=0x6,period=2000003,cmask=6",
+	.desc = "Each cycle there was a miss-pending demand load this thread and no uops dispatched, increment by 1. Note this is in DCU and connected to Umask 1 and 2. Miss Pending demand load should be deduced by OR-ing increment bits of DCACHE_MISS_PEND.PENDING",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l2_pending",
+	.event = "event=0xA3,umask=0x5,period=2000003,cmask=5",
+	.desc = "Each cycle there was a MLC-miss pending demand load and no uops dispatched on this thread (i.e. Non-completed valid SQ entry allocated for demand load and waiting for Uncore), increment by 1. Note this is in MLC and connected to Umask 0 and 2",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.sw_pf",
+	.event = "event=0x4C,umask=0x1,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for software prefetch",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.hw_pf",
+	.event = "event=0x4C,umask=0x2,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for hardware prefetch",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks.data_unknown",
+	.event = "event=0x03,umask=0x1,period=100003",
+	.desc = "Loads delayed due to SB blocks, preceding store operations with known addresses but unknown data",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks.store_forward",
+	.event = "event=0x03,umask=0x2,period=100003",
+	.desc = "Cases when loads get true Block-on-Store blocking code preventing store forwarding",
+	.topic = "pipeline",
+	.long_desc = "This event counts loads that followed a store to the same address, where the data could not be forwarded inside the pipeline from the store to the load.  The most common reason why store forwarding would be blocked is when a load's address range overlaps with a preceeding smaller uncompleted store.  See the table of not supported store forwards in the Intel? 64 and IA-32 Architectures Optimization Reference Manual.  The penalty for blocked store forwarding is that the load must wait for the store to complete before it can be issued",
+},
+{
+	.name = "ld_blocks.no_sr",
+	.event = "event=0x03,umask=0x8,period=100003",
+	.desc = "This event counts the number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks.all_block",
+	.event = "event=0x03,umask=0x10,period=100003",
+	.desc = "Number of cases where any load ends up with a valid block-code written to the load buffer (including blocks due to Memory Order Buffer (MOB), Data Cache Unit (DCU), TLB, but load has no DCU miss)",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks_partial.address_alias",
+	.event = "event=0x07,umask=0x1,period=100003",
+	.desc = "False dependencies in MOB due to partial compare",
+	.topic = "pipeline",
+	.long_desc = "Aliasing occurs when a load is issued after a store and their memory addresses are offset by 4K.  This event counts the number of loads that aliased with a preceding store, resulting in an extended address check in the pipeline.  The enhanced address check typically has a performance penalty of 5 cycles",
+},
+{
+	.name = "ld_blocks_partial.all_sta_block",
+	.event = "event=0x07,umask=0x8,period=100003",
+	.desc = "This event counts the number of times that load operations are temporarily blocked because of older stores, with addresses that are not yet known. A load operation may incur more than one block of this type",
+	.topic = "pipeline",
+},
+{
+	.name = "agu_bypass_cancel.count",
+	.event = "event=0xB6,umask=0x1,period=100003",
+	.desc = "This event counts executed load operations with all the following traits: 1. addressing of the format [base + offset], 2. the offset is between 1 and 2047, 3. the address specified in the base register is in one page and the address [base+offset] is in an",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_0_core",
+	.event = "event=0xA1,umask=0x1,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_1_core",
+	.event = "event=0xA1,umask=0x2,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_4_core",
+	.event = "event=0xA1,umask=0x40,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_5_core",
+	.event = "event=0xA1,umask=0x80,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_2",
+	.event = "event=0xA1,umask=0xc,period=2000003",
+	.desc = "Cycles per thread when load or STA uops are dispatched to port 2",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_3",
+	.event = "event=0xA1,umask=0x30,period=2000003",
+	.desc = "Cycles per thread when load or STA uops are dispatched to port 3",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_2_core",
+	.event = "event=0xA1,umask=0xc,any=1,period=2000003",
+	.desc = "Cycles per core when load or STA uops are dispatched to port 2",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_3_core",
+	.event = "event=0xA1,umask=0x30,any=1,period=2000003",
+	.desc = "Cycles per core when load or STA uops are dispatched to port 3",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.prec_dist",
+	.event = "event=0xC0,umask=0x1,period=2000003",
+	.desc = "Instructions retired. (Precise Event - PEBS) (Must be precise)",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls2.all_prf_control",
+	.event = "event=0x5B,umask=0xf,period=2000003",
+	.desc = "Resource stalls2 control structures full for physical registers",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls2.all_fl_empty",
+	.event = "event=0x5B,umask=0xc,period=2000003",
+	.desc = "Cycles with either free list is empty",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.mem_rs",
+	.event = "event=0xA2,umask=0xe,period=2000003",
+	.desc = "Resource stalls due to memory buffers or Reservation Station (RS) being fully utilized",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.ooo_rsrc",
+	.event = "event=0xA2,umask=0xf0,period=2000003",
+	.desc = "Resource stalls due to Rob being full, FCSW, MXCSR and OTHER",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls2.ooo_rsrc",
+	.event = "event=0x5B,umask=0x4f,period=2000003",
+	.desc = "Resource stalls out of order resources full",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.lb_sb",
+	.event = "event=0xA2,umask=0xa,period=2000003",
+	.desc = "Resource stalls due to load or store buffers all being in use",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_cycles",
+	.event = "event=0x0D,umask=0x3,period=2000003,cmask=1",
+	.desc = "Number of cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc...)",
+	.topic = "pipeline",
+},
+{
+	.name = "partial_rat_stalls.flags_merge_uop_cycles",
+	.event = "event=0x59,umask=0x20,period=2000003,cmask=1",
+	.desc = "Performance sensitive flags-merging uops added by Sandy Bridge u-arch",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of cycles spent executing performance-sensitive flags-merging uops. For example, shift CL (merge_arith_flags). For more details, See the Intel? 64 and IA-32 Architectures Optimization Reference Manual",
+},
+{
+	.name = "int_misc.recovery_stalls_count",
+	.event = "event=0x0D,umask=0x3,edge=1,period=2000003,cmask=1",
+	.desc = "Number of occurences waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc...)",
+	.topic = "pipeline",
+},
+{
+	.name = "baclears.any",
+	.event = "event=0xE6,umask=0x1f,period=100003",
+	.desc = "Counts the total number when the front end is resteered, mainly when the BPU cannot provide a correct prediction and this is corrected by other branch handling mechanisms at the front end",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_branches",
+	.event = "event=0x88,umask=0xff,period=200003",
+	.desc = "Speculative and retired  branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_branches",
+	.event = "event=0x89,umask=0xff,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.core_stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_4_uops",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.count",
+	.event = "event=0xc3,umask=0x1,edge=1,period=100003,cmask=1",
+	.desc = "Number of machine clears (nukes) of any type",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_end",
+	.event = "event=0x5E,inv=1,umask=0x1,edge=1,period=2000003,cmask=1",
+	.desc = "Counts end of periods where the Reservation Station (RS) was empty. Could be useful to precisely locate Frontend Latency Bound issues",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_any",
+	.event = "event=0x3c,any=1",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p_any",
+	.event = "event=0x3C,umask=0x0,any=1,period=2000003",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_cycles_any",
+	.event = "event=0x0D,umask=0x3,any=1,period=2000003,cmask=1",
+	.desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_1",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles at least 1 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_2",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles at least 2 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_3",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=3",
+	.desc = "Cycles at least 3 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_4",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=4",
+	.desc = "Cycles at least 4 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_none",
+	.event = "event=0xB1,inv=1,umask=0x2,period=2000003",
+	.desc = "Cycles with no micro-ops executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=2000003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Reads. both cacheable and noncacheable, including UC fetches",
+	.topic = "frontend",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "Instruction cache, streaming buffer and victim cache misses",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of instruction cache, streaming buffer and victim cache misses. Counting includes unchacheable accesses",
+},
+{
+	.name = "idq.empty",
+	.event = "event=0x79,umask=0x2,period=2000003",
+	.desc = "Instruction Decode Queue (IDQ) empty cycles",
+	.topic = "frontend",
+},
+{
+	.name = "idq.mite_uops",
+	.event = "event=0x79,umask=0x4,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.dsb_uops",
+	.event = "event=0x79,umask=0x8,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_dsb_uops",
+	.event = "event=0x79,umask=0x10,period=2000003",
+	.desc = "Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_mite_uops",
+	.event = "event=0x79,umask=0x20,period=2000003",
+	.desc = "Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_uops",
+	.event = "event=0x79,umask=0x30,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_cycles",
+	.event = "event=0x79,umask=0x30,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which the microcode sequencer assisted the front-end in delivering uops.  Microcode assists are used for complex instructions or scenarios that can't be handled by the standard decoder.  Using other instructions, if possible, will usually improve performance.  See the Intel? 64 and IA-32 Architectures Optimization Reference Manual for more information",
+},
+{
+	.name = "idq_uops_not_delivered.core",
+	.event = "event=0x9C,umask=0x1,period=2000003",
+	.desc = "Uops not delivered to Resource Allocation Table (RAT) per thread when backend of the machine is not stalled ",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops not delivered to the back-end per cycle, per thread, when the back-end was not stalled.  In the ideal case 4 uops can be delivered each cycle.  The event counts the undelivered uops - so if 3 were delivered in one cycle, the counter would be incremented by 1 for that cycle (4 - 3). If the back-end is stalled, the count for this event is not incremented even when uops were not delivered, because the back-end would not have been able to accept them.  This event is used in determining the front-end bound category of the top-down pipeline slots characterization",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_0_uops_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles per thread when 4 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_1_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles per thread when 3 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.count",
+	.event = "event=0xAB,umask=0x1,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switches",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.penalty_cycles",
+	.event = "event=0xAB,umask=0x2,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles",
+	.topic = "frontend",
+	.long_desc = "This event counts the cycles attributed to a switch from the Decoded Stream Buffer (DSB), which holds decoded instructions, to the legacy decode pipeline.  It excludes cycles when the back-end cannot  accept new micro-ops.  The penalty for these switches is potentially several cycles of instruction starvation, where no micro-ops are delivered to the back-end",
+},
+{
+	.name = "dsb_fill.other_cancel",
+	.event = "event=0xAC,umask=0x2,period=2000003",
+	.desc = "Cases of cancelling valid DSB fill not because of exceeding way limit",
+	.topic = "frontend",
+},
+{
+	.name = "dsb_fill.exceed_dsb_lines",
+	.event = "event=0xAC,umask=0x8,period=2000003",
+	.desc = "Cycles when Decode Stream Buffer (DSB) fill encounter more than 3 Decode Stream Buffer (DSB) lines",
+	.topic = "frontend",
+},
+{
+	.name = "idq.mite_cycles",
+	.event = "event=0x79,umask=0x4,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.dsb_cycles",
+	.event = "event=0x79,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_dsb_cycles",
+	.event = "event=0x79,umask=0x10,period=2000003,cmask=1",
+	.desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_dsb_occur",
+	.event = "event=0x79,umask=0x10,edge=1,period=2000003,cmask=1",
+	.desc = "Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_2_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles with less than 2 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_3_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with less than 3 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_ge_1_uop_deliv.core",
+	.event = "event=0x9C,inv=1,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles when 1 or more uops were delivered to the by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq.all_dsb_cycles_4_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=4",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering 4 Uops",
+	.topic = "frontend",
+},
+{
+	.name = "idq.all_dsb_cycles_any_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=1",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering any Uop",
+	.topic = "frontend",
+},
+{
+	.name = "idq.all_mite_cycles_4_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=4",
+	.desc = "Cycles MITE is delivering 4 Uops",
+	.topic = "frontend",
+},
+{
+	.name = "idq.all_mite_cycles_any_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=1",
+	.desc = "Cycles MITE is delivering any Uop",
+	.topic = "frontend",
+},
+{
+	.name = "dsb_fill.all_cancel",
+	.event = "event=0xAC,umask=0xa,period=2000003",
+	.desc = "Cases of cancelling valid Decode Stream Buffer (DSB) fill not because of exceeding way limit",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_fe_was_ok",
+	.event = "event=0x9C,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts cycles FE delivered 4 uops or Resource Allocation Table (RAT) was stalling FE",
+	.topic = "frontend",
+},
+{
+	.name = "idq.mite_all_uops",
+	.event = "event=0x79,umask=0x3c,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_switches",
+	.event = "event=0x79,umask=0x30,edge=1,period=2000003,cmask=1",
+	.desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+	.topic = "frontend",
+},
+{
+	.name = "insts_written_to_iq.insts",
+	.event = "event=0x17,umask=0x1,period=2000003",
+	.desc = "Valid instructions written to IQ per cycle",
+	.topic = "other",
+},
+{
+	.name = "cpl_cycles.ring0",
+	.event = "event=0x5C,umask=0x1,period=2000003",
+	.desc = "Unhalted core cycles when the thread is in ring 0",
+	.topic = "other",
+},
+{
+	.name = "cpl_cycles.ring0_trans",
+	.event = "event=0x5C,umask=0x1,edge=1,period=100007,cmask=1",
+	.desc = "Number of intervals between processor halts while thread is in ring 0",
+	.topic = "other",
+},
+{
+	.name = "cpl_cycles.ring123",
+	.event = "event=0x5C,umask=0x2,period=2000003",
+	.desc = "Unhalted core cycles when thread is in rings 1, 2, or 3",
+	.topic = "other",
+},
+{
+	.name = "hw_pre_req.dl1_miss",
+	.event = "event=0x4E,umask=0x2,period=2000003",
+	.desc = "Hardware Prefetch requests that miss the L1D cache. This accounts for both L1 streamer and IP-based (IPP) HW prefetchers. A request is being counted each time it access the cache & miss it, including if a block is applicable or if hit the Fill Buffer for ",
+	.topic = "other",
+},
+{
+	.name = "lock_cycles.split_lock_uc_lock_duration",
+	.event = "event=0x63,umask=0x1,period=2000003",
+	.desc = "Cycles when L1 and L2 are locked due to UC or split lock",
+	.topic = "other",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=100003",
+	.desc = "Counts the number of machine clears due to memory order conflicts",
+	.topic = "memory",
+	.long_desc = "This event counts the number of memory ordering Machine Clears detected. Memory Ordering Machine Clears can result from memory disambiguation, external snoops, or cross SMT-HW-thread snoop (stores) hitting load buffers.  Machine clears can have a significant performance impact if they are happening frequently",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_4",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x4",
+	.desc = "Loads with latency value being above 4  (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_8",
+	.event = "event=0xCD,umask=0x1,period=50021,ldlat=0x8",
+	.desc = "Loads with latency value being above 8 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_16",
+	.event = "event=0xCD,umask=0x1,period=20011,ldlat=0x10",
+	.desc = "Loads with latency value being above 16 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_32",
+	.event = "event=0xCD,umask=0x1,period=100007,ldlat=0x20",
+	.desc = "Loads with latency value being above 32 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_64",
+	.event = "event=0xCD,umask=0x1,period=2003,ldlat=0x40",
+	.desc = "Loads with latency value being above 64 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_128",
+	.event = "event=0xCD,umask=0x1,period=1009,ldlat=0x80",
+	.desc = "Loads with latency value being above 128 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_256",
+	.event = "event=0xCD,umask=0x1,period=503,ldlat=0x100",
+	.desc = "Loads with latency value being above 256 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_512",
+	.event = "event=0xCD,umask=0x1,period=101,ldlat=0x200",
+	.desc = "Loads with latency value being above 512 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.precise_store",
+	.event = "event=0xCD,umask=0x2,period=2000003",
+	.desc = "Sample stores and collect precise store operation via PEBS record. PMC3 only. (Precise Event - PEBS) (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "page_walks.llc_miss",
+	.event = "event=0xBE,umask=0x1,period=100003",
+	.desc = "Number of any page walk that had a miss in LLC. Does not necessary cause a SUSPEND",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.loads",
+	.event = "event=0x05,umask=0x1,period=2000003",
+	.desc = "Speculative cache line split load uops dispatched to L1 cache",
+	.topic = "memory",
+},
+{
+	.name = "misalign_mem_ref.stores",
+	.event = "event=0x05,umask=0x2,period=2000003",
+	.desc = "Speculative cache line split STA uops dispatched to L1 cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400244",
+	.desc = "Counts all demand & prefetch code reads that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400091",
+	.desc = "Counts all demand & prefetch data reads that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400240",
+	.desc = "Counts all prefetch code reads that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400090",
+	.desc = "Counts all prefetch data reads that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_pf_rfo.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400120",
+	.desc = "Counts all prefetch RFOs that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3004003f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400122",
+	.desc = "Counts all demand & prefetch RFOs that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400004",
+	.desc = "Counts demand code reads that miss the LLC and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400001",
+	.desc = "Counts demand data reads that miss the LLC and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400002",
+	.desc = "Counts demand data writes (RFOs) that miss the LLC and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400040",
+	.desc = "Counts all prefetch (that bring data to L2) code reads that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss the LLC and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400020",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400200",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400080",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_rfo.llc_miss.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x300400100",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that miss the LLC  and the data returned from dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in_socket.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x6004001b3",
+	.desc = "Counts LLC replacements",
+	.topic = "memory",
+	.long_desc = "This event counts all data requests (demand/prefetch data reads and demand data writes (RFOs) that miss the LLC  where the data is returned from local DRAM",
+},
+{
+	.name = "offcore_response.any_request.llc_miss_local.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1f80408fff",
+	.desc = " REQUEST = ANY_REQUEST and RESPONSE = LLC_MISS_LOCAL and SNOOP = DRAM",
+	.topic = "memory",
+	.long_desc = "This event counts any requests that miss the LLC where the data was returned from local DRAM",
+},
+{
+	.name = "offcore_response.data_in_socket.llc_miss_local.any_llc_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x17004001b3",
+	.desc = " REQUEST = DATA_IN_SOCKET and RESPONSE = LLC_MISS_LOCAL and SNOOP = ANY_LLC_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_miss_local.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1f80400004",
+	.desc = " REQUEST = DEMAND_IFETCH and RESPONSE = LLC_MISS_LOCAL and SNOOP = DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_miss_local.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1f80400010",
+	.desc = " REQUEST = PF_DATA_RD and RESPONSE = LLC_MISS_LOCAL and SNOOP = DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_miss_local.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1f80400040",
+	.desc = " REQUEST = PF_RFO and RESPONSE = LLC_MISS_LOCAL and SNOOP = DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l_data_rd.llc_miss_local.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1f80400080",
+	.desc = " REQUEST = PF_LLC_DATA_RD and RESPONSE = LLC_MISS_LOCAL and SNOOP = DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l_ifetch.llc_miss_local.dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1f80400200",
+	.desc = " REQUEST = PF_LLC_IFETCH and RESPONSE = LLC_MISS_LOCAL and SNOOP = DRAM",
+	.topic = "memory",
+},
+{
+	.name = "itlb.itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=100007",
+	.desc = "Flushing of the Instruction TLB (ITLB) pages, includes 4k/2M/4M pages",
+	.topic = "virtual memory",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4F,umask=0x10,period=2000003",
+	.desc = "Cycle count for an Extended Page table walk.  The Extended Page Directory cache is used by Virtual Machine operating systems while the guest operating systems use the standard TLB caches",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.miss_causes_a_walk",
+	.event = "event=0x85,umask=0x1,period=100003",
+	.desc = "Misses at all ITLB levels that cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0x2,period=100003",
+	.desc = "Misses in all ITLB levels that cause completed page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_duration",
+	.event = "event=0x85,umask=0x4,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event count cycles when Page Miss Handler (PMH) is servicing page walks caused by ITLB misses",
+},
+{
+	.name = "itlb_misses.stlb_hit",
+	.event = "event=0x85,umask=0x10,period=100003",
+	.desc = "Operations that miss the first ITLB level but hit the second and do not cause any page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.miss_causes_a_walk",
+	.event = "event=0x08,umask=0x1,period=100003",
+	.desc = "Load misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x08,umask=0x2,period=100003",
+	.desc = "Load misses at all DTLB levels that cause completed page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_duration",
+	.event = "event=0x08,umask=0x4,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts cycles when the  page miss handler (PMH) is servicing page walks caused by DTLB load misses",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x08,umask=0x10,period=100003",
+	.desc = "Load operations that miss the first DTLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load operations that miss the first DTLB level but hit the second and do not cause any page walks. The penalty in this case is approximately 7 cycles",
+},
+{
+	.name = "dtlb_store_misses.miss_causes_a_walk",
+	.event = "event=0x49,umask=0x1,period=100003",
+	.desc = "Store misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.walk_completed",
+	.event = "event=0x49,umask=0x2,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.walk_duration",
+	.event = "event=0x49,umask=0x4,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit",
+	.event = "event=0x49,umask=0x10,period=100003",
+	.desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "tlb_flush.dtlb_thread",
+	.event = "event=0xBD,umask=0x1,period=100007",
+	.desc = "DTLB flush attempts of the thread-specific entries",
+	.topic = "virtual memory",
+},
+{
+	.name = "tlb_flush.stlb_any",
+	.event = "event=0xBD,umask=0x20,period=100007",
+	.desc = "STLB flush attempts",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_loads",
+	.event = "event=0xD0,umask=0x11,period=100003",
+	.desc = "Retired load uops that miss the STLB (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_stores",
+	.event = "event=0xD0,umask=0x12,period=100003",
+	.desc = "Retired store uops that miss the STLB (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.lock_loads",
+	.event = "event=0xD0,umask=0x21,period=100007",
+	.desc = "Retired load uops with locked access (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.split_loads",
+	.event = "event=0xD0,umask=0x41,period=100003",
+	.desc = "Retired load uops that split across a cacheline boundary (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts line-splitted load uops retired to the architected path. A line split is across 64B cache-line which includes a page split (4K) (Precise event)",
+},
+{
+	.name = "mem_uops_retired.split_stores",
+	.event = "event=0xD0,umask=0x42,period=100003",
+	.desc = "Retired store uops that split across a cacheline boundary (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts line-splitted store uops retired to the architected path. A line split is across 64B cache-line which includes a page split (4K) (Precise event)",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0xD0,umask=0x81,period=2000003",
+	.desc = "All retired load uops (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts the number of load uops retired (Precise event)",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0xD0,umask=0x82,period=2000003",
+	.desc = "All retired store uops (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts the number of store uops retired (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_hit",
+	.event = "event=0xD1,umask=0x1,period=2000003",
+	.desc = "Retired load uops with L1 cache hits as data sources (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.l2_hit",
+	.event = "event=0xD1,umask=0x2,period=100003",
+	.desc = "Retired load uops with L2 cache hits as data sources (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.llc_hit",
+	.event = "event=0xD1,umask=0x4,period=50021",
+	.desc = "Retired load uops which data sources were data hits in LLC without snoops required (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts retired load uops that hit in the last-level (L3) cache without snoops required (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.hit_lfb",
+	.event = "event=0xD1,umask=0x40,period=100003",
+	.desc = "Retired load uops which data sources were load uops missed L1 but hit FB due to preceding miss to the same cache line with data not ready (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_miss",
+	.event = "event=0xD2,umask=0x1,period=20011",
+	.desc = "Retired load uops which data sources were LLC hit and cross-core snoop missed in on-pkg core cache (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_hit",
+	.event = "event=0xD2,umask=0x2,period=20011",
+	.desc = "Retired load uops which data sources were LLC and cross-core snoop hits in on-pkg core cache (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts retired load uops that hit in the last-level cache (L3) and were found in a non-modified state in a neighboring core's private cache (same package).  Since the last level cache is inclusive, hits to the L3 may require snooping the private L2 caches of any cores on the same socket that have the line.  In this case, a snoop was required, and another L2 had the line in a non-modified state (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_hitm",
+	.event = "event=0xD2,umask=0x4,period=20011",
+	.desc = "Retired load uops which data sources were HitM responses from shared LLC (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts retired load uops that hit in the last-level cache (L3) and were found in a non-modified state in a neighboring core's private cache (same package).  Since the last level cache is inclusive, hits to the L3 may require snooping the private L2 caches of any cores on the same socket that have the line.  In this case, a snoop was required, and another L2 had the line in a modified state, so the line had to be invalidated in that L2 cache and transferred to the requesting L2 (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_none",
+	.event = "event=0xD2,umask=0x8,period=100003",
+	.desc = "Retired load uops which data sources were hits in LLC without snoops required (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_misc_retired.llc_miss",
+	.event = "event=0xD4,umask=0x2,period=100007",
+	.desc = "Retired load uops with unknown information as data source in cache serviced the load (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts retired demand loads that missed the  last-level (L3) cache. This means that the load is usually satisfied from memory in a client system or possibly from the remote socket in a server. Demand loads are non speculative load uops (Precise event)",
+},
+{
+	.name = "l1d.replacement",
+	.event = "event=0x51,umask=0x1,period=2000003",
+	.desc = "L1D data line replacements",
+	.topic = "cache",
+	.long_desc = "This event counts L1D data line replacements.  Replacements occur when a new line is brought into the cache, causing eviction of a line loaded earlier",
+},
+{
+	.name = "l1d.allocated_in_m",
+	.event = "event=0x51,umask=0x2,period=2000003",
+	.desc = "Allocated L1D data cache lines in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.eviction",
+	.event = "event=0x51,umask=0x4,period=2000003",
+	.desc = "L1D data cache lines in M state evicted due to replacement",
+	.topic = "cache",
+},
+{
+	.name = "l1d.all_m_replacement",
+	.event = "event=0x51,umask=0x8,period=2000003",
+	.desc = "Cache lines in M state evicted out of L1D due to Snoop HitM or dirty line replacement",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending",
+	.event = "event=0x48,umask=0x1,period=2000003",
+	.desc = "L1D miss oustandings duration in cycles",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles",
+	.event = "event=0x48,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding",
+	.topic = "cache",
+},
+{
+	.name = "lock_cycles.cache_lock_duration",
+	.event = "event=0x63,umask=0x2,period=2000003",
+	.desc = "Cycles when L1D is locked",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003",
+	.desc = "Offcore outstanding Demand Data Read transactions in uncore queue",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003",
+	.desc = "Offcore outstanding RFO store transactions in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.all_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003",
+	.desc = "Offcore outstanding cacheable Core Data Read transactions in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand_data_rd",
+	.event = "event=0xB0,umask=0x1,period=100003",
+	.desc = "Demand Data Read requests sent to uncore",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand_code_rd",
+	.event = "event=0xB0,umask=0x2,period=100003",
+	.desc = "Cacheable and noncachaeble code read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.demand_rfo",
+	.event = "event=0xB0,umask=0x4,period=100003",
+	.desc = "Demand RFO requests including regular RFOs, locks, ItoM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.all_data_rd",
+	.event = "event=0xB0,umask=0x8,period=100003",
+	.desc = "Demand and prefetch data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_buffer.sq_full",
+	.event = "event=0xB2,umask=0x1,period=2000003",
+	.desc = "Cases when offcore requests buffer cannot take more entries for core",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_hit",
+	.event = "event=0x24,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that hit L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x4,period=200003",
+	.desc = "RFO requests that hit L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x8,period=200003",
+	.desc = "RFO requests that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.code_rd_hit",
+	.event = "event=0x24,umask=0x10,period=200003",
+	.desc = "L2 cache hits when fetching instructions, code reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.code_rd_miss",
+	.event = "event=0x24,umask=0x20,period=200003",
+	.desc = "L2 cache misses when fetching instructions",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.pf_hit",
+	.event = "event=0x24,umask=0x40,period=200003",
+	.desc = "Requests from the L2 hardware prefetchers that hit L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.pf_miss",
+	.event = "event=0x24,umask=0x80,period=200003",
+	.desc = "Requests from the L2 hardware prefetchers that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_store_lock_rqsts.miss",
+	.event = "event=0x27,umask=0x1,period=200003",
+	.desc = "RFOs that miss cache lines",
+	.topic = "cache",
+},
+{
+	.name = "l2_store_lock_rqsts.hit_e",
+	.event = "event=0x27,umask=0x4,period=200003",
+	.desc = "RFOs that hit cache lines in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_store_lock_rqsts.hit_m",
+	.event = "event=0x27,umask=0x8,period=200003",
+	.desc = "RFOs that hit cache lines in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_store_lock_rqsts.all",
+	.event = "event=0x27,umask=0xf,period=200003",
+	.desc = "RFOs that access cache lines in any state",
+	.topic = "cache",
+},
+{
+	.name = "l2_l1d_wb_rqsts.miss",
+	.event = "event=0x28,umask=0x1,period=200003",
+	.desc = "Count the number of modified Lines evicted from L1 and missed L2. (Non-rejected WBs from the DCU.)",
+	.topic = "cache",
+},
+{
+	.name = "l2_l1d_wb_rqsts.hit_s",
+	.event = "event=0x28,umask=0x2,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_l1d_wb_rqsts.hit_e",
+	.event = "event=0x28,umask=0x4,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_l1d_wb_rqsts.hit_m",
+	.event = "event=0x28,umask=0x8,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_l1d_wb_rqsts.all",
+	.event = "event=0x28,umask=0xf,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in any state",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.demand_data_rd",
+	.event = "event=0xF0,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.rfo",
+	.event = "event=0xF0,umask=0x2,period=200003",
+	.desc = "RFO requests that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.code_rd",
+	.event = "event=0xF0,umask=0x4,period=200003",
+	.desc = "L2 cache accesses when fetching instructions",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.all_pf",
+	.event = "event=0xF0,umask=0x8,period=200003",
+	.desc = "L2 or LLC HW prefetches that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200003",
+	.desc = "L1D writebacks that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.l2_fill",
+	.event = "event=0xF0,umask=0x20,period=200003",
+	.desc = "L2 fill requests that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.l2_wb",
+	.event = "event=0xF0,umask=0x40,period=200003",
+	.desc = "L2 writebacks that access L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.all_requests",
+	.event = "event=0xF0,umask=0x80,period=200003",
+	.desc = "Transactions accessing L2 pipe",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.i",
+	.event = "event=0xF1,umask=0x1,period=100003",
+	.desc = "L2 cache lines in I state filling L2",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.s",
+	.event = "event=0xF1,umask=0x2,period=100003",
+	.desc = "L2 cache lines in S state filling L2",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.e",
+	.event = "event=0xF1,umask=0x4,period=100003",
+	.desc = "L2 cache lines in E state filling L2",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.all",
+	.event = "event=0xF1,umask=0x7,period=100003",
+	.desc = "L2 cache lines filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines brought into the L2 cache.  Lines are filled into the L2 cache when there was an L2 miss",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x1,period=100003",
+	.desc = "Clean L2 cache lines evicted by demand",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_dirty",
+	.event = "event=0xF2,umask=0x2,period=100003",
+	.desc = "Dirty L2 cache lines evicted by demand",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.pf_clean",
+	.event = "event=0xF2,umask=0x4,period=100003",
+	.desc = "Clean L2 cache lines evicted by L2 prefetch",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.pf_dirty",
+	.event = "event=0xF2,umask=0x8,period=100003",
+	.desc = "Dirty L2 cache lines evicted by L2 prefetch",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.dirty_all",
+	.event = "event=0xF2,umask=0xa,period=100003",
+	.desc = "Dirty L2 cache lines filling the L2",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100003",
+	.desc = "Core-originated cacheable demand requests missed LLC",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=100003",
+	.desc = "Core-originated cacheable demand requests that refer to LLC",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xF4,umask=0x10,period=100003",
+	.desc = "Split locks in SQ",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_demand_data_rd",
+	.event = "event=0x24,umask=0x3,period=200003",
+	.desc = "Demand Data Read requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_rfo",
+	.event = "event=0x24,umask=0xc,period=200003",
+	.desc = "RFO requests to L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_code_rd",
+	.event = "event=0x24,umask=0x30,period=200003",
+	.desc = "L2 code requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_pf",
+	.event = "event=0x24,umask=0xc0,period=200003",
+	.desc = "Requests from L2 hardware prefetchers",
+	.topic = "cache",
+},
+{
+	.name = "l1d_blocks.bank_conflict_cycles",
+	.event = "event=0xBF,umask=0x5,period=100003,cmask=1",
+	.desc = "Cycles when dispatched loads are cancelled due to L1D bank conflicts with other load ports",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003,cmask=1",
+	.desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd_c6",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=6",
+	.desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles_any",
+	.event = "event=0x48,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.fb_full",
+	.event = "event=0x48,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles a demand request was blocked due to Fill Buffers inavailability",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0244",
+	.desc = "Counts demand & prefetch code reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0244",
+	.desc = "Counts demand & prefetch code reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0244",
+	.desc = "Counts demand & prefetch code reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0091",
+	.desc = "Counts all demand & prefetch data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0240",
+	.desc = "Counts all prefetch code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0240",
+	.desc = "Counts prefetch code reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0240",
+	.desc = "Counts prefetch code reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0240",
+	.desc = "Counts prefetch code reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_code_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0240",
+	.desc = "Counts prefetch code reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0090",
+	.desc = "Counts all prefetch data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0120",
+	.desc = "Counts all prefetch RFOs that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0120",
+	.desc = "Counts prefetch RFOs that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0120",
+	.desc = "Counts prefetch RFOs that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0120",
+	.desc = "Counts prefetch RFOs that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_rfo.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0120",
+	.desc = "Counts prefetch RFOs that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c03f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c03f7",
+	.desc = "Counts data/code/rfo reads (demand & prefetch) that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c03f7",
+	.desc = "Counts data/code/rfo reads (demand & prefetch) that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c03f7",
+	.desc = "Counts data/code/rfo reads (demand & prefetch) that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c03f7",
+	.desc = "Counts data/code/rfo reads (demand & prefetch) that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0122",
+	.desc = "Counts all demand & prefetch RFOs that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0122",
+	.desc = "Counts demand & prefetch RFOs that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0122",
+	.desc = "Counts demand & prefetch RFOs that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0122",
+	.desc = "Counts demand & prefetch RFOs that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0122",
+	.desc = "Counts demand & prefetch RFOs that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10008",
+	.desc = "COREWB & ANY_RESPONSE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0004",
+	.desc = "Counts all demand code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0004",
+	.desc = "Counts demand code reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0004",
+	.desc = "Counts demand code reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0004",
+	.desc = "Counts demand code reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0004",
+	.desc = "Counts demand code reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0001",
+	.desc = "Counts all demand data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0002",
+	.desc = "Counts all demand data writes (RFOs) that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0002",
+	.desc = "Counts demand data writes (RFOs) that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0002",
+	.desc = "Counts demand data writes (RFOs) that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0002",
+	.desc = "Counts demand data writes (RFOs) that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0002",
+	.desc = "Counts demand data writes (RFOs) that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x18000",
+	.desc = "Counts miscellaneous accesses that include port i/o, MMIO and uncacheable memory accesses. It also includes L2 hints sent to LLC to keep a line from being evicted out of the core caches",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.lru_hints",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x803c8000",
+	.desc = "Counts L2 hints sent to LLC to keep a line from being evicted out of the core caches",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.portio_mmio_uc",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2380408000",
+	.desc = "Counts miscellaneous accesses that include port i/o, MMIO and uncacheable memory accesses",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0040",
+	.desc = "Counts all prefetch (that bring data to L2) code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0040",
+	.desc = "Counts prefetch (that bring data to L2) code reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0040",
+	.desc = "Counts prefetch (that bring data to L2) code reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0040",
+	.desc = "Counts prefetch (that bring data to L2) code reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0040",
+	.desc = "Counts prefetch (that bring data to L2) code reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0010",
+	.desc = "Counts all prefetch (that bring data to L2) data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0020",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0020",
+	.desc = "Counts prefetch (that bring data to L2) RFOs that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0020",
+	.desc = "Counts prefetch (that bring data to L2) RFOs that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0020",
+	.desc = "Counts prefetch (that bring data to L2) RFOs that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0020",
+	.desc = "Counts prefetch (that bring data to L2) RFOs that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0200",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0200",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0200",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0200",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0200",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0080",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_rfo.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0100",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_rfo.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0100",
+	.desc = "Counts prefetch (that bring data to LLC only) RFOs that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_rfo.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0100",
+	.desc = "Counts prefetch (that bring data to LLC only) RFOs that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_rfo.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0100",
+	.desc = "Counts prefetch (that bring data to LLC only) RFOs that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_rfo.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0100",
+	.desc = "Counts prefetch (that bring data to LLC only) RFOs that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.split_lock_uc_lock.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10400",
+	.desc = "Counts requests where the address of an atomic lock instruction spans a cache line boundary or the lock instruction is executed on uncacheable address",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10800",
+	.desc = "Counts non-temporal stores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010001",
+	.desc = "Counts all demand data reads ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010002",
+	.desc = "Counts all demand rfo's ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010004",
+	.desc = "Counts all demand code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x000105B3",
+	.desc = "Counts all demand & prefetch data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00010122",
+	.desc = "Counts all demand & prefetch prefetch RFOs ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x000107F7",
+	.desc = "Counts all data/code/rfo references (demand & prefetch) ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10433",
+	.desc = " REQUEST = DATA_INTO_CORE and RESPONSE = ANY_RESPONSE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_m.hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000040002",
+	.desc = " REQUEST = DEMAND_RFO and RESPONSE = LLC_HIT_M and SNOOP = HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10040",
+	.desc = " REQUEST = PF_RFO and RESPONSE = ANY_RESPONSE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10080",
+	.desc = " REQUEST = PF_LLC_DATA_RD and RESPONSE = ANY_RESPONSE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l_ifetch.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10200",
+	.desc = " REQUEST = PF_LLC_IFETCH and RESPONSE = ANY_RESPONSE",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_broadwellde[] = {
+{
+	.name = "other_assists.avx_to_sse",
+	.event = "event=0xC1,umask=0x8,period=100003",
+	.desc = "Number of transitions from AVX-256 to legacy SSE when penalty applicable  Spec update: BDM30",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of transitions from AVX-256 to legacy SSE when penalty is applicable  Spec update: BDM30",
+},
+{
+	.name = "other_assists.sse_to_avx",
+	.event = "event=0xC1,umask=0x10,period=100003",
+	.desc = "Number of transitions from SSE to AVX-256 when penalty applicable  Spec update: BDM30",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of transitions from legacy SSE to AVX-256 when penalty is applicable  Spec update: BDM30",
+},
+{
+	.name = "fp_arith_inst_retired.scalar_double",
+	.event = "event=0xC7,umask=0x1,period=2000003",
+	.desc = "Number of SSE/AVX computational scalar double precision floating-point instructions retired.  Each count represents 1 computation. Applies to SSE* and AVX* scalar double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT FM(N)ADD/SUB.  FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.scalar_single",
+	.event = "event=0xC7,umask=0x2,period=2000003",
+	.desc = "Number of SSE/AVX computational scalar single precision floating-point instructions retired.  Each count represents 1 computation. Applies to SSE* and AVX* scalar single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT FM(N)ADD/SUB.  FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.128b_packed_double",
+	.event = "event=0xC7,umask=0x4,period=2000003",
+	.desc = "Number of SSE/AVX computational 128-bit packed double precision floating-point instructions retired.  Each count represents 2 computations. Applies to SSE* and AVX* packed double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.128b_packed_single",
+	.event = "event=0xC7,umask=0x8,period=2000003",
+	.desc = "Number of SSE/AVX computational 128-bit packed single precision floating-point instructions retired.  Each count represents 4 computations. Applies to SSE* and AVX* packed single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.256b_packed_double",
+	.event = "event=0xC7,umask=0x10,period=2000003",
+	.desc = "Number of SSE/AVX computational 256-bit packed double precision floating-point instructions retired.  Each count represents 4 computations. Applies to SSE* and AVX* packed double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.x87_output",
+	.event = "event=0xCA,umask=0x2,period=100003",
+	.desc = "Number of X87 assists due to output value",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of x87 floating point (FP) micro-code assist (numeric overflow/underflow, inexact result) when the output value (destination register) is invalid",
+},
+{
+	.name = "fp_assist.x87_input",
+	.event = "event=0xCA,umask=0x4,period=100003",
+	.desc = "Number of X87 assists due to input value",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts x87 floating point (FP) micro-code assist (invalid operation, denormal operand, SNaN operand) when the input value (one of the source operands to an FP instruction) is invalid",
+},
+{
+	.name = "fp_assist.simd_output",
+	.event = "event=0xCA,umask=0x8,period=100003",
+	.desc = "Number of SIMD FP assists due to Output values",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of SSE* floating point (FP) micro-code assist (numeric overflow/underflow) when the output value (destination register) is invalid. Counting covers only cases involving penalties that require micro-code assist intervention",
+},
+{
+	.name = "fp_assist.simd_input",
+	.event = "event=0xCA,umask=0x10,period=100003",
+	.desc = "Number of SIMD FP assists due to input values",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts any input SSE* FP assist - invalid operation, denormal operand, dividing by zero, SNaN operand. Counting includes only cases involving penalties that required micro-code assist intervention",
+},
+{
+	.name = "fp_assist.any",
+	.event = "event=0xCA,umask=0x1e,cmask=1,period=100003",
+	.desc = "Cycles with any input/output SSE or FP assist",
+	.topic = "floating point",
+	.long_desc = "This event counts cycles with any input and output SSE or x87 FP assist. If an input and output assist are detected on the same cycle the event increments by 1",
+},
+{
+	.name = "fp_arith_inst_retired.256b_packed_single",
+	.event = "event=0xc7,umask=0x20,period=2000003",
+	.desc = "Number of SSE/AVX computational 256-bit packed single precision floating-point instructions retired.  Each count represents 8 computations. Applies to SSE* and AVX* packed single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.scalar",
+	.event = "event=0xC7,umask=0x3,period=2000003",
+	.desc = "Number of SSE/AVX computational scalar floating-point instructions retired. Applies to SSE* and AVX* scalar, double and single precision floating-point: ADD SUB MUL DIV MIN MAX RSQRT RCP SQRT FM(N)ADD/SUB. FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.packed",
+	.event = "event=0xC7,umask=0x3c,period=2000004",
+	.desc = "Number of SSE/AVX computational packed floating-point instructions retired. Applies to SSE* and AVX*, packed, double and single precision floating-point: ADD SUB MUL DIV MIN MAX RSQRT RCP SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.single",
+	.event = "event=0xC7,umask=0x2a,period=2000005",
+	.desc = "Number of SSE/AVX computational single precision floating-point instructions retired. Applies to SSE* and AVX*scalar, double and single precision floating-point: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element. ?",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.double",
+	.event = "event=0xC7,umask=0x15,period=2000006",
+	.desc = "Number of SSE/AVX computational double precision floating-point instructions retired. Applies to SSE* and AVX*scalar, double and single precision floating-point: ADD SUB MUL DIV MIN MAX SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element.  ?",
+	.topic = "floating point",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired from execution",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions retired from execution. For instructions that consist of multiple micro-ops, this event counts the retirement of the last micro-op of the instruction. Counting continues during hardware interrupts, traps, and inside interrupt handlers. \nNotes: INST_RETIRED.ANY is counted by a designated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events. INST_RETIRED.ANY_P is counted by a programmable counter and it is an architectural performance event. \nCounting: Faulting executions of GETSEC/VM entry/VM Exit/MWait will not count as retired instructions",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Core cycles when the thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of core cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. This event is a component in many key event ratios. The core frequency may change from time to time due to transitions associated with Enhanced Intel SpeedStep Technology or TM2. For this reason this event may have a changing ratio with regards to time. When the core frequency is constant, this event can approximate elapsed time while the core was not in the halt state. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Reference cycles when the core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of reference cycles when the core is not in a halt state. The core enters the halt state when it is running the HLT instruction or the MWAIT instruction. This event is not affected by core frequency changes (for example, P states, TM2 transitions) but has the same incrementing frequency as the time stamp counter. This event can approximate elapsed time while the core was not in a halt state. This event has a constant ratio with the CPU_CLK_UNHALTED.REF_XCLK event. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events. \nNote: On all current platforms this event stops counting during 'throttling (TM)' states duty off periods the processor is 'halted'.  This event is clocked by base clock (100 Mhz) on Sandy Bridge. The counter update is done at a lower clock rate then the core clock the overflow status bit for this counter may appear 'sticky'.  After the counter has overflowed and software clears the overflow status bit and resets the counter to less than MAX. The reset value to the counter is not clocked immediately so the overflow status bit will flip 'high (1)' and generate another PMI (if enabled) after which the reset value gets clocked into the counter. Therefore, software will get the interrupt, read the overflow status bit '1 for bit 34 while the counter value is less than MAX. Software should ignore this case",
+},
+{
+	.name = "ld_blocks.store_forward",
+	.event = "event=0x03,umask=0x2,period=100003",
+	.desc = "Cases when loads get true Block-on-Store blocking code preventing store forwarding",
+	.topic = "pipeline",
+	.long_desc = "This event counts how many times the load operation got the true Block-on-Store blocking code preventing store forwarding. This includes cases when:\n - preceding store conflicts with the load (incomplete overlap);\n - store forwarding is impossible due to u-arch limitations;\n - preceding lock RMW operations are not forwarded;\n - store has the no-forward bit set (uncacheable/page-split/masked stores);\n - all-blocking stores are used (mostly, fences and port I/O);\nand others.\nThe most common case is a load blocked due to its address range overlapping with a preceding smaller uncompleted store. Note: This event does not take into account cases of out-of-SW-control (for example, SbTailHit), unknown physical STA, and cases of blocking loads on store due to being non-WB memory type or a lock. These cases are covered by other events.\nSee the table of not supported store forwards in the Optimization Guide",
+},
+{
+	.name = "ld_blocks.no_sr",
+	.event = "event=0x03,umask=0x8,period=100003",
+	.desc = "This event counts the number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks_partial.address_alias",
+	.event = "event=0x07,umask=0x1,period=100003",
+	.desc = "False dependencies in MOB due to partial compare",
+	.topic = "pipeline",
+	.long_desc = "This event counts false dependencies in MOB when the partial comparison upon loose net check and dependency was resolved by the Enhanced Loose net mechanism. This may not result in high performance penalties. Loose net checks can fail when loads and stores are 4k aliased",
+},
+{
+	.name = "int_misc.rat_stall_cycles",
+	.event = "event=0x0D,umask=0x8,period=2000003",
+	.desc = "Cycles when Resource Allocation Table (RAT) external stall is sent to Instruction Decode Queue (IDQ) for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of cycles during which Resource Allocation Table (RAT) external stall is sent to Instruction Decode Queue (IDQ) for the current thread. This also includes the cycles during which the Allocator is serving another thread",
+},
+{
+	.name = "int_misc.recovery_cycles",
+	.event = "event=0x0D,umask=0x3,cmask=1,period=2000003",
+	.desc = "Number of cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc...)",
+	.topic = "pipeline",
+	.long_desc = "Cycles checkpoints in Resource Allocation Table (RAT) are recovering from JEClear or machine clear",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0x0E,umask=0x1,period=2000003",
+	.desc = "Uops that Resource Allocation Table (RAT) issues to Reservation Station (RS)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of Uops issued by the Resource Allocation Table (RAT) to the reservation station (RS)",
+},
+{
+	.name = "uops_issued.flags_merge",
+	.event = "event=0x0E,umask=0x10,period=2000003",
+	.desc = "Number of flags-merge uops being allocated. Such uops considered perf sensitive; added by GSR u-arch",
+	.topic = "pipeline",
+	.long_desc = "Number of flags-merge uops being allocated. Such uops considered perf sensitive\n added by GSR u-arch",
+},
+{
+	.name = "uops_issued.slow_lea",
+	.event = "event=0x0E,umask=0x20,period=2000003",
+	.desc = "Number of slow LEA uops being allocated. A uop is generally considered SlowLea if it has 3 sources (e.g. 2 sources + immediate) regardless if as a result of LEA instruction or not",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.single_mul",
+	.event = "event=0x0E,umask=0x40,period=2000003",
+	.desc = "Number of Multiply packed/scalar single precision uops allocated",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "inv=1,event=0x0E,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which the Resource Allocation Table (RAT) does not issue any Uops to the reservation station (RS) for the current thread",
+},
+{
+	.name = "arith.fpu_div_active",
+	.event = "event=0x14,umask=0x1,period=2000003",
+	.desc = "Cycles when divider is busy executing divide operations",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of the divide operations executed. Uses edge-detect and a cmask value of 1 on ARITH.FPU_DIV_ACTIVE to get the number of the divide operations executed",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "This is a fixed-frequency event programmed to general counters. It counts when the core is unhalted at 100 Mhz",
+},
+{
+	.name = "cpu_clk_thread_unhalted.one_thread_active",
+	.event = "event=0x3c,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.sw_pf",
+	.event = "event=0x4c,umask=0x1,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for software prefetch",
+	.topic = "pipeline",
+	.long_desc = "This event counts all not software-prefetch load dispatches that hit the fill buffer (FB) allocated for the software prefetch. It can also be incremented by some lock instructions. So it should only be used with profiling so that the locks can be excluded by asm inspection of the nearby instructions",
+},
+{
+	.name = "load_hit_pre.hw_pf",
+	.event = "event=0x4C,umask=0x2,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for hardware prefetch",
+	.topic = "pipeline",
+	.long_desc = "This event counts all not software-prefetch load dispatches that hit the fill buffer (FB) allocated for the hardware prefetch",
+},
+{
+	.name = "move_elimination.int_eliminated",
+	.event = "event=0x58,umask=0x1,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.simd_eliminated",
+	.event = "event=0x58,umask=0x2,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.int_not_eliminated",
+	.event = "event=0x58,umask=0x4,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.simd_not_eliminated",
+	.event = "event=0x58,umask=0x8,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_cycles",
+	.event = "event=0x5E,umask=0x1,period=2000003",
+	.desc = "Cycles when Reservation Station (RS) is empty for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which the reservation station (RS) is empty for the thread.\nNote: In ST-mode, not active thread should drive 0. This is usually caused by severely costly branch mispredictions, or allocator/FE issues",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000003",
+	.desc = "Stalls caused by changing prefix length of the instruction",
+	.topic = "pipeline",
+	.long_desc = "This event counts stalls occured due to changing prefix length (66, 67 or REX.W when they change the length of the decoded instruction). Occurrences counting is proportional to the number of prefixes in a 16B-line. This may result in the following penalties: three-cycle penalty for each LCP in a 16-byte chunk",
+},
+{
+	.name = "br_inst_exec.nontaken_conditional",
+	.event = "event=0x88,umask=0x41,period=200003",
+	.desc = "Not taken macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts not taken macro-conditional branch instructions",
+},
+{
+	.name = "br_inst_exec.taken_conditional",
+	.event = "event=0x88,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired macro-conditional branch instructions",
+},
+{
+	.name = "br_inst_exec.taken_direct_jump",
+	.event = "event=0x88,umask=0x82,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branch instructions excluding calls and indirects",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired macro-conditional branch instructions excluding calls and indirect branches",
+},
+{
+	.name = "br_inst_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired indirect branches excluding calls and return branches",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_return",
+	.event = "event=0x88,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired indirect branches with return mnemonic",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired indirect branches that have a return mnemonic",
+},
+{
+	.name = "br_inst_exec.taken_direct_near_call",
+	.event = "event=0x88,umask=0x90,period=200003",
+	.desc = "Taken speculative and retired direct near calls",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired direct near calls",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_call",
+	.event = "event=0x88,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired indirect calls",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired indirect calls including both register and memory indirect",
+},
+{
+	.name = "br_inst_exec.all_conditional",
+	.event = "event=0x88,umask=0xc1,period=200003",
+	.desc = "Speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired macro-conditional branch instructions",
+},
+{
+	.name = "br_inst_exec.all_direct_jmp",
+	.event = "event=0x88,umask=0xc2,period=200003",
+	.desc = "Speculative and retired macro-unconditional branches excluding calls and indirects",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired macro-unconditional branch instructions, excluding calls and indirects",
+},
+{
+	.name = "br_inst_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0xc4,period=200003",
+	.desc = "Speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired indirect branches excluding calls and return branches",
+},
+{
+	.name = "br_inst_exec.all_indirect_near_return",
+	.event = "event=0x88,umask=0xc8,period=200003",
+	.desc = "Speculative and retired indirect return branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired indirect branches that have a return mnemonic",
+},
+{
+	.name = "br_inst_exec.all_direct_near_call",
+	.event = "event=0x88,umask=0xd0,period=200003",
+	.desc = "Speculative and retired direct near calls",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired direct near calls",
+},
+{
+	.name = "br_inst_exec.all_branches",
+	.event = "event=0x88,umask=0xff,period=200003",
+	.desc = "Speculative and retired  branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired branch instructions",
+},
+{
+	.name = "br_misp_exec.nontaken_conditional",
+	.event = "event=0x89,umask=0x41,period=200003",
+	.desc = "Not taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts not taken speculative and retired mispredicted macro conditional branch instructions",
+},
+{
+	.name = "br_misp_exec.taken_conditional",
+	.event = "event=0x89,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired mispredicted macro conditional branch instructions",
+},
+{
+	.name = "br_misp_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired mispredicted indirect branches excluding calls and returns",
+},
+{
+	.name = "br_misp_exec.taken_return_near",
+	.event = "event=0x89,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches with return mnemonic",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired mispredicted indirect branches that have a return mnemonic",
+},
+{
+	.name = "br_misp_exec.all_conditional",
+	.event = "event=0x89,umask=0xc1,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired mispredicted macro conditional branch instructions",
+},
+{
+	.name = "br_misp_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0xc4,period=200003",
+	.desc = "Mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken mispredicted indirect branches excluding calls and returns",
+},
+{
+	.name = "br_misp_exec.all_branches",
+	.event = "event=0x89,umask=0xff,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired mispredicted branch instructions",
+},
+{
+	.name = "uops_dispatched_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 0",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 0",
+},
+{
+	.name = "uops_dispatched_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 1",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 1",
+},
+{
+	.name = "uops_dispatched_port.port_2",
+	.event = "event=0xA1,umask=0x4,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 2",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 2",
+},
+{
+	.name = "uops_dispatched_port.port_3",
+	.event = "event=0xA1,umask=0x8,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 3",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 3",
+},
+{
+	.name = "uops_dispatched_port.port_4",
+	.event = "event=0xA1,umask=0x10,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 4",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 4",
+},
+{
+	.name = "uops_dispatched_port.port_5",
+	.event = "event=0xA1,umask=0x20,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 5",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 5",
+},
+{
+	.name = "uops_dispatched_port.port_6",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 6",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 6",
+},
+{
+	.name = "uops_dispatched_port.port_7",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 7",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 7",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000003",
+	.desc = "Resource-related stall cycles",
+	.topic = "pipeline",
+	.long_desc = "This event counts resource-related stall cycles. Reasons for stalls can be as follows:\n - *any* u-arch structure got full (LB, SB, RS, ROB, BOB, LM, Physical Register Reclaim Table (PRRT), or Physical History Table (PHT) slots)\n - *any* u-arch structure got empty (like INT/SIMD FreeLists)\n - FPU control word (FPCW), MXCSR\nand others. This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "resource_stalls.rs",
+	.event = "event=0xA2,umask=0x4,period=2000003",
+	.desc = "Cycles stalled due to no eligible RS entry available",
+	.topic = "pipeline",
+	.long_desc = "This event counts stall cycles caused by absence of eligible entries in the reservation station (RS). This may result from RS overflow, or from RS deallocation because of the RS array Write Port allocation scheme (each RS entry has two write ports instead of four. As a result, empty entries could not be used, although RS is not really full). This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "resource_stalls.sb",
+	.event = "event=0xA2,umask=0x8,period=2000003",
+	.desc = "Cycles stalled due to no store buffers available. (not including draining form sync)",
+	.topic = "pipeline",
+	.long_desc = "This event counts stall cycles caused by the store buffer (SB) overflow (excluding draining from synch). This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "resource_stalls.rob",
+	.event = "event=0xA2,umask=0x10,period=2000003",
+	.desc = "Cycles stalled due to re-order buffer full",
+	.topic = "pipeline",
+	.long_desc = "This event counts ROB full stall cycles. This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "cycle_activity.cycles_l2_pending",
+	.event = "event=0xA3,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles the CPU has at least one pending  demand* load request missing the L2 cache",
+},
+{
+	.name = "cycle_activity.cycles_l1d_pending",
+	.event = "event=0xA3,umask=0x8,cmask=8,period=2000003",
+	.desc = "Cycles while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles the CPU has at least one pending  demand load request missing the L1 data cache",
+},
+{
+	.name = "cycle_activity.cycles_ldm_pending",
+	.event = "event=0xA3,umask=0x2,cmask=2,period=2000003",
+	.desc = "Cycles while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles the CPU has at least one pending  demand load request (that is cycles with non-completed load waiting for its data from memory subsystem)",
+},
+{
+	.name = "cycle_activity.cycles_no_execute",
+	.event = "event=0xA3,umask=0x4,cmask=4,period=2000003",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port",
+},
+{
+	.name = "cycle_activity.stalls_l2_pending",
+	.event = "event=0xA3,umask=0x5,cmask=5,period=2000003",
+	.desc = "Execution stalls while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port, while there was at least one pending demand* load request missing the L2 cache.(as a footprint) * includes also L1 HW prefetch requests that may or may not be required by demands",
+},
+{
+	.name = "cycle_activity.stalls_ldm_pending",
+	.event = "event=0xA3,umask=0x6,cmask=6,period=2000003",
+	.desc = "Execution stalls while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port, while there was at least one pending demand load request",
+},
+{
+	.name = "cycle_activity.stalls_l1d_pending",
+	.event = "event=0xA3,umask=0xc,cmask=12,period=2000003",
+	.desc = "Execution stalls while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port, while there was at least one pending demand load request missing the L1 data cache",
+},
+{
+	.name = "lsd.uops",
+	.event = "event=0xA8,umask=0x1,period=2000003",
+	.desc = "Number of Uops delivered by the LSD",
+	.topic = "pipeline",
+	.long_desc = "Number of Uops delivered by the LSD",
+},
+{
+	.name = "uops_executed.thread",
+	.event = "event=0xB1,umask=0x1,period=2000003",
+	.desc = "Counts the number of uops to be executed per-thread each cycle",
+	.topic = "pipeline",
+	.long_desc = "Number of uops to be executed per-thread each cycle",
+},
+{
+	.name = "uops_executed.core",
+	.event = "event=0xB1,umask=0x2,period=2000003",
+	.desc = "Number of uops executed on the core",
+	.topic = "pipeline",
+	.long_desc = "Number of uops executed from any thread",
+},
+{
+	.name = "uops_executed.stall_cycles",
+	.event = "inv=1,event=0xB1,umask=0x1,cmask=1,period=2000003",
+	.desc = "Counts number of cycles no uops were dispatched to be executed on this thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which no uops were dispatched from the Reservation Station (RS) per thread",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Number of instructions retired. General Counter   - architectural event  Spec update: BDM61",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions (EOMs) retired. Counting covers macro-fused instructions individually (that is, increments by two)  Spec update: BDM61",
+},
+{
+	.name = "inst_retired.x87",
+	.event = "event=0xC0,umask=0x2,period=2000003",
+	.desc = "FP operations  retired. X87 FP operations that have no exceptions:",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts FP operations retired. For X87 FP operations that have no exceptions counting also includes flows that have several X87, or flows that use X87 uops in the exception handling",
+},
+{
+	.name = "inst_retired.prec_dist",
+	.event = "event=0xC0,umask=0x1,period=2000003",
+	.desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution  Spec update: BDM11, BDM55 (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version (that is, uses PEBS) of the event that counts instructions retired  Spec update: BDM11, BDM55 (Must be precise)",
+},
+{
+	.name = "other_assists.any_wb_assist",
+	.event = "event=0xC1,umask=0x40,period=100003",
+	.desc = "Number of times any microcode assist is invoked by HW upon uop writeback",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.all",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "Actually retired uops  Supports address when precise (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This event counts all actually retired uops. Counting increments by two for micro-fused uops, and by one for macro-fused and other uops. Maximal increment value for one cycle is eight  Supports address when precise (Precise event)",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000003",
+	.desc = "Retirement slots used (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of retirement slots used (Precise event)",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "inv=1,event=0xC2,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts cycles without actually retired uops",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "inv=1,event=0xC2,umask=0x1,cmask=10,period=2000003",
+	.desc = "Cycles with less than 10 actually retired uops",
+	.topic = "pipeline",
+	.long_desc = "Number of cycles using always true condition (uops_ret < 16) applied to non PEBS uops retired event",
+},
+{
+	.name = "machine_clears.cycles",
+	.event = "event=0xC3,umask=0x1,period=2000003",
+	.desc = "Cycles there was a Nuke. Account for both thread-specific and All Thread Nukes",
+	.topic = "pipeline",
+	.long_desc = "This event counts both thread-specific (TS) and all-thread (AT) nukes",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=100003",
+	.desc = "Self-modifying code (SMC) detected",
+	.topic = "pipeline",
+	.long_desc = "This event counts self-modifying code (SMC) detected, which causes a machine clear",
+},
+{
+	.name = "machine_clears.maskmov",
+	.event = "event=0xC3,umask=0x20,period=100003",
+	.desc = "This event counts the number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0",
+	.topic = "pipeline",
+	.long_desc = "Maskmov false fault - counts number of time ucode passes through Maskmov flow due to instruction's mask being 0 while the flow was completed without raising a fault",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=400009",
+	.desc = "Conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=100007",
+	.desc = "Direct and indirect near call instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts both direct and indirect near call instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=400009",
+	.desc = "All (macro) branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts all (macro) branch instructions retired",
+},
+{
+	.name = "br_inst_retired.near_return",
+	.event = "event=0xC4,umask=0x8,period=100007",
+	.desc = "Return instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts return instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.not_taken",
+	.event = "event=0xC4,umask=0x10,period=400009",
+	.desc = "Not taken branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts not taken branch instructions retired",
+},
+{
+	.name = "br_inst_retired.near_taken",
+	.event = "event=0xC4,umask=0x20,period=400009",
+	.desc = "Taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts taken branch instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0x40,period=100007",
+	.desc = "Far branch instructions retired  Spec update: BDW98",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts far branch instructions retired  Spec update: BDW98",
+},
+{
+	.name = "br_inst_retired.all_branches_pebs",
+	.event = "event=0xC4,umask=0x4,period=400009",
+	.desc = "All (macro) branch instructions retired. (Precise Event - PEBS)  Spec update: BDW98 (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version of BR_INST_RETIRED.ALL_BRANCHES that counts all (macro) branch instructions retired  Spec update: BDW98 (Must be precise)",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=400009",
+	.desc = "Mispredicted conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts mispredicted conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=400009",
+	.desc = "All mispredicted macro branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts all mispredicted macro branch instructions retired",
+},
+{
+	.name = "br_misp_retired.ret",
+	.event = "event=0xC5,umask=0x8,period=100007",
+	.desc = "This event counts the number of mispredicted ret instructions retired. Non PEBS (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts mispredicted return instructions retired (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches_pebs",
+	.event = "event=0xC5,umask=0x4,period=400009",
+	.desc = "Mispredicted macro branch instructions retired. (Precise Event - PEBS) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version of BR_MISP_RETIRED.ALL_BRANCHES that counts all mispredicted macro branch instructions retired (Must be precise)",
+},
+{
+	.name = "rob_misc_events.lbr_inserts",
+	.event = "event=0xCC,umask=0x20,period=2000003",
+	.desc = "Count cases of saving new LBR",
+	.topic = "pipeline",
+	.long_desc = "This event counts cases of saving new LBR records by hardware. This assumes proper enabling of LBRs and takes into account LBR filtering done by the LBR_SELECT register",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Thread cycles when thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This is an architectural event that counts the number of thread cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. The core frequency may change from time to time due to power or thermal throttling. For this reason, this event may have a changing ratio with regards to wall clock time",
+},
+{
+	.name = "br_misp_exec.taken_indirect_near_call",
+	.event = "event=0x89,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_0_core",
+	.event = "event=0xA1,umask=0x1,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_1_core",
+	.event = "event=0xA1,umask=0x2,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_2_core",
+	.event = "event=0xA1,umask=0x4,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 2",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_3_core",
+	.event = "event=0xA1,umask=0x8,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 3",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_4_core",
+	.event = "event=0xA1,umask=0x10,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_5_core",
+	.event = "event=0xA1,umask=0x20,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_6_core",
+	.event = "event=0xA1,umask=0x40,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 6",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_7_core",
+	.event = "event=0xA1,umask=0x80,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 7",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_taken",
+	.event = "event=0xC5,umask=0x20,period=400009",
+	.desc = "number of near branch instructions retired that were mispredicted and taken (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Number of near branch instructions retired that were mispredicted and taken (Precise event)",
+},
+{
+	.name = "uops_executed.cycles_ge_1_uop_exec",
+	.event = "event=0xB1,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles where at least 1 uop was executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.cycles_ge_2_uops_exec",
+	.event = "event=0xB1,umask=0x1,cmask=2,period=2000003",
+	.desc = "Cycles where at least 2 uops were executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.cycles_ge_3_uops_exec",
+	.event = "event=0xB1,umask=0x1,cmask=3,period=2000003",
+	.desc = "Cycles where at least 3 uops were executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.cycles_ge_4_uops_exec",
+	.event = "event=0xB1,umask=0x1,cmask=4,period=2000003",
+	.desc = "Cycles where at least 4 uops were executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "baclears.any",
+	.event = "event=0xe6,umask=0x1f,period=100003",
+	.desc = "Counts the total number when the front end is resteered, mainly when the BPU cannot provide a correct prediction and this is corrected by other branch handling mechanisms at the front end",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l1d_miss",
+	.event = "event=0xA3,umask=0x8,cmask=8,period=2000003",
+	.desc = "Cycles while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l2_miss",
+	.event = "event=0xA3,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_mem_any",
+	.event = "event=0xA3,umask=0x2,cmask=2,period=2000003",
+	.desc = "Cycles while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_total",
+	.event = "event=0xA3,umask=0x4,cmask=4,period=2000003",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l1d_miss",
+	.event = "event=0xA3,umask=0xc,cmask=12,period=2000003",
+	.desc = "Execution stalls while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l2_miss",
+	.event = "event=0xA3,umask=0x5,cmask=5,period=2000003",
+	.desc = "Execution stalls while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_mem_any",
+	.event = "event=0xA3,umask=0x6,cmask=6,period=2000003",
+	.desc = "Execution stalls while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.count",
+	.event = "edge=1,event=0xC3,umask=0x1,cmask=1,period=100003",
+	.desc = "Number of machine clears (nukes) of any type",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_4_uops",
+	.event = "event=0xA8,umask=0x1,cmask=4,period=2000003",
+	.desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_end",
+	.event = "edge=1,inv=1,event=0x5E,umask=0x1,cmask=1,period=200003",
+	.desc = "Counts end of periods where the Reservation Station (RS) was empty. Could be useful to precisely locate Frontend Latency Bound issues",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_active",
+	.event = "event=0xA8,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 0",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 0",
+},
+{
+	.name = "uops_executed_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 1",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 1",
+},
+{
+	.name = "uops_executed_port.port_2",
+	.event = "event=0xA1,umask=0x4,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 2",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 2",
+},
+{
+	.name = "uops_executed_port.port_3",
+	.event = "event=0xA1,umask=0x8,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 3",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 3",
+},
+{
+	.name = "uops_executed_port.port_4",
+	.event = "event=0xA1,umask=0x10,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 4",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 4",
+},
+{
+	.name = "uops_executed_port.port_5",
+	.event = "event=0xA1,umask=0x20,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 5",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 5",
+},
+{
+	.name = "uops_executed_port.port_6",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 6",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 6",
+},
+{
+	.name = "uops_executed_port.port_7",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 7",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 7",
+},
+{
+	.name = "uop_dispatches_cancelled.simd_prf",
+	.event = "event=0xA0,umask=0x3,period=2000003",
+	.desc = "Micro-op dispatches cancelled due to insufficient SIMD physical register file read ports",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of micro-operations cancelled after they were dispatched from the scheduler to the execution units when the total number of physical register read ports across all dispatch ports exceeds the read bandwidth of the physical register file.  The SIMD_PRF subevent applies to the following instructions: VDPPS, DPPS, VPCMPESTRI, PCMPESTRI, VPCMPESTRM, PCMPESTRM, VFMADD*, VFMADDSUB*, VFMSUB*, VMSUBADD*, VFNMADD*, VFNMSUB*.  See the Broadwell Optimization Guide for more information",
+},
+{
+	.name = "cpu_clk_unhalted.thread_any",
+	.event = "event=0x3c,any=1",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p_any",
+	.event = "event=0x3C,umask=0x0,any=1,period=2000003",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_cycles_any",
+	.event = "event=0x0D,umask=0x3,any=1,cmask=1,period=2000003",
+	.desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_1",
+	.event = "event=0xb1,umask=0x2,cmask=1,period=2000003",
+	.desc = "Cycles at least 1 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_2",
+	.event = "event=0xb1,umask=0x2,cmask=2,period=2000003",
+	.desc = "Cycles at least 2 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_3",
+	.event = "event=0xb1,umask=0x2,cmask=3,period=2000003",
+	.desc = "Cycles at least 3 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_4",
+	.event = "event=0xb1,umask=0x2,cmask=4,period=2000003",
+	.desc = "Cycles at least 4 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_none",
+	.event = "inv=1,event=0xb1,umask=0x2,period=2000003",
+	.desc = "Cycles with no micro-ops executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "idq.empty",
+	.event = "event=0x79,umask=0x2,period=2000003",
+	.desc = "Instruction Decode Queue (IDQ) empty cycles",
+	.topic = "frontend",
+	.long_desc = "This counts the number of cycles that the instruction decoder queue is empty and can indicate that the application may be bound in the front end.  It does not determine whether there are uops being delivered to the Alloc stage since uops can be delivered by bypass skipping the Instruction Decode Queue (IDQ) when it is empty",
+},
+{
+	.name = "idq.mite_uops",
+	.event = "event=0x79,umask=0x4,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq.dsb_uops",
+	.event = "event=0x79,umask=0x8,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_dsb_uops",
+	.event = "event=0x79,umask=0x10,period=2000003",
+	.desc = "Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while the Microcode Sequencer (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_mite_uops",
+	.event = "event=0x79,umask=0x20,period=2000003",
+	.desc = "Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_uops",
+	.event = "event=0x79,umask=0x30,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the total number of uops delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may \"bypass\" the IDQ. Uops maybe initiated by Decode Stream Buffer (DSB) or MITE",
+},
+{
+	.name = "idq.ms_cycles",
+	.event = "event=0x79,umask=0x30,cmask=1,period=2000003",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may \"bypass\" the IDQ. Uops maybe initiated by Decode Stream Buffer (DSB) or MITE",
+},
+{
+	.name = "idq.mite_cycles",
+	.event = "event=0x79,umask=0x4,cmask=1,period=2000003",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.dsb_cycles",
+	.event = "event=0x79,umask=0x8,cmask=1,period=2000003",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_dsb_cycles",
+	.event = "event=0x79,umask=0x10,cmask=1,period=2000003",
+	.desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while the Microcode Sequencer (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_dsb_occur",
+	.event = "edge=1,event=0x79,umask=0x10,cmask=1,period=2000003",
+	.desc = "Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while the Microcode Sequencer (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.all_dsb_cycles_4_uops",
+	.event = "event=0x79,umask=0x18,cmask=4,period=2000003",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles 4  uops were  delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.all_dsb_cycles_any_uops",
+	.event = "event=0x79,umask=0x18,cmask=1,period=2000003",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles  uops were  delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.all_mite_cycles_4_uops",
+	.event = "event=0x79,umask=0x24,cmask=4,period=2000003",
+	.desc = "Cycles MITE is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles 4  uops were  delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq.all_mite_cycles_any_uops",
+	.event = "event=0x79,umask=0x24,cmask=1,period=2000003",
+	.desc = "Cycles MITE is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles  uops were delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq.mite_all_uops",
+	.event = "event=0x79,umask=0x3c,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=2000003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Reads. both cacheable and noncacheable, including UC fetches",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of both cacheable and noncacheable Instruction Cache, Streaming Buffer and Victim Cache Reads including UC fetches",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Misses. Includes Uncacheable accesses",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of instruction cache, streaming buffer and victim cache misses. Counting includes UC accesses",
+},
+{
+	.name = "icache.ifdata_stall",
+	.event = "event=0x80,umask=0x4,period=2000003",
+	.desc = "Cycles where a code fetch is stalled due to L1 instruction-cache miss",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which the demand fetch waits for data (wfdM104H) from L2 or iSB (opportunistic hit)",
+},
+{
+	.name = "idq_uops_not_delivered.core",
+	.event = "event=0x9C,umask=0x1,period=2000003",
+	.desc = "Uops not delivered to Resource Allocation Table (RAT) per thread when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops not delivered to Resource Allocation Table (RAT) per thread adding ?4 ? x? when Resource Allocation Table (RAT) is not stalled and Instruction Decode Queue (IDQ) delivers x uops to Resource Allocation Table (RAT) (where x belongs to {0,1,2,3}). Counting does not cover cases when:\n a. IDQ-Resource Allocation Table (RAT) pipe serves the other thread;\n b. Resource Allocation Table (RAT) is stalled for the thread (including uop drops and clear BE conditions); \n c. Instruction Decode Queue (IDQ) delivers four uops",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_0_uops_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=4,period=2000003",
+	.desc = "Cycles per thread when 4 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts, on the per-thread basis, cycles when no uops are delivered to Resource Allocation Table (RAT). IDQ_Uops_Not_Delivered.core =4",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_1_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=3,period=2000003",
+	.desc = "Cycles per thread when 3 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts, on the per-thread basis, cycles when less than 1 uop is  delivered to Resource Allocation Table (RAT). IDQ_Uops_Not_Delivered.core >=3",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_2_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=2,period=2000003",
+	.desc = "Cycles with less than 2 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_3_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles with less than 3 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_fe_was_ok",
+	.event = "inv=1,event=0x9C,umask=0x1,cmask=1,period=2000003",
+	.desc = "Counts cycles FE delivered 4 uops or Resource Allocation Table (RAT) was stalling FE",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.penalty_cycles",
+	.event = "event=0xAB,umask=0x2,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles",
+	.topic = "frontend",
+	.long_desc = "This event counts Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles. These cycles do not include uops routed through because of the switch itself, for example, when Instruction Decode Queue (IDQ) pre-allocation is unavailable, or Instruction Decode Queue (IDQ) is full. SBD-to-MITE switch true penalty cycles happen after the merge mux (MM) receives Decode Stream Buffer (DSB) Sync-indication until receiving the first MITE uop. \nMM is placed before Instruction Decode Queue (IDQ) to merge uops being fed from the MITE and Decode Stream Buffer (DSB) paths. Decode Stream Buffer (DSB) inserts the Sync-indication whenever a Decode Stream Buffer (DSB)-to-MITE switch occurs.\nPenalty: A Decode Stream Buffer (DSB) hit followed by a Decode Stream Buffer (DSB) miss can cost up to six cycles in which no uops are delivered to the IDQ. Most often, such switches from the Decode Stream Buffer (DSB) to the legacy pipeline cost 0?2 cycles",
+},
+{
+	.name = "idq.ms_switches",
+	.event = "edge=1,event=0x79,umask=0x30,cmask=1,period=2000003",
+	.desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+	.topic = "frontend",
+},
+{
+	.name = "cpl_cycles.ring0",
+	.event = "event=0x5C,umask=0x1,period=2000003",
+	.desc = "Unhalted core cycles when the thread is in ring 0",
+	.topic = "other",
+	.long_desc = "This event counts the unhalted core cycles during which the thread is in the ring 0 privileged mode",
+},
+{
+	.name = "cpl_cycles.ring123",
+	.event = "event=0x5C,umask=0x2,period=2000003",
+	.desc = "Unhalted core cycles when thread is in rings 1, 2, or 3",
+	.topic = "other",
+	.long_desc = "This event counts unhalted core cycles during which the thread is in rings 1, 2, or 3",
+},
+{
+	.name = "cpl_cycles.ring0_trans",
+	.event = "edge=1,event=0x5C,umask=0x1,cmask=1,period=100007",
+	.desc = "Number of intervals between processor halts while thread is in ring 0",
+	.topic = "other",
+	.long_desc = "This event counts when there is a transition from ring 1,2 or 3 to ring0",
+},
+{
+	.name = "lock_cycles.split_lock_uc_lock_duration",
+	.event = "event=0x63,umask=0x1,period=2000003",
+	.desc = "Cycles when L1 and L2 are locked due to UC or split lock",
+	.topic = "other",
+	.long_desc = "This event counts cycles in which the L1 and L2 are locked due to a UC lock or split lock. A lock is asserted in case of locked memory access, due to noncacheable memory, locked operation that spans two cache lines, or a page walk from the noncacheable page table. L1D and L2 locks have a very high performance penalty and it is highly recommended to avoid such access",
+},
+{
+	.name = "misalign_mem_ref.loads",
+	.event = "event=0x05,umask=0x1,period=2000003",
+	.desc = "Speculative cache line split load uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "This event counts speculative cache-line split load uops dispatched to the L1 cache",
+},
+{
+	.name = "misalign_mem_ref.stores",
+	.event = "event=0x05,umask=0x2,period=2000003",
+	.desc = "Speculative cache line split STA uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "This event counts speculative cache line split store-address (STA) uops dispatched to the L1 cache",
+},
+{
+	.name = "tx_mem.abort_conflict",
+	.event = "event=0x54,umask=0x1,period=2000003",
+	.desc = "Number of times a TSX line had a cache conflict",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX line had a cache conflict",
+},
+{
+	.name = "tx_mem.abort_capacity_write",
+	.event = "event=0x54,umask=0x2,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to an evicted line caused by a transaction overflow",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to an evicted line caused by a transaction overflow",
+},
+{
+	.name = "tx_mem.abort_hle_store_to_elided_lock",
+	.event = "event=0x54,umask=0x4,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to a non-release/commit store to lock",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to a non-release/commit store to lock",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_not_empty",
+	.event = "event=0x54,umask=0x8,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to commit but Lock Buffer not empty",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to commit but Lock Buffer not empty",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_mismatch",
+	.event = "event=0x54,umask=0x10,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to release/commit but data and address mismatch",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to release/commit but data and address mismatch",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_unsupported_alignment",
+	.event = "event=0x54,umask=0x20,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to attempting an unsupported alignment from Lock Buffer",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to attempting an unsupported alignment from Lock Buffer",
+},
+{
+	.name = "tx_mem.hle_elision_buffer_full",
+	.event = "event=0x54,umask=0x40,period=2000003",
+	.desc = "Number of times we could not allocate Lock Buffer",
+	.topic = "memory",
+	.long_desc = "Number of times we could not allocate Lock Buffer",
+},
+{
+	.name = "tx_exec.misc1",
+	.event = "event=0x5d,umask=0x1,period=2000003",
+	.desc = "Counts the number of times a class of instructions that may cause a transactional abort was executed. Since this is the count of execution, it may not always cause a transactional abort",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by  a flowmarker",
+},
+{
+	.name = "tx_exec.misc2",
+	.event = "event=0x5d,umask=0x2,period=2000003",
+	.desc = "Counts the number of times a class of instructions (e.g., vzeroupper) that may cause a transactional abort was executed inside a transactional region",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by  a vzeroupper instruction",
+},
+{
+	.name = "tx_exec.misc3",
+	.event = "event=0x5d,umask=0x4,period=2000003",
+	.desc = "Counts the number of times an instruction execution caused the transactional nest count supported to be exceeded",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by a nest count that is too deep",
+},
+{
+	.name = "tx_exec.misc4",
+	.event = "event=0x5d,umask=0x8,period=2000003",
+	.desc = "Counts the number of times a XBEGIN instruction was executed inside an HLE transactional region",
+	.topic = "memory",
+	.long_desc = "RTM region detected inside HLE",
+},
+{
+	.name = "tx_exec.misc5",
+	.event = "event=0x5d,umask=0x10,period=2000003",
+	.desc = "Counts the number of times an HLE XACQUIRE instruction was executed inside an RTM transactional region",
+	.topic = "memory",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=100003",
+	.desc = "Counts the number of machine clears due to memory order conflicts",
+	.topic = "memory",
+	.long_desc = "This event counts the number of memory ordering Machine Clears detected. Memory Ordering Machine Clears can result from one of the following:\n1. memory disambiguation,\n2. external snoop, or\n3. cross SMT-HW-thread snoop (stores) hitting load buffer",
+},
+{
+	.name = "hle_retired.start",
+	.event = "event=0xc8,umask=0x1,period=2000003",
+	.desc = "Number of times we entered an HLE region; does not count nested transactions",
+	.topic = "memory",
+	.long_desc = "Number of times we entered an HLE region\n does not count nested transactions",
+},
+{
+	.name = "hle_retired.commit",
+	.event = "event=0xc8,umask=0x2,period=2000003",
+	.desc = "Number of times HLE commit succeeded",
+	.topic = "memory",
+	.long_desc = "Number of times HLE commit succeeded",
+},
+{
+	.name = "hle_retired.aborted",
+	.event = "event=0xc8,umask=0x4,period=2000003",
+	.desc = "Number of times HLE abort was triggered (Precise event)",
+	.topic = "memory",
+	.long_desc = "Number of times HLE abort was triggered (Precise event)",
+},
+{
+	.name = "hle_retired.aborted_misc1",
+	.event = "event=0xc8,umask=0x8,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to various memory events (e.g., read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times an HLE abort was attributed to a Memory condition (See TSX_Memory event for additional details)",
+},
+{
+	.name = "hle_retired.aborted_misc2",
+	.event = "event=0xc8,umask=0x10,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to uncommon conditions",
+	.topic = "memory",
+	.long_desc = "Number of times the TSX watchdog signaled an HLE abort",
+},
+{
+	.name = "hle_retired.aborted_misc3",
+	.event = "event=0xc8,umask=0x20,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to HLE-unfriendly instructions",
+	.topic = "memory",
+	.long_desc = "Number of times a disallowed operation caused an HLE abort",
+},
+{
+	.name = "hle_retired.aborted_misc4",
+	.event = "event=0xc8,umask=0x40,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to incompatible memory type",
+	.topic = "memory",
+	.long_desc = "Number of times HLE caused a fault",
+},
+{
+	.name = "hle_retired.aborted_misc5",
+	.event = "event=0xc8,umask=0x80,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to none of the previous 4 categories (e.g. interrupts)",
+	.topic = "memory",
+	.long_desc = "Number of times HLE aborted and was not due to the abort conditions in subevents 3-6",
+},
+{
+	.name = "rtm_retired.start",
+	.event = "event=0xc9,umask=0x1,period=2000003",
+	.desc = "Number of times we entered an RTM region; does not count nested transactions",
+	.topic = "memory",
+	.long_desc = "Number of times we entered an RTM region\n does not count nested transactions",
+},
+{
+	.name = "rtm_retired.commit",
+	.event = "event=0xc9,umask=0x2,period=2000003",
+	.desc = "Number of times RTM commit succeeded",
+	.topic = "memory",
+	.long_desc = "Number of times RTM commit succeeded",
+},
+{
+	.name = "rtm_retired.aborted",
+	.event = "event=0xc9,umask=0x4,period=2000003",
+	.desc = "Number of times RTM abort was triggered (Precise event)",
+	.topic = "memory",
+	.long_desc = "Number of times RTM abort was triggered  (Precise event)",
+},
+{
+	.name = "rtm_retired.aborted_misc1",
+	.event = "event=0xc9,umask=0x8,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to various memory events (e.g. read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times an RTM abort was attributed to a Memory condition (See TSX_Memory event for additional details)",
+},
+{
+	.name = "rtm_retired.aborted_misc2",
+	.event = "event=0xc9,umask=0x10,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to various memory events (e.g., read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times the TSX watchdog signaled an RTM abort",
+},
+{
+	.name = "rtm_retired.aborted_misc3",
+	.event = "event=0xc9,umask=0x20,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to HLE-unfriendly instructions",
+	.topic = "memory",
+	.long_desc = "Number of times a disallowed operation caused an RTM abort",
+},
+{
+	.name = "rtm_retired.aborted_misc4",
+	.event = "event=0xc9,umask=0x40,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to incompatible memory type",
+	.topic = "memory",
+	.long_desc = "Number of times a RTM caused a fault",
+},
+{
+	.name = "rtm_retired.aborted_misc5",
+	.event = "event=0xc9,umask=0x80,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to none of the previous 4 categories (e.g. interrupt)",
+	.topic = "memory",
+	.long_desc = "Number of times RTM aborted and was not due to the abort conditions in subevents 3-6",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_4",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x4",
+	.desc = "Loads with latency value being above 4  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above four  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_8",
+	.event = "event=0xCD,umask=0x1,period=50021,ldlat=0x8",
+	.desc = "Loads with latency value being above 8  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above eight  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_16",
+	.event = "event=0xCD,umask=0x1,period=20011,ldlat=0x10",
+	.desc = "Loads with latency value being above 16  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 16  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_32",
+	.event = "event=0xCD,umask=0x1,period=100007,ldlat=0x20",
+	.desc = "Loads with latency value being above 32  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 32  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_64",
+	.event = "event=0xCD,umask=0x1,period=2003,ldlat=0x40",
+	.desc = "Loads with latency value being above 64  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 64  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_128",
+	.event = "event=0xCD,umask=0x1,period=1009,ldlat=0x80",
+	.desc = "Loads with latency value being above 128  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 128  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_256",
+	.event = "event=0xCD,umask=0x1,period=503,ldlat=0x100",
+	.desc = "Loads with latency value being above 256  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 256  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_512",
+	.event = "event=0xCD,umask=0x1,period=101,ldlat=0x200",
+	.desc = "Loads with latency value being above 512  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 512  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "dtlb_load_misses.miss_causes_a_walk",
+	.event = "event=0x08,umask=0x1,period=100003",
+	.desc = "Load misses in all DTLB levels that cause page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_4k",
+	.event = "event=0x08,umask=0x2,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (4K)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_2m_4m",
+	.event = "event=0x08,umask=0x4,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (2M/4M)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_1g",
+	.event = "event=0x08,umask=0x8,period=2000003",
+	.desc = "Load miss in all TLB levels causes a page walk that completes. (1G)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_duration",
+	.event = "event=0x08,umask=0x10,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of cycles while PMH is busy with the page walk  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit_4k",
+	.event = "event=0x08,umask=0x20,period=2000003",
+	.desc = "Load misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit_2m",
+	.event = "event=0x08,umask=0x40,period=2000003",
+	.desc = "Load misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.miss_causes_a_walk",
+	.event = "event=0x49,umask=0x1,period=100003",
+	.desc = "Store misses in all DTLB levels that cause page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_4k",
+	.event = "event=0x49,umask=0x2,period=100003",
+	.desc = "Store miss in all TLB levels causes a page walk that completes. (4K)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_2m_4m",
+	.event = "event=0x49,umask=0x4,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks (2M/4M)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_1g",
+	.event = "event=0x49,umask=0x8,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks (1G)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_duration",
+	.event = "event=0x49,umask=0x10,period=100003",
+	.desc = "Cycles when PMH is busy with page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of cycles while PMH is busy with the page walk  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit_4k",
+	.event = "event=0x49,umask=0x20,period=100003",
+	.desc = "Store misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit_2m",
+	.event = "event=0x49,umask=0x40,period=100003",
+	.desc = "Store misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4F,umask=0x10,period=2000003",
+	.desc = "Cycle count for an Extended Page table walk",
+	.topic = "virtual memory",
+	.long_desc = "This event counts cycles for an extended page table walk. The Extended Page directory cache differs from standard TLB caches by the operating system that use it. Virtual machine operating systems use the extended page directory cache, while guest operating systems use the standard TLB caches",
+},
+{
+	.name = "itlb_misses.miss_causes_a_walk",
+	.event = "event=0x85,umask=0x1,period=100003",
+	.desc = "Misses at all ITLB levels that cause page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_completed_4k",
+	.event = "event=0x85,umask=0x2,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (4K)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_completed_2m_4m",
+	.event = "event=0x85,umask=0x4,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (2M/4M)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_completed_1g",
+	.event = "event=0x85,umask=0x8,period=100003",
+	.desc = "Store miss in all TLB levels causes a page walk that completes. (1G)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_duration",
+	.event = "event=0x85,umask=0x10,period=100003",
+	.desc = "Cycles when PMH is busy with page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of cycles while PMH is busy with the page walk  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.stlb_hit_4k",
+	.event = "event=0x85,umask=0x20,period=100003",
+	.desc = "Core misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.stlb_hit_2m",
+	.event = "event=0x85,umask=0x40,period=100003",
+	.desc = "Code misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb.itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=100007",
+	.desc = "Flushing of the Instruction TLB (ITLB) pages, includes 4k/2M/4M pages",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of flushes of the big or small ITLB pages. Counting include both TLB Flush (covering all sets) and TLB Set Clear (set-specific)",
+},
+{
+	.name = "page_walker_loads.dtlb_l1",
+	.event = "event=0xBC,umask=0x11,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L1+FB  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.itlb_l1",
+	.event = "event=0xBC,umask=0x21,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L1+FB  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_l2",
+	.event = "event=0xBC,umask=0x12,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L2  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.itlb_l2",
+	.event = "event=0xBC,umask=0x22,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L2  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_l3",
+	.event = "event=0xBC,umask=0x14,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L3 + XSNP  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.itlb_l3",
+	.event = "event=0xBC,umask=0x24,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L3 + XSNP  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_memory",
+	.event = "event=0xBC,umask=0x18,period=2000003",
+	.desc = "Number of DTLB page walker hits in Memory  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "tlb_flush.dtlb_thread",
+	.event = "event=0xBD,umask=0x1,period=100007",
+	.desc = "DTLB flush attempts of the thread-specific entries",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of DTLB flush attempts of the thread-specific entries",
+},
+{
+	.name = "tlb_flush.stlb_any",
+	.event = "event=0xBD,umask=0x20,period=100007",
+	.desc = "STLB flush attempts",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of any STLB flush attempts (such as entire, VPID, PCID, InvPage, CR3 write, and so on)",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x08,umask=0xe,period=100003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes of any page size  Spec update: BDM69",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x08,umask=0x60,period=2000003",
+	.desc = "Load operations that miss the first DTLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.walk_completed",
+	.event = "event=0x49,umask=0xe,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit",
+	.event = "event=0x49,umask=0x60,period=100003",
+	.desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0xe,period=100003",
+	.desc = "Misses in all ITLB levels that cause completed page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.stlb_hit",
+	.event = "event=0x85,umask=0x60,period=100003",
+	.desc = "Operations that miss the first ITLB level but hit the second and do not cause any page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_miss",
+	.event = "event=0x24,umask=0x21,period=200003",
+	.desc = "Demand Data Read miss L2, no rejects",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests that miss L2 cache. Only not rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_hit",
+	.event = "event=0x24,umask=0x41,period=200003",
+	.desc = "Demand Data Read requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests that hit L2 cache. Only not rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.l2_pf_miss",
+	.event = "event=0x24,umask=0x30,period=200003",
+	.desc = "L2 prefetch requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of requests from the L2 hardware prefetchers that miss L2 cache",
+},
+{
+	.name = "l2_rqsts.l2_pf_hit",
+	.event = "event=0x24,umask=0x50,period=200003",
+	.desc = "L2 prefetch requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of requests from the L2 hardware prefetchers that hit L2 cache. L3 prefetch new types",
+},
+{
+	.name = "l2_rqsts.all_demand_data_rd",
+	.event = "event=0x24,umask=0xe1,period=200003",
+	.desc = "Demand Data Read requests",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests (including requests from L1D hardware prefetchers). These loads may hit or miss L2 cache. Only non rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.all_rfo",
+	.event = "event=0x24,umask=0xe2,period=200003",
+	.desc = "RFO requests to L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of RFO (read for ownership) requests to L2 cache. L2 RFO requests include both L1D demand RFO misses as well as L1D RFO prefetches",
+},
+{
+	.name = "l2_rqsts.all_code_rd",
+	.event = "event=0x24,umask=0xe4,period=200003",
+	.desc = "L2 code requests",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of L2 code requests",
+},
+{
+	.name = "l2_rqsts.all_pf",
+	.event = "event=0x24,umask=0xf8,period=200003",
+	.desc = "Requests from L2 hardware prefetchers",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of requests from the L2 hardware prefetchers",
+},
+{
+	.name = "l2_demand_rqsts.wb_hit",
+	.event = "event=0x27,umask=0x50,period=200003",
+	.desc = "Not rejected writebacks that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of WB requests that hit L2 cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100003",
+	.desc = "Core-originated cacheable demand requests missed L3",
+	.topic = "cache",
+	.long_desc = "This event counts core-originated cacheable demand requests that miss the last level cache (LLC). Demand requests include loads, RFOs, and hardware prefetches from L1D, and instruction fetches from IFU",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=100003",
+	.desc = "Core-originated cacheable demand requests that refer to L3",
+	.topic = "cache",
+	.long_desc = "This event counts core-originated cacheable demand requests that refer to the last level cache (LLC). Demand requests include loads, RFOs, and hardware prefetches from L1D, and instruction fetches from IFU",
+},
+{
+	.name = "l1d_pend_miss.pending",
+	.event = "event=0x48,umask=0x1,period=2000003",
+	.desc = "L1D miss oustandings duration in cycles",
+	.topic = "cache",
+	.long_desc = "This event counts duration of L1D miss outstanding, that is each cycle number of Fill Buffers (FB) outstanding required by Demand Reads. FB either is held by demand loads, or it is held by non-demand loads and gets hit at least once by demand. The valid outstanding interval is defined until the FB deallocation by one of the following ways: from FB allocation, if FB is allocated by demand; from the demand Hit FB, if it is allocated by hardware or software prefetch.\nNote: In the L1D, a Demand Read contains cacheable or noncacheable demand loads, including ones causing cache-line splits and reads due to page walks resulted from any request type",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles",
+	.event = "event=0x48,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles with L1D load Misses outstanding",
+	.topic = "cache",
+	.long_desc = "This event counts duration of L1D miss outstanding in cycles",
+},
+{
+	.name = "l1d.replacement",
+	.event = "event=0x51,umask=0x1,period=2000003",
+	.desc = "L1D data line replacements",
+	.topic = "cache",
+	.long_desc = "This event counts L1D data line replacements including opportunistic replacements, and replacements that require stall-for-replace or block-for-replace",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003",
+	.desc = "Offcore outstanding Demand Data Read transactions in uncore queue  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding Demand Data Read transactions in the super queue (SQ) every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor. See the corresponding Umask under OFFCORE_REQUESTS.\nNote: A prefetch promoted to Demand is counted from the promotion point  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.demand_code_rd",
+	.event = "event=0x60,umask=0x2,period=2000003",
+	.desc = "Offcore outstanding code reads transactions in SuperQueue (SQ), queue to uncore, every cycle  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding Code Reads transactions in the super queue every cycle. The \"Offcore outstanding\" state of the transaction lasts from the L2 miss until the sending transaction completion to requestor (SQ deallocation). See the corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003",
+	.desc = "Offcore outstanding RFO store transactions in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding RFO (store) transactions in the super queue (SQ) every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.all_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003",
+	.desc = "Offcore outstanding cacheable Core Data Read transactions in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding cacheable Core Data Read transactions in the super queue every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_data_rd",
+	.event = "event=0x60,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts cycles when offcore outstanding Demand Data Read transactions are present in the super queue (SQ). A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation)  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_data_rd",
+	.event = "event=0x60,umask=0x8,cmask=1,period=2000003",
+	.desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts cycles when offcore outstanding cacheable Core Data Read transactions are present in the super queue. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_rfo",
+	.event = "event=0x60,umask=0x4,cmask=1,period=2000003",
+	.desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding demand rfo Reads transactions in the super queue every cycle. The \"Offcore outstanding\" state of the transaction lasts from the L2 miss until the sending transaction completion to requestor (SQ deallocation). See the corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "lock_cycles.cache_lock_duration",
+	.event = "event=0x63,umask=0x2,period=2000003",
+	.desc = "Cycles when L1D is locked",
+	.topic = "cache",
+	.long_desc = "This event counts the number of cycles when the L1D is locked. It is a superset of the 0x1 mask (BUS_LOCK_CLOCKS.BUS_LOCK_DURATION)",
+},
+{
+	.name = "offcore_requests.demand_data_rd",
+	.event = "event=0xB0,umask=0x1,period=100003",
+	.desc = "Demand Data Read requests sent to uncore",
+	.topic = "cache",
+	.long_desc = "This event counts the Demand Data Read requests sent to uncore. Use it in conjunction with OFFCORE_REQUESTS_OUTSTANDING to determine average latency in the uncore",
+},
+{
+	.name = "offcore_requests.demand_code_rd",
+	.event = "event=0xB0,umask=0x2,period=100003",
+	.desc = "Cacheable and noncachaeble code read requests",
+	.topic = "cache",
+	.long_desc = "This event counts both cacheable and noncachaeble code read requests",
+},
+{
+	.name = "offcore_requests.demand_rfo",
+	.event = "event=0xB0,umask=0x4,period=100003",
+	.desc = "Demand RFO requests including regular RFOs, locks, ItoM",
+	.topic = "cache",
+	.long_desc = "This event counts the demand RFO (read for ownership) requests including regular RFOs, locks, ItoM",
+},
+{
+	.name = "offcore_requests.all_data_rd",
+	.event = "event=0xB0,umask=0x8,period=100003",
+	.desc = "Demand and prefetch data reads",
+	.topic = "cache",
+	.long_desc = "This event counts the demand and prefetch data reads. All Core Data Reads include cacheable \"Demands\" and L2 prefetchers (not L3 prefetchers). Counting also covers reads due to page walks resulted from any request type",
+},
+{
+	.name = "offcore_requests_buffer.sq_full",
+	.event = "event=0xb2,umask=0x1,period=2000003",
+	.desc = "Offcore requests buffer cannot take more entries for this thread core",
+	.topic = "cache",
+	.long_desc = "This event counts the number of cases when the offcore requests buffer cannot take more entries for the core. This can happen when the superqueue does not contain eligible entries, or when L1D writeback pending FIFO requests is full.\nNote: Writeback pending FIFO has six entries",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_loads",
+	.event = "event=0xD0,umask=0x11,period=100003",
+	.desc = "Retired load uops that miss the STLB  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts load uops with true STLB miss retired to the architected path. True STLB miss is an uop triggering page walk that gets completed without blocks, and later gets retired. This page walk can end up with or without a fault  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_stores",
+	.event = "event=0xD0,umask=0x12,period=100003",
+	.desc = "Retired store uops that miss the STLB  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts store uops with true STLB miss retired to the architected path. True STLB miss is an uop triggering page walk that gets completed without blocks, and later gets retired. This page walk can end up with or without a fault  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.lock_loads",
+	.event = "event=0xD0,umask=0x21,period=100007",
+	.desc = "Retired load uops with locked access  Supports address when precise.  Spec update: BDM35 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts load uops with locked access retired to the architected path  Supports address when precise.  Spec update: BDM35 (Precise event)",
+},
+{
+	.name = "mem_uops_retired.split_loads",
+	.event = "event=0xD0,umask=0x41,period=100003",
+	.desc = "Retired load uops that split across a cacheline boundary  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts line-splitted load uops retired to the architected path. A line split is across 64B cache-line which includes a page split (4K)  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.split_stores",
+	.event = "event=0xD0,umask=0x42,period=100003",
+	.desc = "Retired store uops that split across a cacheline boundary  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts line-splitted store uops retired to the architected path. A line split is across 64B cache-line which includes a page split (4K)  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0xD0,umask=0x81,period=2000003",
+	.desc = "All retired load uops  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts load uops retired to the architected path with a filter on bits 0 and 1 applied.\nNote: This event counts AVX-256bit load/store double-pump memory uops as a single uop at retirement. This event also counts SW prefetches  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0xD0,umask=0x82,period=2000003",
+	.desc = "All retired store uops  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts store uops retired to the architected path with a filter on bits 0 and 1 applied.\nNote: This event counts AVX-256bit load/store double-pump memory uops as a single uop at retirement  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_hit",
+	.event = "event=0xD1,umask=0x1,period=2000003",
+	.desc = "Retired load uops with L1 cache hits as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were hits in the nearest-level (L1) cache.\nNote: Only two data-sources of L1/FB are applicable for AVX-256bit  even though the corresponding AVX load could be serviced by a deeper level in the memory hierarchy. Data source is reported for the Low-half load. This event also counts SW prefetches independent of the actual data source  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_hit",
+	.event = "event=0xD1,umask=0x2,period=100003",
+	.desc = "Retired load uops with L2 cache hits as data sources  Supports address when precise.  Spec update: BDM35 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were hits in the mid-level (L2) cache  Supports address when precise.  Spec update: BDM35 (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l3_hit",
+	.event = "event=0xD1,umask=0x4,period=50021",
+	.desc = "Retired load uops which data sources were data hits in L3 without snoops required  Supports address when precise.  Spec update: BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were data hits in the last-level (L3) cache without snoops required  Supports address when precise.  Spec update: BDM100 (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_miss",
+	.event = "event=0xD1,umask=0x8,period=100003",
+	.desc = "Retired load uops misses in L1 cache as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were misses in the nearest-level (L1) cache. Counting excludes unknown and UC data source  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_miss",
+	.event = "event=0xD1,umask=0x10,period=50021",
+	.desc = "Miss in mid-level (L2) cache. Excludes Unknown data-source  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were misses in the mid-level (L2) cache. Counting excludes unknown and UC data source  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l3_miss",
+	.event = "event=0xD1,umask=0x20,period=100007",
+	.desc = "Miss in last-level (L3) cache. Excludes Unknown data-source  Supports address when precise.  Spec update: BDM100, BDE70 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.hit_lfb",
+	.event = "event=0xD1,umask=0x40,period=100003",
+	.desc = "Retired load uops which data sources were load uops missed L1 but hit FB due to preceding miss to the same cache line with data not ready  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were load uops missed L1 but hit a fill buffer due to a preceding miss to the same cache line with the data not ready.\nNote: Only two data-sources of L1/FB are applicable for AVX-256bit  even though the corresponding AVX load could be serviced by a deeper level in the memory hierarchy. Data source is reported for the Low-half load  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_miss",
+	.event = "event=0xD2,umask=0x1,period=20011",
+	.desc = "Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache  Supports address when precise.  Spec update: BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were L3 Hit and a cross-core snoop missed in the on-pkg core cache  Supports address when precise.  Spec update: BDM100 (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_hit",
+	.event = "event=0xD2,umask=0x2,period=20011",
+	.desc = "Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache  Supports address when precise.  Spec update: BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were L3 hit and a cross-core snoop hit in the on-pkg core cache  Supports address when precise.  Spec update: BDM100 (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_hitm",
+	.event = "event=0xD2,umask=0x4,period=20011",
+	.desc = "Retired load uops which data sources were HitM responses from shared L3  Supports address when precise.  Spec update: BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were HitM responses from a core on same socket (shared L3)  Supports address when precise.  Spec update: BDM100 (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_none",
+	.event = "event=0xD2,umask=0x8,period=100003",
+	.desc = "Retired load uops which data sources were hits in L3 without snoops required  Supports address when precise.  Spec update: BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were hits in the last-level (L3) cache without snoops required  Supports address when precise.  Spec update: BDM100 (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_miss_retired.local_dram",
+	.event = "event=0xD3,umask=0x1,period=100007",
+	.desc = "Data from local DRAM either Snoop not needed or Snoop Miss (RspI)  Supports address when precise.  Spec update: BDE70, BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uop whose Data Source was: local DRAM either Snoop not needed or Snoop Miss (RspI)  Supports address when precise.  Spec update: BDE70, BDM100 (Precise event)",
+},
+{
+	.name = "l2_trans.demand_data_rd",
+	.event = "event=0xF0,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts Demand Data Read requests that access L2 cache, including rejects",
+},
+{
+	.name = "l2_trans.rfo",
+	.event = "event=0xF0,umask=0x2,period=200003",
+	.desc = "RFO requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts Read for Ownership (RFO) requests that access L2 cache",
+},
+{
+	.name = "l2_trans.code_rd",
+	.event = "event=0xF0,umask=0x4,period=200003",
+	.desc = "L2 cache accesses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache accesses when fetching instructions",
+},
+{
+	.name = "l2_trans.all_pf",
+	.event = "event=0xF0,umask=0x8,period=200003",
+	.desc = "L2 or L3 HW prefetches that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L2 or L3 HW prefetches that access L2 cache including rejects",
+},
+{
+	.name = "l2_trans.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200003",
+	.desc = "L1D writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L1D writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_fill",
+	.event = "event=0xF0,umask=0x20,period=200003",
+	.desc = "L2 fill requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L2 fill requests that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_wb",
+	.event = "event=0xF0,umask=0x40,period=200003",
+	.desc = "L2 writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L2 writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.all_requests",
+	.event = "event=0xF0,umask=0x80,period=200003",
+	.desc = "Transactions accessing L2 pipe",
+	.topic = "cache",
+	.long_desc = "This event counts transactions that access the L2 pipe including snoops, pagewalks, and so on",
+},
+{
+	.name = "l2_lines_in.i",
+	.event = "event=0xF1,umask=0x1,period=100003",
+	.desc = "L2 cache lines in I state filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines in the Invalidate state filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_in.s",
+	.event = "event=0xF1,umask=0x2,period=100003",
+	.desc = "L2 cache lines in S state filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines in the Shared state filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_in.e",
+	.event = "event=0xF1,umask=0x4,period=100003",
+	.desc = "L2 cache lines in E state filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines in the Exclusive state filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_in.all",
+	.event = "event=0xF1,umask=0x7,period=100003",
+	.desc = "L2 cache lines filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x5,period=100003",
+	.desc = "Clean L2 cache lines evicted by demand",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xf4,umask=0x10,period=100003",
+	.desc = "Split locks in SQ",
+	.topic = "cache",
+	.long_desc = "This event counts the number of split locks in the super queue",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x42,period=200003",
+	.desc = "RFO requests that hit L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x22,period=200003",
+	.desc = "RFO requests that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.code_rd_hit",
+	.event = "event=0x24,umask=0x44,period=200003",
+	.desc = "L2 cache hits when fetching instructions, code reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.code_rd_miss",
+	.event = "event=0x24,umask=0x24,period=200003",
+	.desc = "L2 cache misses when fetching instructions",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_demand_miss",
+	.event = "event=0x24,umask=0x27,period=200003",
+	.desc = "Demand requests that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_demand_references",
+	.event = "event=0x24,umask=0xe7,period=200003",
+	.desc = "Demand requests to L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.miss",
+	.event = "event=0x24,umask=0x3f,period=200003",
+	.desc = "All requests that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.references",
+	.event = "event=0x24,umask=0xff,period=200003",
+	.desc = "All L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response",
+	.event = "event=0xB7,umask=0x1,period=100003",
+	.desc = "Offcore response can be programmed only with a specific pair of event select and counter MSR, and with specific event codes and predefine mask bit value in a dedicated MSR to specify attributes of the offcore transaction",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd_ge_6",
+	.event = "event=0x60,umask=0x1,cmask=6,period=2000003",
+	.desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue  Spec update: BDM76",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles_any",
+	.event = "event=0x48,umask=0x1,any=1,cmask=1,period=2000003",
+	.desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.fb_full",
+	.event = "event=0x48,umask=0x2,cmask=1,period=2000003",
+	.desc = "Cycles a demand request was blocked due to Fill Buffers inavailability",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_haswellx[] = {
+{
+	.name = "other_assists.avx_to_sse",
+	.event = "event=0xC1,umask=0x8,period=100003",
+	.desc = "Number of transitions from AVX-256 to legacy SSE when penalty applicable  Spec update: HSD56, HSM57",
+	.topic = "floating point",
+},
+{
+	.name = "other_assists.sse_to_avx",
+	.event = "event=0xC1,umask=0x10,period=100003",
+	.desc = "Number of transitions from SSE to AVX-256 when penalty applicable  Spec update: HSD56, HSM57",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.x87_output",
+	.event = "event=0xCA,umask=0x2,period=100003",
+	.desc = "Number of X87 assists due to output value",
+	.topic = "floating point",
+	.long_desc = "Number of X87 FP assists due to output values",
+},
+{
+	.name = "fp_assist.x87_input",
+	.event = "event=0xCA,umask=0x4,period=100003",
+	.desc = "Number of X87 assists due to input value",
+	.topic = "floating point",
+	.long_desc = "Number of X87 FP assists due to input values",
+},
+{
+	.name = "fp_assist.simd_output",
+	.event = "event=0xCA,umask=0x8,period=100003",
+	.desc = "Number of SIMD FP assists due to Output values",
+	.topic = "floating point",
+	.long_desc = "Number of SIMD FP assists due to output values",
+},
+{
+	.name = "fp_assist.simd_input",
+	.event = "event=0xCA,umask=0x10,period=100003",
+	.desc = "Number of SIMD FP assists due to input values",
+	.topic = "floating point",
+	.long_desc = "Number of SIMD FP assists due to input values",
+},
+{
+	.name = "fp_assist.any",
+	.event = "event=0xCA,umask=0x1e,cmask=1,period=100003",
+	.desc = "Cycles with any input/output SSE or FP assist",
+	.topic = "floating point",
+	.long_desc = "Cycles with any input/output SSE* or FP assists",
+},
+{
+	.name = "avx_insts.all",
+	.event = "event=0xC6,umask=0x7,period=2000003",
+	.desc = "Approximate counts of AVX & AVX2 256-bit instructions, including non-arithmetic instructions, loads, and stores.  May count non-AVX instructions that employ 256-bit operations, including (but not necessarily limited to) rep string instructions that use 256-bit loads and stores for optimized performance, XSAVE* and XRSTOR*, and operations that transition the x87 FPU data registers between x87 and MMX",
+	.topic = "floating point",
+	.long_desc = "Note that a whole rep string only counts AVX_INST.ALL once",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired from execution  Spec update: HSD140, HSD143",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions retired from execution. For instructions that consist of multiple micro-ops, this event counts the retirement of the last micro-op of the instruction. Counting continues during hardware interrupts, traps, and inside interrupt handlers. INST_RETIRED.ANY is counted by a designated fixed counter, leaving the programmable counters available for other events. Faulting executions of GETSEC/VM entry/VM Exit/MWait will not count as retired instructions  Spec update: HSD140, HSD143",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Core cycles when the thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of thread cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. The core frequency may change from time to time due to power or thermal throttling",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Reference cycles when the core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of reference cycles when the core is not in a halt state. The core enters the halt state when it is running the HLT instruction or the MWAIT instruction. This event is not affected by core frequency changes (for example, P states, TM2 transitions) but has the same incrementing frequency as the time stamp counter. This event can approximate elapsed time while the core was not in a halt state",
+},
+{
+	.name = "ld_blocks.store_forward",
+	.event = "event=0x03,umask=0x2,period=100003",
+	.desc = "loads blocked by overlapping with store buffer that cannot be forwarded",
+	.topic = "pipeline",
+	.long_desc = "This event counts loads that followed a store to the same address, where the data could not be forwarded inside the pipeline from the store to the load.  The most common reason why store forwarding would be blocked is when a load's address range overlaps with a preceding smaller uncompleted store. The penalty for blocked store forwarding is that the load must wait for the store to write its value to the cache before it can be issued",
+},
+{
+	.name = "ld_blocks.no_sr",
+	.event = "event=0x03,umask=0x8,period=100003",
+	.desc = "The number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+	.topic = "pipeline",
+	.long_desc = "The number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+},
+{
+	.name = "ld_blocks_partial.address_alias",
+	.event = "event=0x07,umask=0x1,period=100003",
+	.desc = "False dependencies in MOB due to partial compare on address",
+	.topic = "pipeline",
+	.long_desc = "Aliasing occurs when a load is issued after a store and their memory addresses are offset by 4K.  This event counts the number of loads that aliased with a preceding store, resulting in an extended address check in the pipeline which can have a performance impact",
+},
+{
+	.name = "int_misc.recovery_cycles",
+	.event = "event=0x0D,umask=0x3,cmask=1,period=2000003",
+	.desc = "Number of cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc...)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of cycles spent waiting for a recovery after an event such as a processor nuke, JEClear, assist, hle/rtm abort etc",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0x0E,umask=0x1,period=2000003",
+	.desc = "Uops that Resource Allocation Table (RAT) issues to Reservation Station (RS)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of uops issued by the Front-end of the pipeline to the Back-end. This event is counted at the allocation stage and will count both retired and non-retired uops",
+},
+{
+	.name = "uops_issued.flags_merge",
+	.event = "event=0x0E,umask=0x10,period=2000003",
+	.desc = "Number of flags-merge uops being allocated. Such uops considered perf sensitive; added by GSR u-arch",
+	.topic = "pipeline",
+	.long_desc = "Number of flags-merge uops allocated. Such uops add delay",
+},
+{
+	.name = "uops_issued.slow_lea",
+	.event = "event=0x0E,umask=0x20,period=2000003",
+	.desc = "Number of slow LEA uops being allocated. A uop is generally considered SlowLea if it has 3 sources (e.g. 2 sources + immediate) regardless if as a result of LEA instruction or not",
+	.topic = "pipeline",
+	.long_desc = "Number of slow LEA or similar uops allocated. Such uop has 3 sources (for example, 2 sources + immediate) regardless of whether it is a result of LEA instruction or not",
+},
+{
+	.name = "uops_issued.single_mul",
+	.event = "event=0x0E,umask=0x40,period=2000003",
+	.desc = "Number of Multiply packed/scalar single precision uops allocated",
+	.topic = "pipeline",
+	.long_desc = "Number of multiply packed/scalar single precision uops allocated",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "inv=1,event=0x0E,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.core_stall_cycles",
+	.event = "inv=1,event=0x0E,umask=0x1,any=1,cmask=1,period=2000003",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for all threads",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.divider_uops",
+	.event = "event=0x14,umask=0x2,period=2000003",
+	.desc = "Any uop executed by the Divider. (This includes all divide uops, sqrt, ...)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Increments at the frequency of XCLK (100 MHz) when not halted",
+},
+{
+	.name = "cpu_clk_thread_unhalted.one_thread_active",
+	.event = "event=0x3c,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.sw_pf",
+	.event = "event=0x4c,umask=0x1,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for software prefetch",
+	.topic = "pipeline",
+	.long_desc = "Non-SW-prefetch load dispatches that hit fill buffer allocated for S/W prefetch",
+},
+{
+	.name = "load_hit_pre.hw_pf",
+	.event = "event=0x4c,umask=0x2,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for hardware prefetch",
+	.topic = "pipeline",
+	.long_desc = "Non-SW-prefetch load dispatches that hit fill buffer allocated for H/W prefetch",
+},
+{
+	.name = "move_elimination.int_eliminated",
+	.event = "event=0x58,umask=0x1,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+	.long_desc = "Number of integer move elimination candidate uops that were eliminated",
+},
+{
+	.name = "move_elimination.simd_eliminated",
+	.event = "event=0x58,umask=0x2,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+	.long_desc = "Number of SIMD move elimination candidate uops that were eliminated",
+},
+{
+	.name = "move_elimination.int_not_eliminated",
+	.event = "event=0x58,umask=0x4,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+	.long_desc = "Number of integer move elimination candidate uops that were not eliminated",
+},
+{
+	.name = "move_elimination.simd_not_eliminated",
+	.event = "event=0x58,umask=0x8,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+	.long_desc = "Number of SIMD move elimination candidate uops that were not eliminated",
+},
+{
+	.name = "rs_events.empty_cycles",
+	.event = "event=0x5E,umask=0x1,period=2000003",
+	.desc = "Cycles when Reservation Station (RS) is empty for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles when the Reservation Station ( RS ) is empty for the thread. The RS is a structure that buffers allocated micro-ops from the Front-end. If there are many cycles when the RS is empty, it may represent an underflow of instructions delivered from the Front-end",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000003",
+	.desc = "Stalls caused by changing prefix length of the instruction",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles where the decoder is stalled on an instruction with a length changing prefix (LCP)",
+},
+{
+	.name = "ild_stall.iq_full",
+	.event = "event=0x87,umask=0x4,period=2000003",
+	.desc = "Stall cycles because IQ is full",
+	.topic = "pipeline",
+	.long_desc = "Stall cycles due to IQ is full",
+},
+{
+	.name = "br_inst_exec.nontaken_conditional",
+	.event = "event=0x88,umask=0x41,period=200003",
+	.desc = "Not taken macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_conditional",
+	.event = "event=0x88,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_direct_jump",
+	.event = "event=0x88,umask=0x82,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branch instructions excluding calls and indirects",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_return",
+	.event = "event=0x88,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired indirect branches with return mnemonic",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_direct_near_call",
+	.event = "event=0x88,umask=0x90,period=200003",
+	.desc = "Taken speculative and retired direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_call",
+	.event = "event=0x88,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_conditional",
+	.event = "event=0x88,umask=0xc1,period=200003",
+	.desc = "Speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_direct_jmp",
+	.event = "event=0x88,umask=0xc2,period=200003",
+	.desc = "Speculative and retired macro-unconditional branches excluding calls and indirects",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0xc4,period=200003",
+	.desc = "Speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_indirect_near_return",
+	.event = "event=0x88,umask=0xc8,period=200003",
+	.desc = "Speculative and retired indirect return branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_direct_near_call",
+	.event = "event=0x88,umask=0xd0,period=200003",
+	.desc = "Speculative and retired direct near calls",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_branches",
+	.event = "event=0x88,umask=0xff,period=200003",
+	.desc = "Speculative and retired  branches",
+	.topic = "pipeline",
+	.long_desc = "Counts all near executed branches (not necessarily retired)",
+},
+{
+	.name = "br_misp_exec.nontaken_conditional",
+	.event = "event=0x89,umask=0x41,period=200003",
+	.desc = "Not taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_conditional",
+	.event = "event=0x89,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken_return_near",
+	.event = "event=0x89,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches with return mnemonic",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_conditional",
+	.event = "event=0x89,umask=0xc1,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0xc4,period=200003",
+	.desc = "Mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.all_branches",
+	.event = "event=0x89,umask=0xff,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Counts all near executed branches (not necessarily retired)",
+},
+{
+	.name = "uops_executed_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 0",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 0 in this thread",
+},
+{
+	.name = "uops_executed_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 1",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 1 in this thread",
+},
+{
+	.name = "uops_executed_port.port_2",
+	.event = "event=0xA1,umask=0x4,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 2",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 2 in this thread",
+},
+{
+	.name = "uops_executed_port.port_3",
+	.event = "event=0xA1,umask=0x8,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 3",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 3 in this thread",
+},
+{
+	.name = "uops_executed_port.port_4",
+	.event = "event=0xA1,umask=0x10,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 4",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 4 in this thread",
+},
+{
+	.name = "uops_executed_port.port_5",
+	.event = "event=0xA1,umask=0x20,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 5",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 5 in this thread",
+},
+{
+	.name = "uops_executed_port.port_6",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 6",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 6 in this thread",
+},
+{
+	.name = "uops_executed_port.port_7",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 7",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a uop is dispatched on port 7 in this thread",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000003",
+	.desc = "Resource-related stall cycles  Spec update: HSD135",
+	.topic = "pipeline",
+	.long_desc = "Cycles allocation is stalled due to resource related reason  Spec update: HSD135",
+},
+{
+	.name = "resource_stalls.rs",
+	.event = "event=0xA2,umask=0x4,period=2000003",
+	.desc = "Cycles stalled due to no eligible RS entry available",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.sb",
+	.event = "event=0xA2,umask=0x8,period=2000003",
+	.desc = "Cycles stalled due to no store buffers available. (not including draining form sync)",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which no instructions were allocated because no Store Buffers (SB) were available",
+},
+{
+	.name = "resource_stalls.rob",
+	.event = "event=0xA2,umask=0x10,period=2000003",
+	.desc = "Cycles stalled due to re-order buffer full",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l2_pending",
+	.event = "event=0xA3,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles with pending L2 cache miss loads  Spec update: HSD78",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending L2 miss loads. Set Cmask=2 to count cycle  Spec update: HSD78",
+},
+{
+	.name = "cycle_activity.cycles_l1d_pending",
+	.event = "event=0xA3,umask=0x8,cmask=8,period=2000003",
+	.desc = "Cycles with pending L1 cache miss loads",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending L1 data cache miss loads. Set Cmask=8 to count cycle",
+},
+{
+	.name = "cycle_activity.cycles_ldm_pending",
+	.event = "event=0xA3,umask=0x2,cmask=2,period=2000003",
+	.desc = "Cycles with pending memory loads",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending memory loads. Set Cmask=2 to count cycle",
+},
+{
+	.name = "cycle_activity.cycles_no_execute",
+	.event = "event=0xA3,umask=0x4,cmask=4,period=2000003",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which no instructions were executed in the execution stage of the pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l2_pending",
+	.event = "event=0xA3,umask=0x5,cmask=5,period=2000003",
+	.desc = "Execution stalls due to L2 cache misses",
+	.topic = "pipeline",
+	.long_desc = "Number of loads missed L2",
+},
+{
+	.name = "cycle_activity.stalls_ldm_pending",
+	.event = "event=0xA3,umask=0x6,cmask=6,period=2000003",
+	.desc = "Execution stalls due to memory subsystem",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which no instructions were executed in the execution stage of the pipeline and there were memory instructions pending (waiting for data)",
+},
+{
+	.name = "cycle_activity.stalls_l1d_pending",
+	.event = "event=0xA3,umask=0xc,cmask=12,period=2000003",
+	.desc = "Execution stalls due to L1 data cache misses",
+	.topic = "pipeline",
+	.long_desc = "Execution stalls due to L1 data cache miss loads. Set Cmask=0CH",
+},
+{
+	.name = "lsd.uops",
+	.event = "event=0xa8,umask=0x1,period=2000003",
+	.desc = "Number of Uops delivered by the LSD",
+	.topic = "pipeline",
+	.long_desc = "Number of uops delivered by the LSD",
+},
+{
+	.name = "uops_executed.core",
+	.event = "event=0xB1,umask=0x2,period=2000003",
+	.desc = "Number of uops executed on the core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+	.long_desc = "Counts total number of uops to be executed per-core each cycle  Spec update: HSD30, HSM31",
+},
+{
+	.name = "uops_executed.stall_cycles",
+	.event = "inv=1,event=0xB1,umask=0x1,cmask=1,period=2000003",
+	.desc = "Counts number of cycles no uops were dispatched to be executed on this thread  Spec update: HSD144, HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Number of instructions retired. General Counter   - architectural event  Spec update: HSD11, HSD140",
+	.topic = "pipeline",
+	.long_desc = "Number of instructions at retirement  Spec update: HSD11, HSD140",
+},
+{
+	.name = "inst_retired.x87",
+	.event = "event=0xC0,umask=0x2,period=2000003",
+	.desc = "FP operations retired. X87 FP operations that have no exceptions: Counts also flows that have several X87 or flows that use X87 uops in the exception handling",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts FP operations retired. For X87 FP operations that have no exceptions counting also includes flows that have several X87, or flows that use X87 uops in the exception handling",
+},
+{
+	.name = "inst_retired.prec_dist",
+	.event = "event=0xC0,umask=0x1,period=2000003",
+	.desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution  Spec update: HSD140 (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution  Spec update: HSD140 (Must be precise)",
+},
+{
+	.name = "other_assists.any_wb_assist",
+	.event = "event=0xC1,umask=0x40,period=100003",
+	.desc = "Number of times any microcode assist is invoked by HW upon uop writeback",
+	.topic = "pipeline",
+	.long_desc = "Number of microcode assists invoked by HW upon uop writeback",
+},
+{
+	.name = "uops_retired.all",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "Actually retired uops  Supports address when precise (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of micro-ops retired. Use Cmask=1 and invert to count active cycles or stalled cycles  Supports address when precise (Precise event)",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000003",
+	.desc = "Retirement slots used (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of retirement slots used each cycle.  There are potentially 4 slots that can be used each cycle - meaning, 4 uops or 4 instructions could retire each cycle (Precise event)",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "inv=1,event=0xC2,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "inv=1,event=0xC2,umask=0x1,cmask=10,period=2000003",
+	.desc = "Cycles with less than 10 actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.core_stall_cycles",
+	.event = "inv=1,event=0xC2,umask=0x1,any=1,cmask=1,period=2000003",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.cycles",
+	.event = "event=0xC3,umask=0x1,period=2000003",
+	.desc = "Cycles there was a Nuke. Account for both thread-specific and All Thread Nukes",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=100003",
+	.desc = "Self-modifying code (SMC) detected",
+	.topic = "pipeline",
+	.long_desc = "This event is incremented when self-modifying code (SMC) is detected, which causes a machine clear.  Machine clears can have a significant performance impact if they are happening frequently",
+},
+{
+	.name = "machine_clears.maskmov",
+	.event = "event=0xC3,umask=0x20,period=100003",
+	.desc = "This event counts the number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=400009",
+	.desc = "Conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=100003",
+	.desc = "Direct and indirect near call instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=400009",
+	.desc = "All (macro) branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Branch instructions at retirement",
+},
+{
+	.name = "br_inst_retired.near_return",
+	.event = "event=0xC4,umask=0x8,period=100003",
+	.desc = "Return instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of near return instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.not_taken",
+	.event = "event=0xC4,umask=0x10,period=400009",
+	.desc = "Not taken branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of not taken branch instructions retired",
+},
+{
+	.name = "br_inst_retired.near_taken",
+	.event = "event=0xC4,umask=0x20,period=400009",
+	.desc = "Taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Number of near taken branches retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0x40,period=100003",
+	.desc = "Far branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Number of far branches retired",
+},
+{
+	.name = "br_inst_retired.all_branches_pebs",
+	.event = "event=0xC4,umask=0x4,period=400009",
+	.desc = "All (macro) branch instructions retired (Must be precise)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=400009",
+	.desc = "Mispredicted conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=400009",
+	.desc = "All mispredicted macro branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted branch instructions at retirement",
+},
+{
+	.name = "br_misp_retired.all_branches_pebs",
+	.event = "event=0xC5,umask=0x4,period=400009",
+	.desc = "Mispredicted macro branch instructions retired (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This event counts all mispredicted branch instructions retired. This is a precise event (Must be precise)",
+},
+{
+	.name = "rob_misc_events.lbr_inserts",
+	.event = "event=0xCC,umask=0x20,period=2000003",
+	.desc = "Count cases of saving new LBR",
+	.topic = "pipeline",
+	.long_desc = "Count cases of saving new LBR records by hardware",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Thread cycles when thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of thread cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. The core frequency may change from time to time due to power or thermal throttling",
+},
+{
+	.name = "br_misp_exec.taken_indirect_near_call",
+	.event = "event=0x89,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_0_core",
+	.event = "event=0xA1,umask=0x1,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_1_core",
+	.event = "event=0xA1,umask=0x2,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_2_core",
+	.event = "event=0xA1,umask=0x4,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 2",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_3_core",
+	.event = "event=0xA1,umask=0x8,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 3",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_4_core",
+	.event = "event=0xA1,umask=0x10,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_5_core",
+	.event = "event=0xA1,umask=0x20,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_6_core",
+	.event = "event=0xA1,umask=0x40,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 6",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_7_core",
+	.event = "event=0xA1,umask=0x80,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 7",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_taken",
+	.event = "event=0xC5,umask=0x20,period=400009",
+	.desc = "number of near branch instructions retired that were mispredicted and taken (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Number of near branch instructions retired that were taken but mispredicted (Precise event)",
+},
+{
+	.name = "uops_executed.cycles_ge_1_uop_exec",
+	.event = "event=0xB1,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles where at least 1 uop was executed per-thread  Spec update: HSD144, HSD30, HSM31",
+	.topic = "pipeline",
+	.long_desc = "This events counts the cycles where at least one uop was executed. It is counted per thread  Spec update: HSD144, HSD30, HSM31",
+},
+{
+	.name = "uops_executed.cycles_ge_2_uops_exec",
+	.event = "event=0xB1,umask=0x1,cmask=2,period=2000003",
+	.desc = "Cycles where at least 2 uops were executed per-thread  Spec update: HSD144, HSD30, HSM31",
+	.topic = "pipeline",
+	.long_desc = "This events counts the cycles where at least two uop were executed. It is counted per thread  Spec update: HSD144, HSD30, HSM31",
+},
+{
+	.name = "uops_executed.cycles_ge_3_uops_exec",
+	.event = "event=0xB1,umask=0x1,cmask=3,period=2000003",
+	.desc = "Cycles where at least 3 uops were executed per-thread  Spec update: HSD144, HSD30, HSM31",
+	.topic = "pipeline",
+	.long_desc = "This events counts the cycles where at least three uop were executed. It is counted per thread  Spec update: HSD144, HSD30, HSM31",
+},
+{
+	.name = "uops_executed.cycles_ge_4_uops_exec",
+	.event = "event=0xB1,umask=0x1,cmask=4,period=2000003",
+	.desc = "Cycles where at least 4 uops were executed per-thread  Spec update: HSD144, HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "baclears.any",
+	.event = "event=0xe6,umask=0x1f,period=100003",
+	.desc = "Counts the total number when the front end is resteered, mainly when the BPU cannot provide a correct prediction and this is corrected by other branch handling mechanisms at the front end",
+	.topic = "pipeline",
+	.long_desc = "Number of front end re-steers due to BPU misprediction",
+},
+{
+	.name = "machine_clears.count",
+	.event = "edge=1,event=0xC3,umask=0x1,cmask=1,period=100003",
+	.desc = "Number of machine clears (nukes) of any type",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_active",
+	.event = "event=0xA8,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_4_uops",
+	.event = "event=0xA8,umask=0x1,cmask=4,period=2000003",
+	.desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_end",
+	.event = "edge=1,inv=1,event=0x5E,umask=0x1,cmask=1,period=200003",
+	.desc = "Counts end of periods where the Reservation Station (RS) was empty. Could be useful to precisely locate Frontend Latency Bound issues",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_2",
+	.event = "event=0xA1,umask=0x4,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 2",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_3",
+	.event = "event=0xA1,umask=0x8,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 3",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_4",
+	.event = "event=0xA1,umask=0x10,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_5",
+	.event = "event=0xA1,umask=0x20,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_6",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 6",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_7",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 7",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_any",
+	.event = "event=0x3c,any=1",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p_any",
+	.event = "event=0x3C,umask=0x0,any=1,period=2000003",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+},
+{
+	.name = "int_misc.recovery_cycles_any",
+	.event = "event=0x0D,umask=0x3,any=1,cmask=1,period=2000003",
+	.desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+	.topic = "pipeline",
+	.long_desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+},
+{
+	.name = "uops_executed.core_cycles_ge_1",
+	.event = "event=0xb1,umask=0x2,cmask=1,period=2000003",
+	.desc = "Cycles at least 1 micro-op is executed from any thread on physical core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_2",
+	.event = "event=0xb1,umask=0x2,cmask=2,period=2000003",
+	.desc = "Cycles at least 2 micro-op is executed from any thread on physical core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_3",
+	.event = "event=0xb1,umask=0x2,cmask=3,period=2000003",
+	.desc = "Cycles at least 3 micro-op is executed from any thread on physical core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_4",
+	.event = "event=0xb1,umask=0x2,cmask=4,period=2000003",
+	.desc = "Cycles at least 4 micro-op is executed from any thread on physical core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_none",
+	.event = "inv=1,event=0xb1,umask=0x2,period=2000003",
+	.desc = "Cycles with no micro-ops executed from any thread on physical core  Spec update: HSD30, HSM31",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the thread is unhalted. (counts at 100 MHz rate)",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+},
+{
+	.name = "cpu_clk_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "idq.empty",
+	.event = "event=0x79,umask=0x2,period=2000003",
+	.desc = "Instruction Decode Queue (IDQ) empty cycles  Spec update: HSD135",
+	.topic = "frontend",
+	.long_desc = "Counts cycles the IDQ is empty  Spec update: HSD135",
+},
+{
+	.name = "idq.mite_uops",
+	.event = "event=0x79,umask=0x4,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ from MITE path. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.dsb_uops",
+	.event = "event=0x79,umask=0x8,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle. # of uops delivered to IDQ from DSB path. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.ms_dsb_uops",
+	.event = "event=0x79,umask=0x10,period=2000003",
+	.desc = "Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ when MS_busy by DSB. Set Cmask = 1 to count cycles. Add Edge=1 to count # of delivery",
+},
+{
+	.name = "idq.ms_mite_uops",
+	.event = "event=0x79,umask=0x20,period=2000003",
+	.desc = "Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ when MS_busy by MITE. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.ms_uops",
+	.event = "event=0x79,umask=0x30,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts uops delivered by the Front-end with the assistance of the microcode sequencer.  Microcode assists are used for complex instructions or scenarios that can't be handled by the standard decoder.  Using other instructions, if possible, will usually improve performance",
+},
+{
+	.name = "idq.ms_cycles",
+	.event = "event=0x79,umask=0x30,cmask=1,period=2000003",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which the microcode sequencer assisted the Front-end in delivering uops.  Microcode assists are used for complex instructions or scenarios that can't be handled by the standard decoder.  Using other instructions, if possible, will usually improve performance",
+},
+{
+	.name = "idq.mite_cycles",
+	.event = "event=0x79,umask=0x4,cmask=1,period=2000003",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.dsb_cycles",
+	.event = "event=0x79,umask=0x8,cmask=1,period=2000003",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_dsb_cycles",
+	.event = "event=0x79,umask=0x10,cmask=1,period=2000003",
+	.desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_dsb_occur",
+	.event = "edge=1,event=0x79,umask=0x10,cmask=1,period=2000003",
+	.desc = "Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+},
+{
+	.name = "idq.all_dsb_cycles_4_uops",
+	.event = "event=0x79,umask=0x18,cmask=4,period=2000003",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "Counts cycles DSB is delivered four uops. Set Cmask = 4",
+},
+{
+	.name = "idq.all_dsb_cycles_any_uops",
+	.event = "event=0x79,umask=0x18,cmask=1,period=2000003",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "Counts cycles DSB is delivered at least one uops. Set Cmask = 1",
+},
+{
+	.name = "idq.all_mite_cycles_4_uops",
+	.event = "event=0x79,umask=0x24,cmask=4,period=2000003",
+	.desc = "Cycles MITE is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "Counts cycles MITE is delivered four uops. Set Cmask = 4",
+},
+{
+	.name = "idq.all_mite_cycles_any_uops",
+	.event = "event=0x79,umask=0x24,cmask=1,period=2000003",
+	.desc = "Cycles MITE is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "Counts cycles MITE is delivered at least one uop. Set Cmask = 1",
+},
+{
+	.name = "idq.mite_all_uops",
+	.event = "event=0x79,umask=0x3c,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "Number of uops delivered to IDQ from any path",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=2000003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Reads. both cacheable and noncacheable, including UC fetches",
+	.topic = "frontend",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Misses. Includes Uncacheable accesses",
+	.topic = "frontend",
+	.long_desc = "This event counts Instruction Cache (ICACHE) misses",
+},
+{
+	.name = "icache.ifetch_stall",
+	.event = "event=0x80,umask=0x4,period=2000003",
+	.desc = "Cycles where a code fetch is stalled due to L1 instruction-cache miss",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.core",
+	.event = "event=0x9C,umask=0x1,period=2000003",
+	.desc = "Uops not delivered to Resource Allocation Table (RAT) per thread when backend of the machine is not stalled  Spec update: HSD135",
+	.topic = "frontend",
+	.long_desc = "This event count the number of undelivered (unallocated) uops from the Front-end to the Resource Allocation Table (RAT) while the Back-end of the processor is not stalled. The Front-end can allocate up to 4 uops per cycle so this event can increment 0-4 times per cycle depending on the number of unallocated uops. This event is counted on a per-core basis  Spec update: HSD135",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_0_uops_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=4,period=2000003",
+	.desc = "Cycles per thread when 4 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled  Spec update: HSD135",
+	.topic = "frontend",
+	.long_desc = "This event counts the number cycles during which the Front-end allocated exactly zero uops to the Resource Allocation Table (RAT) while the Back-end of the processor is not stalled.  This event is counted on a per-core basis  Spec update: HSD135",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_1_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=3,period=2000003",
+	.desc = "Cycles per thread when 3 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled  Spec update: HSD135",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_2_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=2,period=2000003",
+	.desc = "Cycles with less than 2 uops delivered by the front end  Spec update: HSD135",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_3_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles with less than 3 uops delivered by the front end  Spec update: HSD135",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_fe_was_ok",
+	.event = "inv=1,event=0x9C,umask=0x1,cmask=1,period=2000003",
+	.desc = "Counts cycles FE delivered 4 uops or Resource Allocation Table (RAT) was stalling FE  Spec update: HSD135",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.penalty_cycles",
+	.event = "event=0xAB,umask=0x2,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_switches",
+	.event = "edge=1,event=0x79,umask=0x30,cmask=1,period=2000003",
+	.desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+	.topic = "frontend",
+},
+{
+	.name = "icache.ifdata_stall",
+	.event = "event=0x80,umask=0x4,period=2000003",
+	.desc = "Cycles where a code fetch is stalled due to L1 instruction-cache miss",
+	.topic = "frontend",
+},
+{
+	.name = "cpl_cycles.ring0",
+	.event = "event=0x5C,umask=0x1,period=2000003",
+	.desc = "Unhalted core cycles when the thread is in ring 0",
+	.topic = "other",
+	.long_desc = "Unhalted core cycles when the thread is in ring 0",
+},
+{
+	.name = "cpl_cycles.ring123",
+	.event = "event=0x5C,umask=0x2,period=2000003",
+	.desc = "Unhalted core cycles when thread is in rings 1, 2, or 3",
+	.topic = "other",
+	.long_desc = "Unhalted core cycles when the thread is not in ring 0",
+},
+{
+	.name = "cpl_cycles.ring0_trans",
+	.event = "edge=1,event=0x5C,umask=0x1,cmask=1,period=100003",
+	.desc = "Number of intervals between processor halts while thread is in ring 0",
+	.topic = "other",
+},
+{
+	.name = "lock_cycles.split_lock_uc_lock_duration",
+	.event = "event=0x63,umask=0x1,period=2000003",
+	.desc = "Cycles when L1 and L2 are locked due to UC or split lock",
+	.topic = "other",
+	.long_desc = "Cycles in which the L1D and L2 are locked, due to a UC lock or split lock",
+},
+{
+	.name = "misalign_mem_ref.loads",
+	.event = "event=0x05,umask=0x1,period=2000003",
+	.desc = "Speculative cache line split load uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "Speculative cache-line split load uops dispatched to L1D",
+},
+{
+	.name = "misalign_mem_ref.stores",
+	.event = "event=0x05,umask=0x2,period=2000003",
+	.desc = "Speculative cache line split STA uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "Speculative cache-line split store-address uops dispatched to L1D",
+},
+{
+	.name = "tx_mem.abort_conflict",
+	.event = "event=0x54,umask=0x1,period=2000003",
+	.desc = "Number of times a transactional abort was signaled due to a data conflict on a transactionally accessed address",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.abort_capacity_write",
+	.event = "event=0x54,umask=0x2,period=2000003",
+	.desc = "Number of times a transactional abort was signaled due to a data capacity limitation for transactional writes",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.abort_hle_store_to_elided_lock",
+	.event = "event=0x54,umask=0x4,period=2000003",
+	.desc = "Number of times a HLE transactional region aborted due to a non XRELEASE prefixed instruction writing to an elided lock in the elision buffer",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_not_empty",
+	.event = "event=0x54,umask=0x8,period=2000003",
+	.desc = "Number of times an HLE transactional execution aborted due to NoAllocatedElisionBuffer being non-zero",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_mismatch",
+	.event = "event=0x54,umask=0x10,period=2000003",
+	.desc = "Number of times an HLE transactional execution aborted due to XRELEASE lock not satisfying the address and value requirements in the elision buffer",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_unsupported_alignment",
+	.event = "event=0x54,umask=0x20,period=2000003",
+	.desc = "Number of times an HLE transactional execution aborted due to an unsupported read alignment from the elision buffer",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.hle_elision_buffer_full",
+	.event = "event=0x54,umask=0x40,period=2000003",
+	.desc = "Number of times HLE lock could not be elided due to ElisionBufferAvailable being zero",
+	.topic = "memory",
+},
+{
+	.name = "tx_exec.misc1",
+	.event = "event=0x5d,umask=0x1,period=2000003",
+	.desc = "Counts the number of times a class of instructions that may cause a transactional abort was executed. Since this is the count of execution, it may not always cause a transactional abort",
+	.topic = "memory",
+},
+{
+	.name = "tx_exec.misc2",
+	.event = "event=0x5d,umask=0x2,period=2000003",
+	.desc = "Counts the number of times a class of instructions (e.g., vzeroupper) that may cause a transactional abort was executed inside a transactional region",
+	.topic = "memory",
+},
+{
+	.name = "tx_exec.misc3",
+	.event = "event=0x5d,umask=0x4,period=2000003",
+	.desc = "Counts the number of times an instruction execution caused the transactional nest count supported to be exceeded",
+	.topic = "memory",
+},
+{
+	.name = "tx_exec.misc4",
+	.event = "event=0x5d,umask=0x8,period=2000003",
+	.desc = "Counts the number of times a XBEGIN instruction was executed inside an HLE transactional region",
+	.topic = "memory",
+},
+{
+	.name = "tx_exec.misc5",
+	.event = "event=0x5d,umask=0x10,period=2000003",
+	.desc = "Counts the number of times an HLE XACQUIRE instruction was executed inside an RTM transactional region",
+	.topic = "memory",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=100003",
+	.desc = "Counts the number of machine clears due to memory order conflicts",
+	.topic = "memory",
+	.long_desc = "This event counts the number of memory ordering machine clears detected. Memory ordering machine clears can result from memory address aliasing or snoops from another hardware thread or core to data inflight in the pipeline.  Machine clears can have a significant performance impact if they are happening frequently",
+},
+{
+	.name = "hle_retired.start",
+	.event = "event=0xC8,umask=0x1,period=2000003",
+	.desc = "Number of times an HLE execution started",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.commit",
+	.event = "event=0xc8,umask=0x2,period=2000003",
+	.desc = "Number of times an HLE execution successfully committed",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted",
+	.event = "event=0xc8,umask=0x4,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to any reasons (multiple categories may count as one) (Precise event)",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_misc1",
+	.event = "event=0xc8,umask=0x8,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to various memory events (e.g., read/write capacity and conflicts)",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_misc2",
+	.event = "event=0xc8,umask=0x10,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to uncommon conditions",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_misc3",
+	.event = "event=0xc8,umask=0x20,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to HLE-unfriendly instructions",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_misc4",
+	.event = "event=0xc8,umask=0x40,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to incompatible memory type  Spec update: HSD65",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_misc5",
+	.event = "event=0xc8,umask=0x80,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to none of the previous 4 categories (e.g. interrupts)",
+	.topic = "memory",
+	.long_desc = "Number of times an HLE execution aborted due to none of the previous 4 categories (e.g. interrupts)",
+},
+{
+	.name = "rtm_retired.start",
+	.event = "event=0xC9,umask=0x1,period=2000003",
+	.desc = "Number of times an RTM execution started",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.commit",
+	.event = "event=0xc9,umask=0x2,period=2000003",
+	.desc = "Number of times an RTM execution successfully committed",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.aborted",
+	.event = "event=0xc9,umask=0x4,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to any reasons (multiple categories may count as one) (Precise event)",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.aborted_misc1",
+	.event = "event=0xc9,umask=0x8,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to various memory events (e.g. read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times an RTM execution aborted due to various memory events (e.g. read/write capacity and conflicts)",
+},
+{
+	.name = "rtm_retired.aborted_misc2",
+	.event = "event=0xc9,umask=0x10,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to various memory events (e.g., read/write capacity and conflicts)",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.aborted_misc3",
+	.event = "event=0xc9,umask=0x20,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to HLE-unfriendly instructions",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.aborted_misc4",
+	.event = "event=0xc9,umask=0x40,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to incompatible memory type  Spec update: HSD65",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.aborted_misc5",
+	.event = "event=0xc9,umask=0x80,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to none of the previous 4 categories (e.g. interrupt)",
+	.topic = "memory",
+	.long_desc = "Number of times an RTM execution aborted due to none of the previous 4 categories (e.g. interrupt)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_4",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x4",
+	.desc = "Loads with latency value being above 4  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_8",
+	.event = "event=0xCD,umask=0x1,period=50021,ldlat=0x8",
+	.desc = "Loads with latency value being above 8  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_16",
+	.event = "event=0xCD,umask=0x1,period=20011,ldlat=0x10",
+	.desc = "Loads with latency value being above 16  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_32",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x20",
+	.desc = "Loads with latency value being above 32  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_64",
+	.event = "event=0xCD,umask=0x1,period=2003,ldlat=0x40",
+	.desc = "Loads with latency value being above 64  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_128",
+	.event = "event=0xCD,umask=0x1,period=1009,ldlat=0x80",
+	.desc = "Loads with latency value being above 128  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_256",
+	.event = "event=0xCD,umask=0x1,period=503,ldlat=0x100",
+	.desc = "Loads with latency value being above 256  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_512",
+	.event = "event=0xCD,umask=0x1,period=101,ldlat=0x200",
+	.desc = "Loads with latency value being above 512  Spec update: HSD76, HSD25, HSM26 (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00001",
+	.desc = "Counts demand data reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0600400001",
+	.desc = "Counts demand data reads that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00002",
+	.desc = "Counts all demand data writes (RFOs) that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0600400002",
+	.desc = "Counts all demand data writes (RFOs) that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103fc00002",
+	.desc = "Counts all demand data writes (RFOs) that miss the L3 and the modified data is transferred from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00004",
+	.desc = "Counts all demand code reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0600400004",
+	.desc = "Counts all demand code reads that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00020",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00040",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00080",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_rfo.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00100",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00200",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00091",
+	.desc = "Counts all demand & prefetch data reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0600400091",
+	.desc = "Counts all demand & prefetch data reads that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x063f800091",
+	.desc = "Counts all demand & prefetch data reads that miss the L3 and the data is returned from remote dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103fc00091",
+	.desc = "Counts all demand & prefetch data reads that miss the L3 and the modified data is transferred from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x083fc00091",
+	.desc = "Counts all demand & prefetch data reads that miss the L3 and clean or shared data is transferred from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00122",
+	.desc = "Counts all demand & prefetch RFOs that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0600400122",
+	.desc = "Counts all demand & prefetch RFOs that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00244",
+	.desc = "Counts all demand & prefetch code reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0600400244",
+	.desc = "Counts all demand & prefetch code reads that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x06004007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x063f8007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the L3 and the data is returned from remote dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103fc007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the L3 and the modified data is transferred from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x083fc007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the L3 and clean or shared data is transferred from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_requests.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc08fff",
+	.desc = "Counts all requests that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "dtlb_load_misses.miss_causes_a_walk",
+	.event = "event=0x08,umask=0x1,period=100003",
+	.desc = "Load misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Misses in all TLB levels that cause a page walk of any page size",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_4k",
+	.event = "event=0x08,umask=0x2,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (4K)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to demand load misses that caused 4K page walks in any TLB levels",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_2m_4m",
+	.event = "event=0x08,umask=0x4,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (2M/4M)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to demand load misses that caused 2M/4M page walks in any TLB levels",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_1g",
+	.event = "event=0x08,umask=0x8,period=2000003",
+	.desc = "Load miss in all TLB levels causes a page walk that completes. (1G)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_duration",
+	.event = "event=0x08,umask=0x10,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts cycles when the  page miss handler (PMH) is servicing page walks caused by DTLB load misses",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit_4k",
+	.event = "event=0x08,umask=0x20,period=2000003",
+	.desc = "Load misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load operations from a 4K page that miss the first DTLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit_2m",
+	.event = "event=0x08,umask=0x40,period=2000003",
+	.desc = "Load misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load operations from a 2M page that miss the first DTLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "dtlb_load_misses.pde_cache_miss",
+	.event = "event=0x08,umask=0x80,period=100003",
+	.desc = "DTLB demand load misses with low part of linear-to-physical address translation missed",
+	.topic = "virtual memory",
+	.long_desc = "DTLB demand load misses with low part of linear-to-physical address translation missed",
+},
+{
+	.name = "dtlb_store_misses.miss_causes_a_walk",
+	.event = "event=0x49,umask=0x1,period=100003",
+	.desc = "Store misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Miss in all TLB levels causes a page walk of any page size (4K/2M/4M/1G)",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_4k",
+	.event = "event=0x49,umask=0x2,period=100003",
+	.desc = "Store miss in all TLB levels causes a page walk that completes. (4K)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to store misses in one or more TLB levels of 4K page structure",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_2m_4m",
+	.event = "event=0x49,umask=0x4,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks (2M/4M)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to store misses in one or more TLB levels of 2M/4M page structure",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_1g",
+	.event = "event=0x49,umask=0x8,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks. (1G)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.walk_duration",
+	.event = "event=0x49,umask=0x10,period=100003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts cycles when the  page miss handler (PMH) is servicing page walks caused by DTLB store misses",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit_4k",
+	.event = "event=0x49,umask=0x20,period=100003",
+	.desc = "Store misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store operations from a 4K page that miss the first DTLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit_2m",
+	.event = "event=0x49,umask=0x40,period=100003",
+	.desc = "Store misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store operations from a 2M page that miss the first DTLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "dtlb_store_misses.pde_cache_miss",
+	.event = "event=0x49,umask=0x80,period=100003",
+	.desc = "DTLB store misses with low part of linear-to-physical address translation missed",
+	.topic = "virtual memory",
+	.long_desc = "DTLB store misses with low part of linear-to-physical address translation missed",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4f,umask=0x10,period=2000003",
+	.desc = "Cycle count for an Extended Page table walk",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.miss_causes_a_walk",
+	.event = "event=0x85,umask=0x1,period=100003",
+	.desc = "Misses at all ITLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Misses in ITLB that causes a page walk of any page size",
+},
+{
+	.name = "itlb_misses.walk_completed_4k",
+	.event = "event=0x85,umask=0x2,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (4K)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to misses in ITLB 4K page entries",
+},
+{
+	.name = "itlb_misses.walk_completed_2m_4m",
+	.event = "event=0x85,umask=0x4,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (2M/4M)",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to misses in ITLB 2M/4M page entries",
+},
+{
+	.name = "itlb_misses.walk_completed_1g",
+	.event = "event=0x85,umask=0x8,period=100003",
+	.desc = "Store miss in all TLB levels causes a page walk that completes. (1G)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_duration",
+	.event = "event=0x85,umask=0x10,period=100003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts cycles when the  page miss handler (PMH) is servicing page walks caused by ITLB misses",
+},
+{
+	.name = "itlb_misses.stlb_hit_4k",
+	.event = "event=0x85,umask=0x20,period=100003",
+	.desc = "Core misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+	.long_desc = "ITLB misses that hit STLB (4K)",
+},
+{
+	.name = "itlb_misses.stlb_hit_2m",
+	.event = "event=0x85,umask=0x40,period=100003",
+	.desc = "Code misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+	.long_desc = "ITLB misses that hit STLB (2M)",
+},
+{
+	.name = "itlb.itlb_flush",
+	.event = "event=0xae,umask=0x1,period=100003",
+	.desc = "Flushing of the Instruction TLB (ITLB) pages, includes 4k/2M/4M pages",
+	.topic = "virtual memory",
+	.long_desc = "Counts the number of ITLB flushes, includes 4k/2M/4M pages",
+},
+{
+	.name = "page_walker_loads.dtlb_l1",
+	.event = "event=0xBC,umask=0x11,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L1+FB",
+	.topic = "virtual memory",
+	.long_desc = "Number of DTLB page walker loads that hit in the L1+FB",
+},
+{
+	.name = "page_walker_loads.itlb_l1",
+	.event = "event=0xBC,umask=0x21,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L1+FB",
+	.topic = "virtual memory",
+	.long_desc = "Number of ITLB page walker loads that hit in the L1+FB",
+},
+{
+	.name = "page_walker_loads.ept_dtlb_l1",
+	.event = "event=0xBC,umask=0x41,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the DTLB that hit in the L1 and FB",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.ept_itlb_l1",
+	.event = "event=0xBC,umask=0x81,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the ITLB that hit in the L1 and FB",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_l2",
+	.event = "event=0xBC,umask=0x12,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L2",
+	.topic = "virtual memory",
+	.long_desc = "Number of DTLB page walker loads that hit in the L2",
+},
+{
+	.name = "page_walker_loads.itlb_l2",
+	.event = "event=0xBC,umask=0x22,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L2",
+	.topic = "virtual memory",
+	.long_desc = "Number of ITLB page walker loads that hit in the L2",
+},
+{
+	.name = "page_walker_loads.ept_dtlb_l2",
+	.event = "event=0xBC,umask=0x42,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the DTLB that hit in the L2",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.ept_itlb_l2",
+	.event = "event=0xBC,umask=0x82,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the ITLB that hit in the L2",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_l3",
+	.event = "event=0xBC,umask=0x14,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L3 + XSNP  Spec update: HSD25",
+	.topic = "virtual memory",
+	.long_desc = "Number of DTLB page walker loads that hit in the L3  Spec update: HSD25",
+},
+{
+	.name = "page_walker_loads.itlb_l3",
+	.event = "event=0xBC,umask=0x24,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L3 + XSNP  Spec update: HSD25",
+	.topic = "virtual memory",
+	.long_desc = "Number of ITLB page walker loads that hit in the L3  Spec update: HSD25",
+},
+{
+	.name = "page_walker_loads.ept_dtlb_l3",
+	.event = "event=0xBC,umask=0x44,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the DTLB that hit in the L3",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.ept_itlb_l3",
+	.event = "event=0xBC,umask=0x84,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the ITLB that hit in the L2",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_memory",
+	.event = "event=0xBC,umask=0x18,period=2000003",
+	.desc = "Number of DTLB page walker hits in Memory  Spec update: HSD25",
+	.topic = "virtual memory",
+	.long_desc = "Number of DTLB page walker loads from memory  Spec update: HSD25",
+},
+{
+	.name = "page_walker_loads.itlb_memory",
+	.event = "event=0xBC,umask=0x28,period=2000003",
+	.desc = "Number of ITLB page walker hits in Memory  Spec update: HSD25",
+	.topic = "virtual memory",
+	.long_desc = "Number of ITLB page walker loads from memory  Spec update: HSD25",
+},
+{
+	.name = "page_walker_loads.ept_dtlb_memory",
+	.event = "event=0xBC,umask=0x48,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the DTLB that hit in memory",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.ept_itlb_memory",
+	.event = "event=0xBC,umask=0x88,period=2000003",
+	.desc = "Counts the number of Extended Page Table walks from the ITLB that hit in memory",
+	.topic = "virtual memory",
+},
+{
+	.name = "tlb_flush.dtlb_thread",
+	.event = "event=0xBD,umask=0x1,period=100003",
+	.desc = "DTLB flush attempts of the thread-specific entries",
+	.topic = "virtual memory",
+	.long_desc = "DTLB flush attempts of the thread-specific entries",
+},
+{
+	.name = "tlb_flush.stlb_any",
+	.event = "event=0xBD,umask=0x20,period=100003",
+	.desc = "STLB flush attempts",
+	.topic = "virtual memory",
+	.long_desc = "Count number of STLB flush attempts",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x08,umask=0xe,period=100003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes of any page size",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks in any TLB of any page size due to demand load misses",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x08,umask=0x60,period=2000003",
+	.desc = "Load operations that miss the first DTLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Number of cache load STLB hits. No page walk",
+},
+{
+	.name = "dtlb_store_misses.walk_completed",
+	.event = "event=0x49,umask=0xe,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks due to store miss in any TLB levels of any page size (4K/2M/4M/1G)",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit",
+	.event = "event=0x49,umask=0x60,period=100003",
+	.desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0xe,period=100003",
+	.desc = "Misses in all ITLB levels that cause completed page walks",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks in ITLB of any page size",
+},
+{
+	.name = "itlb_misses.stlb_hit",
+	.event = "event=0x85,umask=0x60,period=100003",
+	.desc = "Operations that miss the first ITLB level but hit the second and do not cause any page walks",
+	.topic = "virtual memory",
+	.long_desc = "ITLB misses that hit STLB. No page walk",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_miss",
+	.event = "event=0x24,umask=0x21,period=200003",
+	.desc = "Demand Data Read miss L2, no rejects  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Demand data read requests that missed L2, no rejects  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_hit",
+	.event = "event=0x24,umask=0x41,period=200003",
+	.desc = "Demand Data Read requests that hit L2 cache  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Demand data read requests that hit L2 cache  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.l2_pf_miss",
+	.event = "event=0x24,umask=0x30,period=200003",
+	.desc = "L2 prefetch requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests that missed L2",
+},
+{
+	.name = "l2_rqsts.l2_pf_hit",
+	.event = "event=0x24,umask=0x50,period=200003",
+	.desc = "L2 prefetch requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests that hit L2",
+},
+{
+	.name = "l2_rqsts.all_demand_data_rd",
+	.event = "event=0x24,umask=0xe1,period=200003",
+	.desc = "Demand Data Read requests  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Counts any demand and L1 HW prefetch data load requests to L2  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.all_rfo",
+	.event = "event=0x24,umask=0xe2,period=200003",
+	.desc = "RFO requests to L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 store RFO requests",
+},
+{
+	.name = "l2_rqsts.all_code_rd",
+	.event = "event=0x24,umask=0xe4,period=200003",
+	.desc = "L2 code requests",
+	.topic = "cache",
+	.long_desc = "Counts all L2 code requests",
+},
+{
+	.name = "l2_rqsts.all_pf",
+	.event = "event=0x24,umask=0xf8,period=200003",
+	.desc = "Requests from L2 hardware prefetchers",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests",
+},
+{
+	.name = "l2_demand_rqsts.wb_hit",
+	.event = "event=0x27,umask=0x50,period=200003",
+	.desc = "Not rejected writebacks that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "Not rejected writebacks that hit L2 cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100003",
+	.desc = "Core-originated cacheable demand requests missed L3",
+	.topic = "cache",
+	.long_desc = "This event counts each cache miss condition for references to the last level cache",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=100003",
+	.desc = "Core-originated cacheable demand requests that refer to L3",
+	.topic = "cache",
+	.long_desc = "This event counts requests originating from the core that reference a cache line in the last level cache",
+},
+{
+	.name = "l1d_pend_miss.pending",
+	.event = "event=0x48,umask=0x1,period=2000003",
+	.desc = "L1D miss oustandings duration in cycles",
+	.topic = "cache",
+	.long_desc = "Increments the number of outstanding L1D misses every cycle. Set Cmask = 1 and Edge =1 to count occurrences",
+},
+{
+	.name = "l1d_pend_miss.request_fb_full",
+	.event = "event=0x48,umask=0x2,period=2000003",
+	.desc = "Number of times a request needed a FB entry but there was no entry available for it. That is the FB unavailability was dominant reason for blocking the request. A request includes cacheable/uncacheable demands that is load, store or SW prefetch. HWP are e",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles",
+	.event = "event=0x48,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles with L1D load Misses outstanding",
+	.topic = "cache",
+},
+{
+	.name = "l1d.replacement",
+	.event = "event=0x51,umask=0x1,period=2000003",
+	.desc = "L1D data line replacements",
+	.topic = "cache",
+	.long_desc = "This event counts when new data lines are brought into the L1 Data cache, which cause other lines to be evicted from the cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003",
+	.desc = "Offcore outstanding Demand Data Read transactions in uncore queue  Spec update: HSD78, HSD62, HSD61",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding demand data read transactions in SQ to uncore. Set Cmask=1 to count cycles  Spec update: HSD78, HSD62, HSD61",
+},
+{
+	.name = "offcore_requests_outstanding.demand_code_rd",
+	.event = "event=0x60,umask=0x2,period=2000003",
+	.desc = "Offcore outstanding code reads transactions in SuperQueue (SQ), queue to uncore, every cycle  Spec update: HSD62, HSD61",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding Demand code Read transactions in SQ to uncore. Set Cmask=1 to count cycles  Spec update: HSD62, HSD61",
+},
+{
+	.name = "offcore_requests_outstanding.demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003",
+	.desc = "Offcore outstanding RFO store transactions in SuperQueue (SQ), queue to uncore  Spec update: HSD62, HSD61",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding RFO store transactions in SQ to uncore. Set Cmask=1 to count cycles  Spec update: HSD62, HSD61",
+},
+{
+	.name = "offcore_requests_outstanding.all_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003",
+	.desc = "Offcore outstanding cacheable Core Data Read transactions in SuperQueue (SQ), queue to uncore  Spec update: HSD62, HSD61",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding cacheable data read transactions in SQ to uncore. Set Cmask=1 to count cycles  Spec update: HSD62, HSD61",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_data_rd",
+	.event = "event=0x60,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore  Spec update: HSD78, HSD62, HSD61",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_data_rd",
+	.event = "event=0x60,umask=0x8,cmask=1,period=2000003",
+	.desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore  Spec update: HSD62, HSD61",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_rfo",
+	.event = "event=0x60,umask=0x4,cmask=1,period=2000003",
+	.desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle  Spec update: HSD62, HSD61",
+	.topic = "cache",
+},
+{
+	.name = "lock_cycles.cache_lock_duration",
+	.event = "event=0x63,umask=0x2,period=2000003",
+	.desc = "Cycles when L1D is locked",
+	.topic = "cache",
+	.long_desc = "Cycles in which the L1D is locked",
+},
+{
+	.name = "offcore_requests.demand_data_rd",
+	.event = "event=0xB0,umask=0x1,period=100003",
+	.desc = "Demand Data Read requests sent to uncore  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Demand data read requests sent to uncore  Spec update: HSD78",
+},
+{
+	.name = "offcore_requests.demand_code_rd",
+	.event = "event=0xB0,umask=0x2,period=100003",
+	.desc = "Cacheable and noncachaeble code read requests",
+	.topic = "cache",
+	.long_desc = "Demand code read requests sent to uncore",
+},
+{
+	.name = "offcore_requests.demand_rfo",
+	.event = "event=0xB0,umask=0x4,period=100003",
+	.desc = "Demand RFO requests including regular RFOs, locks, ItoM",
+	.topic = "cache",
+	.long_desc = "Demand RFO read requests sent to uncore, including regular RFOs, locks, ItoM",
+},
+{
+	.name = "offcore_requests.all_data_rd",
+	.event = "event=0xB0,umask=0x8,period=100003",
+	.desc = "Demand and prefetch data reads",
+	.topic = "cache",
+	.long_desc = "Data read requests sent to uncore (demand and prefetch)",
+},
+{
+	.name = "offcore_requests_buffer.sq_full",
+	.event = "event=0xb2,umask=0x1,period=2000003",
+	.desc = "Offcore requests buffer cannot take more entries for this thread core",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_loads",
+	.event = "event=0xD0,umask=0x11,period=100003",
+	.desc = "Retired load uops that miss the STLB  Supports address when precise.  Spec update: HSD29, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_stores",
+	.event = "event=0xD0,umask=0x12,period=100003",
+	.desc = "Retired store uops that miss the STLB  Supports address when precise.  Spec update: HSD29, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.lock_loads",
+	.event = "event=0xD0,umask=0x21,period=100003",
+	.desc = "Retired load uops with locked access  Supports address when precise.  Spec update: HSD76, HSD29, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.split_loads",
+	.event = "event=0xD0,umask=0x41,period=100003",
+	.desc = "Retired load uops that split across a cacheline boundary  Supports address when precise.  Spec update: HSD29, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.split_stores",
+	.event = "event=0xD0,umask=0x42,period=100003",
+	.desc = "Retired store uops that split across a cacheline boundary  Supports address when precise.  Spec update: HSD29, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0xD0,umask=0x81,period=2000003",
+	.desc = "All retired load uops  Supports address when precise.  Spec update: HSD29, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0xD0,umask=0x82,period=2000003",
+	.desc = "All retired store uops  Supports address when precise.  Spec update: HSD29, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.l1_hit",
+	.event = "event=0xD1,umask=0x1,period=2000003",
+	.desc = "Retired load uops with L1 cache hits as data sources  Supports address when precise.  Spec update: HSD29, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.l2_hit",
+	.event = "event=0xD1,umask=0x2,period=100003",
+	.desc = "Retired load uops with L2 cache hits as data sources  Supports address when precise.  Spec update: HSD76, HSD29, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.l3_hit",
+	.event = "event=0xD1,umask=0x4,period=50021",
+	.desc = "Retired load uops which data sources were data hits in L3 without snoops required  Supports address when precise.  Spec update: HSD74, HSD29, HSD25, HSM26, HSM30 (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops with L3 cache hits as data sources  Supports address when precise.  Spec update: HSD74, HSD29, HSD25, HSM26, HSM30 (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_miss",
+	.event = "event=0xD1,umask=0x8,period=100003",
+	.desc = "Retired load uops misses in L1 cache as data sources  Supports address when precise.  Spec update: HSM30 (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops missed L1 cache as data sources  Supports address when precise.  Spec update: HSM30 (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_miss",
+	.event = "event=0xD1,umask=0x10,period=50021",
+	.desc = "Miss in mid-level (L2) cache. Excludes Unknown data-source  Supports address when precise.  Spec update: HSD29, HSM30 (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops missed L2. Unknown data source excluded  Supports address when precise.  Spec update: HSD29, HSM30 (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l3_miss",
+	.event = "event=0xD1,umask=0x20,period=100003",
+	.desc = "Miss in last-level (L3) cache. Excludes Unknown data-source  Supports address when precise.  Spec update: HSD74, HSD29, HSD25, HSM26, HSM30 (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops missed L3. Excludes unknown data source   Supports address when precise.  Spec update: HSD74, HSD29, HSD25, HSM26, HSM30 (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.hit_lfb",
+	.event = "event=0xD1,umask=0x40,period=100003",
+	.desc = "Retired load uops which data sources were load uops missed L1 but hit FB due to preceding miss to the same cache line with data not ready  Supports address when precise.  Spec update: HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_miss",
+	.event = "event=0xD2,umask=0x1,period=20011",
+	.desc = "Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache  Supports address when precise.  Spec update: HSD29, HSD25, HSM26, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_hit",
+	.event = "event=0xD2,umask=0x2,period=20011",
+	.desc = "Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache  Supports address when precise.  Spec update: HSD29, HSD25, HSM26, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_hitm",
+	.event = "event=0xD2,umask=0x4,period=20011",
+	.desc = "Retired load uops which data sources were HitM responses from shared L3  Supports address when precise.  Spec update: HSD29, HSD25, HSM26, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_none",
+	.event = "event=0xD2,umask=0x8,period=100003",
+	.desc = "Retired load uops which data sources were hits in L3 without snoops required  Supports address when precise.  Spec update: HSD74, HSD29, HSD25, HSM26, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_miss_retired.local_dram",
+	.event = "event=0xD3,umask=0x1,period=100003",
+	.desc = "Data from local DRAM either Snoop not needed or Snoop Miss (RspI)  Supports address when precise.  Spec update: HSD74, HSD29, HSD25, HSM30 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts retired load uops where the data came from local DRAM. This does not include hardware prefetches  Supports address when precise.  Spec update: HSD74, HSD29, HSD25, HSM30 (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_miss_retired.remote_dram",
+	.event = "event=0xD3,umask=0x4,period=100003",
+	.desc = "Retired load uop whose Data Source was: remote DRAM either Snoop not needed or Snoop Miss (RspI)  Supports address when precise.  Spec update: HSD29, HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_miss_retired.remote_hitm",
+	.event = "event=0xD3,umask=0x10,period=100003",
+	.desc = "Retired load uop whose Data Source was: Remote cache HITM  Supports address when precise.  Spec update: HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_miss_retired.remote_fwd",
+	.event = "event=0xD3,umask=0x20,period=100003",
+	.desc = "Retired load uop whose Data Source was: forwarded from remote cache  Supports address when precise.  Spec update: HSM30 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.demand_data_rd",
+	.event = "event=0xf0,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "Demand data read requests that access L2 cache",
+},
+{
+	.name = "l2_trans.rfo",
+	.event = "event=0xf0,umask=0x2,period=200003",
+	.desc = "RFO requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "RFO requests that access L2 cache",
+},
+{
+	.name = "l2_trans.code_rd",
+	.event = "event=0xf0,umask=0x4,period=200003",
+	.desc = "L2 cache accesses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "L2 cache accesses when fetching instructions",
+},
+{
+	.name = "l2_trans.all_pf",
+	.event = "event=0xf0,umask=0x8,period=200003",
+	.desc = "L2 or L3 HW prefetches that access L2 cache",
+	.topic = "cache",
+	.long_desc = "Any MLC or L3 HW prefetch accessing L2, including rejects",
+},
+{
+	.name = "l2_trans.l1d_wb",
+	.event = "event=0xf0,umask=0x10,period=200003",
+	.desc = "L1D writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L1D writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_fill",
+	.event = "event=0xf0,umask=0x20,period=200003",
+	.desc = "L2 fill requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L2 fill requests that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_wb",
+	.event = "event=0xf0,umask=0x40,period=200003",
+	.desc = "L2 writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L2 writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.all_requests",
+	.event = "event=0xf0,umask=0x80,period=200003",
+	.desc = "Transactions accessing L2 pipe",
+	.topic = "cache",
+	.long_desc = "Transactions accessing L2 pipe",
+},
+{
+	.name = "l2_lines_in.i",
+	.event = "event=0xF1,umask=0x1,period=100003",
+	.desc = "L2 cache lines in I state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in I state filling L2",
+},
+{
+	.name = "l2_lines_in.s",
+	.event = "event=0xF1,umask=0x2,period=100003",
+	.desc = "L2 cache lines in S state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in S state filling L2",
+},
+{
+	.name = "l2_lines_in.e",
+	.event = "event=0xF1,umask=0x4,period=100003",
+	.desc = "L2 cache lines in E state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in E state filling L2",
+},
+{
+	.name = "l2_lines_in.all",
+	.event = "event=0xF1,umask=0x7,period=100003",
+	.desc = "L2 cache lines filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines brought into the L2 cache.  Lines are filled into the L2 cache when there was an L2 miss",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x5,period=100003",
+	.desc = "Clean L2 cache lines evicted by demand",
+	.topic = "cache",
+	.long_desc = "Clean L2 cache lines evicted by demand",
+},
+{
+	.name = "l2_lines_out.demand_dirty",
+	.event = "event=0xF2,umask=0x6,period=100003",
+	.desc = "Dirty L2 cache lines evicted by demand",
+	.topic = "cache",
+	.long_desc = "Dirty L2 cache lines evicted by demand",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xf4,umask=0x10,period=100003",
+	.desc = "Split locks in SQ",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x42,period=200003",
+	.desc = "RFO requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts the number of store RFO requests that hit the L2 cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x22,period=200003",
+	.desc = "RFO requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts the number of store RFO requests that miss the L2 cache",
+},
+{
+	.name = "l2_rqsts.code_rd_hit",
+	.event = "event=0x24,umask=0x44,period=200003",
+	.desc = "L2 cache hits when fetching instructions, code reads",
+	.topic = "cache",
+	.long_desc = "Number of instruction fetches that hit the L2 cache",
+},
+{
+	.name = "l2_rqsts.code_rd_miss",
+	.event = "event=0x24,umask=0x24,period=200003",
+	.desc = "L2 cache misses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "Number of instruction fetches that missed the L2 cache",
+},
+{
+	.name = "l2_rqsts.all_demand_miss",
+	.event = "event=0x24,umask=0x27,period=200003",
+	.desc = "Demand requests that miss L2 cache  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Demand requests that miss L2 cache  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.all_demand_references",
+	.event = "event=0x24,umask=0xe7,period=200003",
+	.desc = "Demand requests to L2 cache  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "Demand requests to L2 cache  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.miss",
+	.event = "event=0x24,umask=0x3f,period=200003",
+	.desc = "All requests that miss L2 cache  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "All requests that missed L2  Spec update: HSD78",
+},
+{
+	.name = "l2_rqsts.references",
+	.event = "event=0x24,umask=0xff,period=200003",
+	.desc = "All L2 requests  Spec update: HSD78",
+	.topic = "cache",
+	.long_desc = "All requests to L2 cache  Spec update: HSD78",
+},
+{
+	.name = "offcore_response",
+	.event = "event=0xB7,umask=0x1,period=100003",
+	.desc = "Offcore response can be programmed only with a specific pair of event select and counter MSR, and with specific event codes and predefine mask bit value in a dedicated MSR to specify attributes of the offcore transaction",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd_ge_6",
+	.event = "event=0x60,umask=0x1,cmask=6,period=2000003",
+	.desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue  Spec update: HSD78, HSD62, HSD61",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles_any",
+	.event = "event=0x48,umask=0x1,any=1,cmask=1,period=2000003",
+	.desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.fb_full",
+	.event = "event=0x48,umask=0x2,cmask=1,period=2000003",
+	.desc = "Cycles a demand request was blocked due to Fill Buffers inavailability",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0001",
+	.desc = "Counts demand data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0001",
+	.desc = "Counts demand data reads that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0002",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0002",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0004",
+	.desc = "Counts all demand code reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0004",
+	.desc = "Counts all demand code reads that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0020",
+	.desc = "Counts all prefetch (that bring data to L2) RFOs that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0040",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0080",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_rfo.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0100",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0200",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0091",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0091",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0122",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0122",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0244",
+	.desc = "Counts all demand & prefetch code reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c07f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c07f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_requests.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c8fff",
+	.desc = "Counts all requests that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_skylake[] = {
+{
+	.name = "fp_arith_inst_retired.scalar_double",
+	.event = "event=0xC7,umask=0x1,period=2000003",
+	.desc = "Number of SSE/AVX computational scalar double precision floating-point instructions retired.  Each count represents 1 computation. Applies to SSE* and AVX* scalar double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT FM(N)ADD/SUB.  FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.scalar_single",
+	.event = "event=0xC7,umask=0x2,period=2000003",
+	.desc = "Number of SSE/AVX computational scalar single precision floating-point instructions retired.  Each count represents 1 computation. Applies to SSE* and AVX* scalar single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT FM(N)ADD/SUB.  FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.128b_packed_double",
+	.event = "event=0xC7,umask=0x4,period=2000003",
+	.desc = "Number of SSE/AVX computational 128-bit packed double precision floating-point instructions retired.  Each count represents 2 computations. Applies to SSE* and AVX* packed double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.128b_packed_single",
+	.event = "event=0xC7,umask=0x8,period=2000003",
+	.desc = "Number of SSE/AVX computational 128-bit packed single precision floating-point instructions retired.  Each count represents 4 computations. Applies to SSE* and AVX* packed single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+	.long_desc = "Number of SSE/AVX computational 128-bit packed single precision floating-point instructions retired.  Each count represents 4 computations. Applies to SSE* and AVX* packed single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+},
+{
+	.name = "fp_arith_inst_retired.256b_packed_double",
+	.event = "event=0xC7,umask=0x10,period=2000003",
+	.desc = "Number of SSE/AVX computational 256-bit packed double precision floating-point instructions retired.  Each count represents 4 computations. Applies to SSE* and AVX* packed double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.256b_packed_single",
+	.event = "event=0xC7,umask=0x20,period=2000003",
+	.desc = "Number of SSE/AVX computational 256-bit packed single precision floating-point instructions retired.  Each count represents 8 computations. Applies to SSE* and AVX* packed single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.any",
+	.event = "event=0xCA,umask=0x1e,period=100003,cmask=1",
+	.desc = "Cycles with any input/output SSE or FP assist",
+	.topic = "floating point",
+	.long_desc = "This event counts cycles with any input and output SSE or x87 FP assist. If an input and output assist are detected on the same cycle the event increments by 1",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired from execution",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions retired from execution. For instructions that consist of multiple micro-ops, this event counts the retirement of the last micro-op of the instruction. Counting continues during hardware interrupts, traps, and inside interrupt handlers. \nNotes: INST_RETIRED.ANY is counted by a designated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events. INST_RETIRED.ANY_P is counted by a programmable counter and it is an architectural performance event. \nCounting: Faulting executions of GETSEC/VM entry/VM Exit/MWait will not count as retired instructions",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Core cycles when the thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of core cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. This event is a component in many key event ratios. The core frequency may change from time to time due to transitions associated with Enhanced Intel SpeedStep Technology or TM2. For this reason this event may have a changing ratio with regards to time. When the core frequency is constant, this event can approximate elapsed time while the core was not in the halt state. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Reference cycles when the core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of reference cycles when the core is not in a halt state. The core enters the halt state when it is running the HLT instruction or the MWAIT instruction. This event is not affected by core frequency changes (for example, P states, TM2 transitions) but has the same incrementing frequency as the time stamp counter. This event can approximate elapsed time while the core was not in a halt state. This event has a constant ratio with the CPU_CLK_UNHALTED.REF_XCLK event. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events. Note: On all current platforms this event stops counting during 'throttling (TM)' states duty off periods the processor is 'halted'.  The counter update is done at a lower clock rate then the core clock the overflow status bit for this counter may appear 'sticky'.  After the counter has overflowed and software clears the overflow status bit and resets the counter to less than MAX. The reset value to the counter is not clocked immediately so the overflow status bit will flip 'high (1)' and generate another PMI (if enabled) after which the reset value gets clocked into the counter. Therefore, software will get the interrupt, read the overflow status bit '1 for bit 34 while the counter value is less than MAX. Software should ignore this case",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Thread cycles when thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This is an architectural event that counts the number of thread cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. The core frequency may change from time to time due to power or thermal throttling. For this reason, this event may have a changing ratio with regards to wall clock time",
+},
+{
+	.name = "baclears.any",
+	.event = "event=0xE6,umask=0x1,period=100003",
+	.desc = "Counts the total number when the front end is resteered, mainly when the BPU cannot provide a correct prediction and this is corrected by other branch handling mechanisms at the front end",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.uops",
+	.event = "event=0xA8,umask=0x1,period=2000003",
+	.desc = "Number of Uops delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000003",
+	.desc = "Stalls caused by changing prefix length of the instruction",
+	.topic = "pipeline",
+	.long_desc = "This event counts stalls occured due to changing prefix length (66, 67 or REX.W when they change the length of the decoded instruction). Occurrences counting is proportional to the number of prefixes in a 16B-line. This may result in the following penalties: three-cycle penalty for each LCP in a 16-byte chunk",
+},
+{
+	.name = "int_misc.recovery_cycles",
+	.event = "event=0x0D,umask=0x1,period=2000003",
+	.desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for this thread (e.g. misprediction or memory nuke)",
+	.topic = "pipeline",
+	.long_desc = "Cycles checkpoints in Resource Allocation Table (RAT) are recovering from JEClear or machine clear",
+},
+{
+	.name = "int_misc.clear_resteer_cycles",
+	.event = "event=0x0D,umask=0x80,period=2000003",
+	.desc = "Cycles the issue-stage is waiting for front-end to fetch from resteered path following branch misprediction or machine clear events",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000003",
+	.desc = "Resource-related stall cycles",
+	.topic = "pipeline",
+	.long_desc = "This event counts resource-related stall cycles. Reasons for stalls can be as follows:\n - *any* u-arch structure got full (LB, SB, RS, ROB, BOB, LM, Physical Register Reclaim Table (PRRT), or Physical History Table (PHT) slots)\n - *any* u-arch structure got empty (like INT/SIMD FreeLists)\n - FPU control word (FPCW), MXCSR\nand others. This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "resource_stalls.sb",
+	.event = "event=0xA2,umask=0x8,period=2000003",
+	.desc = "Cycles stalled due to no store buffers available. (not including draining form sync)",
+	.topic = "pipeline",
+	.long_desc = "This event counts stall cycles caused by the store buffer (SB) overflow (excluding draining from synch). This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0x0E,umask=0x1,period=2000003",
+	.desc = "Uops that Resource Allocation Table (RAT) issues to Reservation Station (RS)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of Uops issued by the Resource Allocation Table (RAT) to the reservation station (RS)",
+},
+{
+	.name = "uops_issued.slow_lea",
+	.event = "event=0x0E,umask=0x20,period=2000003",
+	.desc = "Number of slow LEA uops being allocated. A uop is generally considered SlowLea if it has 3 sources (e.g. 2 sources + immediate) regardless if as a result of LEA instruction or not",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which the Resource Allocation Table (RAT) does not issue any Uops to the reservation station (RS) for the current thread",
+},
+{
+	.name = "rs_events.empty_cycles",
+	.event = "event=0x5E,umask=0x1,period=2000003",
+	.desc = "Cycles when Reservation Station (RS) is empty for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which the reservation station (RS) is empty for the thread.\nNote: In ST-mode, not active thread should drive 0. This is usually caused by severely costly branch mispredictions, or allocator/FE issues",
+},
+{
+	.name = "rs_events.empty_end",
+	.event = "event=0x5E,inv=1,umask=0x1,edge=1,period=2000003,cmask=1",
+	.desc = "Counts end of periods where the Reservation Station (RS) was empty. Could be useful to precisely locate Frontend Latency Bound issues",
+	.topic = "pipeline",
+},
+{
+	.name = "rob_misc_events.lbr_inserts",
+	.event = "event=0xCC,umask=0x20,period=2000003",
+	.desc = "Increments whenever there is an update to the LBR array",
+	.topic = "pipeline",
+	.long_desc = "Increments when an entry is added to the Last Branch Record (LBR) array (or removed from the array in case of RETURNs in call stack mode). The event requires LBR enable via IA32_DEBUGCTL MSR and branch type selection via MSR_LBR_SELECT",
+},
+{
+	.name = "machine_clears.count",
+	.event = "event=0xC3,umask=0x1,edge=1,period=100003,cmask=1",
+	.desc = "Number of machine clears (nukes) of any type",
+	.topic = "pipeline",
+	.long_desc = "Number of machine clears (nukes) of any type",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=100003",
+	.desc = "Self-modifying code (SMC) detected",
+	.topic = "pipeline",
+	.long_desc = "This event counts self-modifying code (SMC) detected, which causes a machine clear",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Number of instructions retired. General Counter - architectural event  Spec update: SKL091, SKL044",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions (EOMs) retired. Counting covers macro-fused instructions individually (that is, increments by two)  Spec update: SKL091, SKL044",
+},
+{
+	.name = "inst_retired.prec_dist",
+	.event = "event=0xC0,umask=0x1,period=2000003",
+	.desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution  Spec update: SKL091, SKL044 (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version (that is, uses PEBS) of the event that counts instructions retired  Spec update: SKL091, SKL044 (Must be precise)",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000003",
+	.desc = "Retirement slots used",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of retirement slots used",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts cycles without actually retired uops",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=10",
+	.desc = "Cycles with less than 10 actually retired uops",
+	.topic = "pipeline",
+	.long_desc = "Number of cycles using always true condition (uops_ret < 16) applied to non PEBS uops retired event",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=400009",
+	.desc = "Conditional branch instructions retired  Spec update: SKL091 (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts conditional branch instructions retired  Spec update: SKL091 (Precise event)",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=100007",
+	.desc = "Direct and indirect near call instructions retired  Spec update: SKL091 (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts both direct and indirect near call instructions retired  Spec update: SKL091 (Precise event)",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=400009",
+	.desc = "All (macro) branch instructions retired  Spec update: SKL091",
+	.topic = "pipeline",
+	.long_desc = "This event counts all (macro) branch instructions retired  Spec update: SKL091",
+},
+{
+	.name = "br_inst_retired.near_return",
+	.event = "event=0xC4,umask=0x8,period=100007",
+	.desc = "Return instructions retired  Spec update: SKL091 (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts return instructions retired  Spec update: SKL091 (Precise event)",
+},
+{
+	.name = "br_inst_retired.not_taken",
+	.event = "event=0xC4,umask=0x10,period=400009",
+	.desc = "Not taken branch instructions retired  Spec update: SKL091",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts not taken branch instructions retired  Spec update: SKL091",
+},
+{
+	.name = "br_inst_retired.near_taken",
+	.event = "event=0xC4,umask=0x20,period=400009",
+	.desc = "Taken branch instructions retired  Spec update: SKL091 (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts taken branch instructions retired  Spec update: SKL091 (Precise event)",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0x40,period=100007",
+	.desc = "Far branch instructions retired  Spec update: SKL091 (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts far branch instructions retired  Spec update: SKL091 (Precise event)",
+},
+{
+	.name = "br_inst_retired.all_branches_pebs",
+	.event = "event=0xC4,umask=0x4,period=400009",
+	.desc = "All (macro) branch instructions retired  Spec update: SKL091 (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version of BR_INST_RETIRED.ALL_BRANCHES that counts all (macro) branch instructions retired  Spec update: SKL091 (Must be precise)",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=400009",
+	.desc = "Mispredicted conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts mispredicted conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_misp_retired.near_call",
+	.event = "event=0xC5,umask=0x2,period=400009",
+	.desc = "Mispredicted direct and indirect near call instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken retired mispredicted direct and indirect near calls, including both register and memory indirect (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=400009",
+	.desc = "All mispredicted macro branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts all mispredicted macro branch instructions retired",
+},
+{
+	.name = "br_misp_retired.near_taken",
+	.event = "event=0xC5,umask=0x20,period=400009",
+	.desc = "Number of near branch instructions retired that were mispredicted and taken (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.all_branches_pebs",
+	.event = "event=0xC5,umask=0x4,period=400009",
+	.desc = "Mispredicted macro branch instructions retired (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version of BR_MISP_RETIRED.ALL_BRANCHES that counts all mispredicted macro branch instructions retired (Must be precise)",
+},
+{
+	.name = "uops_executed.thread",
+	.event = "event=0xB1,umask=0x1,period=2000003",
+	.desc = "Counts the number of uops to be executed per-thread each cycle",
+	.topic = "pipeline",
+	.long_desc = "Number of uops to be executed per-thread each cycle",
+},
+{
+	.name = "uops_executed.core",
+	.event = "event=0xB1,umask=0x2,period=2000003",
+	.desc = "Number of uops executed on the core",
+	.topic = "pipeline",
+	.long_desc = "Number of uops executed from any thread",
+},
+{
+	.name = "uops_executed.x87",
+	.event = "event=0xB1,umask=0x10,period=2000003",
+	.desc = "Counts the number of x87 uops dispatched",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts number of cycles no uops were dispatched to be executed on this thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which no uops were dispatched from the Reservation Station (RS) per thread",
+},
+{
+	.name = "uops_executed.cycles_ge_1_uop_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles where at least 1 uop was executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 1 uop was executed per-thread",
+},
+{
+	.name = "uops_executed.cycles_ge_2_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles where at least 2 uops were executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 2 uops were executed per-thread",
+},
+{
+	.name = "uops_executed.cycles_ge_3_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles where at least 3 uops were executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 3 uops were executed per-thread",
+},
+{
+	.name = "uops_executed.cycles_ge_4_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles where at least 4 uops were executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 4 uops were executed per-thread",
+},
+{
+	.name = "exe_activity.exe_bound_0_ports",
+	.event = "event=0xA6,umask=0x1,period=2000003",
+	.desc = "Cycles where no uops were executed, the Reservation Station was not empty, the Store Buffer was full and there was no outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "exe_activity.1_ports_util",
+	.event = "event=0xA6,umask=0x2,period=2000003",
+	.desc = "Cycles total of 1 uop is executed on all ports and Reservation Station was not empty",
+	.topic = "pipeline",
+},
+{
+	.name = "exe_activity.2_ports_util",
+	.event = "event=0xA6,umask=0x4,period=2000003",
+	.desc = "Cycles total of 2 uops are executed on all ports and Reservation Station was not empty",
+	.topic = "pipeline",
+},
+{
+	.name = "exe_activity.3_ports_util",
+	.event = "event=0xA6,umask=0x8,period=2000003",
+	.desc = "Cycles total of 3 uops are executed on all ports and Reservation Station was not empty",
+	.topic = "pipeline",
+},
+{
+	.name = "exe_activity.4_ports_util",
+	.event = "event=0xA6,umask=0x10,period=2000003",
+	.desc = "Cycles total of 4 uops are executed on all ports and Reservation Station was not empty",
+	.topic = "pipeline",
+},
+{
+	.name = "exe_activity.bound_on_stores",
+	.event = "event=0xA6,umask=0x40,period=2000003",
+	.desc = "Cycles where the Store Buffer was full and no outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 0",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 0",
+},
+{
+	.name = "uops_dispatched_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 1",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 1",
+},
+{
+	.name = "uops_dispatched_port.port_2",
+	.event = "event=0xA1,umask=0x4,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 2",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 2",
+},
+{
+	.name = "uops_dispatched_port.port_3",
+	.event = "event=0xA1,umask=0x8,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 3",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 3",
+},
+{
+	.name = "uops_dispatched_port.port_4",
+	.event = "event=0xA1,umask=0x10,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 4",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 4",
+},
+{
+	.name = "uops_dispatched_port.port_5",
+	.event = "event=0xA1,umask=0x20,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 5",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 5",
+},
+{
+	.name = "uops_dispatched_port.port_6",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 6",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 6",
+},
+{
+	.name = "uops_dispatched_port.port_7",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 7",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 7",
+},
+{
+	.name = "cycle_activity.stalls_total",
+	.event = "event=0xA3,umask=0x4,period=2000003,cmask=4",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l1d_miss",
+	.event = "event=0xA3,umask=0x8,period=2000003,cmask=8",
+	.desc = "Cycles while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l1d_miss",
+	.event = "event=0xA3,umask=0xc,period=2000003,cmask=12",
+	.desc = "Execution stalls while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.sw_pf",
+	.event = "event=0x4C,umask=0x1,period=100003",
+	.desc = "Demand load dispatches that hit L1D fill buffer (FB) allocated for software prefetch",
+	.topic = "pipeline",
+	.long_desc = "This event counts all not software-prefetch load dispatches that hit the fill buffer (FB) allocated for the software prefetch. It can also be incremented by some lock instructions. So it should only be used with profiling so that the locks can be excluded by asm inspection of the nearby instructions",
+},
+{
+	.name = "ld_blocks.store_forward",
+	.event = "event=0x03,umask=0x2,period=100003",
+	.desc = "Loads blocked by overlapping with store buffer that cannot be forwarded ",
+	.topic = "pipeline",
+	.long_desc = "This event counts how many times the load operation got the true Block-on-Store blocking code preventing store forwarding. This includes cases when:\n - preceding store conflicts with the load (incomplete overlap)\n\n - store forwarding is impossible due to u-arch limitations\n\n - preceding lock RMW operations are not forwarded\n\n - store has the no-forward bit set (uncacheable/page-split/masked stores)\n\n - all-blocking stores are used (mostly, fences and port I/O)\n\nand others.\nThe most common case is a load blocked due to its address range overlapping with a preceding smaller uncompleted store. Note: This event does not take into account cases of out-of-SW-control (for example, SbTailHit), unknown physical STA, and cases of blocking loads on store due to being non-WB memory type or a lock. These cases are covered by other events.\nSee the table of not supported store forwards in the Optimization Guide",
+},
+{
+	.name = "ld_blocks.no_sr",
+	.event = "event=0x03,umask=0x8,period=100003",
+	.desc = "The number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+	.topic = "pipeline",
+	.long_desc = "The number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+},
+{
+	.name = "ld_blocks_partial.address_alias",
+	.event = "event=0x07,umask=0x1,period=100003",
+	.desc = "False dependencies in MOB due to partial compare on address",
+	.topic = "pipeline",
+	.long_desc = "This event counts false dependencies in MOB when the partial comparison upon loose net check and dependency was resolved by the Enhanced Loose net mechanism. This may not result in high performance penalties. Loose net checks can fail when loads and stores are 4k aliased",
+},
+{
+	.name = "cycle_activity.cycles_l2_miss",
+	.event = "event=0xA3,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l2_miss",
+	.event = "event=0xA3,umask=0x5,period=2000003,cmask=5",
+	.desc = "Execution stalls while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_mem_any",
+	.event = "event=0xA3,umask=0x10,period=2000003,cmask=16",
+	.desc = "Cycles while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_mem_any",
+	.event = "event=0xA3,umask=0x14,period=2000003,cmask=20",
+	.desc = "Execution stalls while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2503",
+	.desc = "Core crystal clock cycles when the thread is unhalted",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Core crystal clock cycles when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.total_cycles_ps",
+	.event = "event=0xC0,inv=1,umask=0x1,period=2000003,cmask=10",
+	.desc = "Number of cycles using always true condition applied to  PEBS instructions retired event  Spec update: SKL091, SKL044 (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Number of cycles using an always true condition applied to  PEBS instructions retired event. (inst_ret< 16)  Spec update: SKL091, SKL044 (Must be precise)",
+},
+{
+	.name = "arith.divider_active",
+	.event = "event=0x14,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when divide unit is busy executing divide or square root operations. Accounts for integer and floating-point operations",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_active",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_4_uops",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "other_assists.any",
+	.event = "event=0xC1,umask=0x3f,period=100003",
+	.desc = "Number of times a microcode assist is invoked by HW other than FP-assist. Examples include AD (page Access Dirty) and AVX* related assists",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.vector_width_mismatch",
+	.event = "event=0x0E,umask=0x2,period=2000003",
+	.desc = "Uops inserted at issue-stage in order to preserve upper bits of vector registers",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of Blend Uops issued by the Resource Allocation Table (RAT) to the reservation station (RS) in order to preserve upper bits of vector registers. Starting the Skylake microarchitecture, these Blend uops are needed since every Intel SSE instruction executed in Dirty Upper State needs to preserve bits 128-255 of the destination register.\r\nFor more information, refer to ?Mixing Intel AVX and Intel SSE Code? section of the Optimization Guide",
+},
+{
+	.name = "cpu_clk_unhalted.thread_any",
+	.event = "event=0x3c,any=1",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p_any",
+	.event = "event=0x3C,umask=0x0,any=1,period=2000003",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2503",
+	.desc = "Core crystal clock cycles when at least one thread on the physical core is unhalted",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_cycles_any",
+	.event = "event=0x0D,umask=0x1,any=1,period=2000003",
+	.desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_1",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles at least 1 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_2",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles at least 2 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_3",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=3",
+	.desc = "Cycles at least 3 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_4",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=4",
+	.desc = "Cycles at least 4 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_none",
+	.event = "event=0xB1,inv=1,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles with no micro-ops executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ring0_trans",
+	.event = "event=0x3C,umask=0x0,edge=1,period=100007,cmask=1",
+	.desc = "Counts when there is a transition from ring 1, 2 or 3 to ring 0",
+	.topic = "pipeline",
+	.long_desc = "This event counts when the Current Privilege Level (CPL) transitions from ring 1, 2 or 3 to ring 0 (Kernel)",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2503",
+	.desc = "Core crystal clock cycles when the thread is unhalted",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2503",
+	.desc = "Core crystal clock cycles when at least one thread on the physical core is unhalted",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2503",
+	.desc = "Core crystal clock cycles when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "icache_16b.ifdata_stall",
+	.event = "event=0x80,umask=0x4,period=2000003",
+	.desc = "Cycles where a code fetch is stalled due to L1 instruction cache miss",
+	.topic = "frontend",
+},
+{
+	.name = "icache_64b.iftag_hit",
+	.event = "event=0x83,umask=0x1,period=200003",
+	.desc = "Instruction fetch tag lookups that hit in the instruction cache (L1I). Counts at 64-byte cache-line granularity",
+	.topic = "frontend",
+},
+{
+	.name = "icache_64b.iftag_miss",
+	.event = "event=0x83,umask=0x2,period=200003",
+	.desc = "Instruction fetch tag lookups that miss in the instruction cache (L1I). Counts at 64-byte cache-line granularity",
+	.topic = "frontend",
+},
+{
+	.name = "icache_64b.iftag_stall",
+	.event = "event=0x83,umask=0x4,period=200003",
+	.desc = "Cycles where a code fetch is stalled due to L1 instruction cache tag miss",
+	.topic = "frontend",
+},
+{
+	.name = "idq.mite_uops",
+	.event = "event=0x79,umask=0x4,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may 'bypass' the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq.dsb_uops",
+	.event = "event=0x79,umask=0x8,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may 'bypass' the IDQ",
+},
+{
+	.name = "idq.ms_mite_uops",
+	.event = "event=0x79,umask=0x20,period=2000003",
+	.desc = "Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may 'bypass' the IDQ",
+},
+{
+	.name = "idq.ms_cycles",
+	.event = "event=0x79,umask=0x30,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may 'bypass' the IDQ. Uops maybe initiated by Decode Stream Buffer (DSB) or MITE",
+},
+{
+	.name = "idq.mite_cycles",
+	.event = "event=0x79,umask=0x4,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may 'bypass' the IDQ",
+},
+{
+	.name = "idq.dsb_cycles",
+	.event = "event=0x79,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may 'bypass' the IDQ",
+},
+{
+	.name = "idq.ms_dsb_cycles",
+	.event = "event=0x79,umask=0x10,period=2000003,cmask=1",
+	.desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while the Microcode Sequencer (MS) is busy. Counting includes uops that may 'bypass' the IDQ",
+},
+{
+	.name = "idq.all_dsb_cycles_4_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=4",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles 4  uops were  delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may 'bypass' the IDQ",
+},
+{
+	.name = "idq.all_dsb_cycles_any_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=1",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles  uops were  delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may 'bypass' the IDQ",
+},
+{
+	.name = "idq.all_mite_cycles_4_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=4",
+	.desc = "Cycles MITE is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles 4  uops were  delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may 'bypass' the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq.all_mite_cycles_any_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=1",
+	.desc = "Cycles MITE is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles  uops were delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may 'bypass' the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq_uops_not_delivered.core",
+	.event = "event=0x9C,umask=0x1,period=2000003",
+	.desc = "Uops not delivered to Resource Allocation Table (RAT) per thread when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops not delivered to Resource Allocation Table (RAT) per thread adding ?4 ? x? when Resource Allocation Table (RAT) is not stalled and Instruction Decode Queue (IDQ) delivers x uops to Resource Allocation Table (RAT) (where x belongs to {0,1,2,3}). Counting does not cover cases when:\n a. IDQ-Resource Allocation Table (RAT) pipe serves the other thread\n\n b. Resource Allocation Table (RAT) is stalled for the thread (including uop drops and clear BE conditions)\n \n c. Instruction Decode Queue (IDQ) delivers four uops",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_0_uops_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles per thread when 4 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts, on the per-thread basis, cycles when no uops are delivered to Resource Allocation Table (RAT). IDQ_Uops_Not_Delivered.core =4",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_1_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles per thread when 3 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts, on the per-thread basis, cycles when less than 1 uop is  delivered to Resource Allocation Table (RAT). IDQ_Uops_Not_Delivered.core >=3",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_2_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles with less than 2 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_3_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with less than 3 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_fe_was_ok",
+	.event = "event=0x9C,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts cycles FE delivered 4 uops or Resource Allocation Table (RAT) was stalling FE",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.penalty_cycles",
+	.event = "event=0xAB,umask=0x2,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles",
+	.topic = "frontend",
+	.long_desc = "This event counts Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles. These cycles do not include uops routed through because of the switch itself, for example, when Instruction Decode Queue (IDQ) pre-allocation is unavailable, or Instruction Decode Queue (IDQ) is full. SBD-to-MITE switch true penalty cycles happen after the merge mux (MM) receives Decode Stream Buffer (DSB) Sync-indication until receiving the first MITE uop. \nMM is placed before Instruction Decode Queue (IDQ) to merge uops being fed from the MITE and Decode Stream Buffer (DSB) paths. Decode Stream Buffer (DSB) inserts the Sync-indication whenever a Decode Stream Buffer (DSB)-to-MITE switch occurs.\nPenalty: A Decode Stream Buffer (DSB) hit followed by a Decode Stream Buffer (DSB) miss can cost up to six cycles in which no uops are delivered to the IDQ. Most often, such switches from the Decode Stream Buffer (DSB) to the legacy pipeline cost 0?2 cycles",
+},
+{
+	.name = "frontend_retired.dsb_miss",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x11",
+	.desc = "Retired Instructions who experienced decode stream buffer (DSB - the decoded instruction-cache) miss (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.l1i_miss",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x12",
+	.desc = "Retired Instructions who experienced Instruction L1 Cache true miss (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.l2_miss",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x13",
+	.desc = "Retired Instructions who experienced Instruction L2 Cache true miss (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.itlb_miss",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x14",
+	.desc = "Retired Instructions who experienced iTLB true miss (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.stlb_miss",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x15",
+	.desc = "Retired Instructions who experienced STLB (2nd level TLB) true miss (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.latency_ge_2",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x400206",
+	.desc = "Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 2 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.latency_ge_2_bubbles_ge_2",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x200206",
+	.desc = "Retired instructions that are fetched after an interval where the front-end had at least 2 bubble-slots for a period of 2 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.latency_ge_4",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x400406",
+	.desc = "Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 4 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "idq.ms_switches",
+	.event = "event=0x79,umask=0x30,edge=1,period=2000003,cmask=1",
+	.desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+	.topic = "frontend",
+	.long_desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+},
+{
+	.name = "idq.ms_uops",
+	.event = "event=0x79,umask=0x30,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the total number of uops delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may 'bypass' the IDQ. Uops maybe initiated by Decode Stream Buffer (DSB) or MITE",
+},
+{
+	.name = "frontend_retired.latency_ge_8",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x400806",
+	.desc = "Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 8 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.latency_ge_16",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x401006",
+	.desc = "Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 16 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.latency_ge_32",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x402006",
+	.desc = "Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 32 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.latency_ge_64",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x404006",
+	.desc = "Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 64 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.latency_ge_128",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x408006",
+	.desc = "Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 128 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.latency_ge_256",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x410006",
+	.desc = "Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 256 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.latency_ge_512",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x420006",
+	.desc = "Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 512 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.latency_ge_2_bubbles_ge_1",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x100206",
+	.desc = "Retired instructions that are fetched after an interval where the front-end had at least 1 bubble-slot for a period of 2 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "frontend_retired.latency_ge_2_bubbles_ge_3",
+	.event = "event=0xC6,umask=0x1,period=100007,frontend=0x300206",
+	.desc = "Retired instructions that are fetched after an interval where the front-end had at least 3 bubble-slots for a period of 2 cycles which was not interrupted by a back-end stall (Precise event)",
+	.topic = "frontend",
+},
+{
+	.name = "hw_interrupts.received",
+	.event = "event=0xCB,umask=0x1,period=100003",
+	.desc = "Number of hardware interrupts received by the processor",
+	.topic = "other",
+	.long_desc = "This event counts the number of hardware interruptions received by the processor",
+},
+{
+	.name = "tx_exec.misc1",
+	.event = "event=0x5d,umask=0x1,period=2000003",
+	.desc = "Counts the number of times a class of instructions that may cause a transactional abort was executed. Since this is the count of execution, it may not always cause a transactional abort",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by  a flowmarker",
+},
+{
+	.name = "tx_exec.misc2",
+	.event = "event=0x5d,umask=0x2,period=2000003",
+	.desc = "Counts the number of times a class of instructions (e.g., vzeroupper) that may cause a transactional abort was executed inside a transactional region",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by  a vzeroupper instruction",
+},
+{
+	.name = "tx_exec.misc3",
+	.event = "event=0x5d,umask=0x4,period=2000003",
+	.desc = "Counts the number of times an instruction execution caused the transactional nest count supported to be exceeded",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by a nest count that is too deep",
+},
+{
+	.name = "tx_exec.misc4",
+	.event = "event=0x5d,umask=0x8,period=2000003",
+	.desc = "Counts the number of times a XBEGIN instruction was executed inside an HLE transactional region",
+	.topic = "memory",
+	.long_desc = "RTM region detected inside HLE",
+},
+{
+	.name = "tx_exec.misc5",
+	.event = "event=0x5d,umask=0x10,period=2000003",
+	.desc = "Counts the number of times an HLE XACQUIRE instruction was executed inside an RTM transactional region",
+	.topic = "memory",
+	.long_desc = "Counts the number of times an HLE XACQUIRE instruction was executed inside an RTM transactional region",
+},
+{
+	.name = "hle_retired.start",
+	.event = "event=0xC8,umask=0x1,period=2000003",
+	.desc = "Number of times an HLE execution started",
+	.topic = "memory",
+	.long_desc = "Number of times we entered an HLE region\n does not count nested transactions",
+},
+{
+	.name = "hle_retired.commit",
+	.event = "event=0xC8,umask=0x2,period=2000003",
+	.desc = "Number of times an HLE execution successfully committed",
+	.topic = "memory",
+	.long_desc = "Number of times HLE commit succeeded",
+},
+{
+	.name = "hle_retired.aborted",
+	.event = "event=0xC8,umask=0x4,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to any reasons (multiple categories may count as one) (Precise event)",
+	.topic = "memory",
+	.long_desc = "Number of times HLE abort was triggered (Precise event)",
+},
+{
+	.name = "hle_retired.aborted_mem",
+	.event = "event=0xC8,umask=0x8,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to various memory events (e.g., read/write capacity and conflicts)",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_timer",
+	.event = "event=0xC8,umask=0x10,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to hardware timer expiration",
+	.topic = "memory",
+},
+{
+	.name = "hle_retired.aborted_unfriendly",
+	.event = "event=0xC8,umask=0x20,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to HLE-unfriendly instructions and certain unfriendly events (such as AD assists etc.)",
+	.topic = "memory",
+	.long_desc = "Number of times an HLE execution aborted due to HLE-unfriendly instructions and certain unfriendly events (such as AD assists etc.)",
+},
+{
+	.name = "hle_retired.aborted_memtype",
+	.event = "event=0xC8,umask=0x40,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to incompatible memory type",
+	.topic = "memory",
+	.long_desc = "Number of times an HLE execution aborted due to incompatible memory type",
+},
+{
+	.name = "hle_retired.aborted_events",
+	.event = "event=0xC8,umask=0x80,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to unfriendly events (such as interrupts)",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.start",
+	.event = "event=0xC9,umask=0x1,period=2000003",
+	.desc = "Number of times an RTM execution started",
+	.topic = "memory",
+	.long_desc = "Number of times we entered an RTM region\n does not count nested transactions",
+},
+{
+	.name = "rtm_retired.commit",
+	.event = "event=0xC9,umask=0x2,period=2000003",
+	.desc = "Number of times an RTM execution successfully committed",
+	.topic = "memory",
+	.long_desc = "Number of times RTM commit succeeded",
+},
+{
+	.name = "rtm_retired.aborted",
+	.event = "event=0xC9,umask=0x4,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to any reasons (multiple categories may count as one) (Precise event)",
+	.topic = "memory",
+	.long_desc = "Number of times RTM abort was triggered (Precise event)",
+},
+{
+	.name = "rtm_retired.aborted_mem",
+	.event = "event=0xC9,umask=0x8,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to various memory events (e.g. read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times an RTM execution aborted due to various memory events (e.g. read/write capacity and conflicts)",
+},
+{
+	.name = "rtm_retired.aborted_timer",
+	.event = "event=0xC9,umask=0x10,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to uncommon conditions",
+	.topic = "memory",
+},
+{
+	.name = "rtm_retired.aborted_unfriendly",
+	.event = "event=0xC9,umask=0x20,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to HLE-unfriendly instructions",
+	.topic = "memory",
+	.long_desc = "Number of times an RTM execution aborted due to HLE-unfriendly instructions",
+},
+{
+	.name = "rtm_retired.aborted_memtype",
+	.event = "event=0xC9,umask=0x40,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to incompatible memory type",
+	.topic = "memory",
+	.long_desc = "Number of times an RTM execution aborted due to incompatible memory type",
+},
+{
+	.name = "rtm_retired.aborted_events",
+	.event = "event=0xC9,umask=0x80,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to none of the previous 4 categories (e.g. interrupt)",
+	.topic = "memory",
+	.long_desc = "Number of times an RTM execution aborted due to none of the previous 4 categories (e.g. interrupt)",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=100003",
+	.desc = "Counts the number of machine clears due to memory order conflicts  Spec update: SKL089",
+	.topic = "memory",
+	.long_desc = "This event counts the number of memory ordering Machine Clears detected. Memory Ordering Machine Clears can result from one of the following:\n1. memory disambiguation,\n2. external snoop, or\n3. cross SMT-HW-thread snoop (stores) hitting load buffer  Spec update: SKL089",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_4",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x4",
+	.desc = "Counts loads when the latency from first dispatch to completion is greater than 4 cycles (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Counts loads when the latency from first dispatch to completion is greater than 4 cycles.  Reported latency may be longer than just the memory latency (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_8",
+	.event = "event=0xCD,umask=0x1,period=50021,ldlat=0x8",
+	.desc = "Counts loads when the latency from first dispatch to completion is greater than 8 cycles (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Counts loads when the latency from first dispatch to completion is greater than 8 cycles.  Reported latency may be longer than just the memory latency (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_16",
+	.event = "event=0xCD,umask=0x1,period=20011,ldlat=0x10",
+	.desc = "Counts loads when the latency from first dispatch to completion is greater than 16 cycles (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Counts loads when the latency from first dispatch to completion is greater than 16 cycles.  Reported latency may be longer than just the memory latency (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_32",
+	.event = "event=0xCD,umask=0x1,period=100007,ldlat=0x20",
+	.desc = "Counts loads when the latency from first dispatch to completion is greater than 32 cycles (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Counts loads when the latency from first dispatch to completion is greater than 32 cycles.  Reported latency may be longer than just the memory latency (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_64",
+	.event = "event=0xCD,umask=0x1,period=2003,ldlat=0x40",
+	.desc = "Counts loads when the latency from first dispatch to completion is greater than 64 cycles (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Counts loads when the latency from first dispatch to completion is greater than 64 cycles.  Reported latency may be longer than just the memory latency (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_128",
+	.event = "event=0xCD,umask=0x1,period=1009,ldlat=0x80",
+	.desc = "Counts loads when the latency from first dispatch to completion is greater than 128 cycles (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Counts loads when the latency from first dispatch to completion is greater than 128 cycles.  Reported latency may be longer than just the memory latency (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_256",
+	.event = "event=0xCD,umask=0x1,period=503,ldlat=0x100",
+	.desc = "Counts loads when the latency from first dispatch to completion is greater than 256 cycles (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Counts loads when the latency from first dispatch to completion is greater than 256 cycles.  Reported latency may be longer than just the memory latency (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_512",
+	.event = "event=0xCD,umask=0x1,period=101,ldlat=0x200",
+	.desc = "Counts loads when the latency from first dispatch to completion is greater than 512 cycles (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Counts loads when the latency from first dispatch to completion is greater than 512 cycles.  Reported latency may be longer than just the memory latency (Must be precise)",
+},
+{
+	.name = "tx_mem.abort_conflict",
+	.event = "event=0x54,umask=0x1,period=2000003",
+	.desc = "Number of times a transactional abort was signaled due to a data conflict on a transactionally accessed address",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX line had a cache conflict",
+},
+{
+	.name = "tx_mem.abort_capacity",
+	.event = "event=0x54,umask=0x2,period=2000003",
+	.desc = "Number of times a transactional abort was signaled due to a data capacity limitation for transactional reads or writes",
+	.topic = "memory",
+},
+{
+	.name = "tx_mem.abort_hle_store_to_elided_lock",
+	.event = "event=0x54,umask=0x4,period=2000003",
+	.desc = "Number of times a HLE transactional region aborted due to a non XRELEASE prefixed instruction writing to an elided lock in the elision buffer",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to a non-release/commit store to lock",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_not_empty",
+	.event = "event=0x54,umask=0x8,period=2000003",
+	.desc = "Number of times an HLE transactional execution aborted due to NoAllocatedElisionBuffer being non-zero",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to commit but Lock Buffer not empty",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_mismatch",
+	.event = "event=0x54,umask=0x10,period=2000003",
+	.desc = "Number of times an HLE transactional execution aborted due to XRELEASE lock not satisfying the address and value requirements in the elision buffer",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to release/commit but data and address mismatch",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_unsupported_alignment",
+	.event = "event=0x54,umask=0x20,period=2000003",
+	.desc = "Number of times an HLE transactional execution aborted due to an unsupported read alignment from the elision buffer",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to attempting an unsupported alignment from Lock Buffer",
+},
+{
+	.name = "tx_mem.hle_elision_buffer_full",
+	.event = "event=0x54,umask=0x40,period=2000003",
+	.desc = "Number of times HLE lock could not be elided due to ElisionBufferAvailable being zero",
+	.topic = "memory",
+	.long_desc = "Number of times we could not allocate Lock Buffer",
+},
+{
+	.name = "offcore_requests.l3_miss_demand_data_rd",
+	.event = "event=0xB0,umask=0x10,period=100003",
+	.desc = "Demand Data Read requests who miss L3 cache",
+	.topic = "memory",
+	.long_desc = "Demand Data Read requests who miss L3 cache",
+},
+{
+	.name = "offcore_requests_outstanding.l3_miss_demand_data_rd",
+	.event = "event=0x60,umask=0x10,period=2000003",
+	.desc = "Counts number of Offcore outstanding Demand Data Read requests that miss L3 cache in the superQ every cycle",
+	.topic = "memory",
+},
+{
+	.name = "cycle_activity.cycles_l3_miss",
+	.event = "event=0xA3,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles while L3 cache miss demand load is outstanding",
+	.topic = "memory",
+},
+{
+	.name = "cycle_activity.stalls_l3_miss",
+	.event = "event=0xA3,umask=0x6,period=2000003,cmask=6",
+	.desc = "Execution stalls while L3 cache miss demand load is outstanding",
+	.topic = "memory",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_l3_miss_demand_data_rd",
+	.event = "event=0x60,umask=0x10,period=2000003,cmask=1",
+	.desc = "Cycles with at least 1 Demand Data Read requests who miss L3 cache in the superQ",
+	.topic = "memory",
+},
+{
+	.name = "offcore_requests_outstanding.l3_miss_demand_data_rd_ge_6",
+	.event = "event=0x60,umask=0x10,period=2000003,cmask=6",
+	.desc = "Cycles with at least 6 Demand Data Read requests that miss L3 cache in the superQ",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3ffc008000 ",
+	.desc = "OTHER & L3_MISS & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x203c008000 ",
+	.desc = "OTHER & L3_MISS & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103c008000 ",
+	.desc = "OTHER & L3_MISS & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c008000 ",
+	.desc = "OTHER & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c008000 ",
+	.desc = "OTHER & L3_MISS & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c008000 ",
+	.desc = "OTHER & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc008000 ",
+	.desc = "OTHER & L3_MISS & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x007c008000 ",
+	.desc = "OTHER & L3_MISS & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc4008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_miss_local_dram.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0044008000 ",
+	.desc = "OTHER & L3_MISS_LOCAL_DRAM & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l4_hit_local_l4.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000408000 ",
+	.desc = "OTHER & L4_HIT_LOCAL_L4 & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20001c8000 ",
+	.desc = "OTHER & L3_HIT & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_hit_s.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000108000 ",
+	.desc = "OTHER & L3_HIT_S & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_hit_e.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000088000 ",
+	.desc = "OTHER & L3_HIT_E & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.l3_hit_m.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000048000 ",
+	.desc = "OTHER & L3_HIT_M & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3ffc000800 ",
+	.desc = "STREAMING_STORES & L3_MISS & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x203c000800 ",
+	.desc = "STREAMING_STORES & L3_MISS & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103c000800 ",
+	.desc = "STREAMING_STORES & L3_MISS & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000800 ",
+	.desc = "STREAMING_STORES & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000800 ",
+	.desc = "STREAMING_STORES & L3_MISS & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000800 ",
+	.desc = "STREAMING_STORES & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000800 ",
+	.desc = "STREAMING_STORES & L3_MISS & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x007c000800 ",
+	.desc = "STREAMING_STORES & L3_MISS & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc4000800 ",
+	.desc = "STREAMING_STORES & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000800 ",
+	.desc = "STREAMING_STORES & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000800 ",
+	.desc = "STREAMING_STORES & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000800 ",
+	.desc = "STREAMING_STORES & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000800 ",
+	.desc = "STREAMING_STORES & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000800 ",
+	.desc = "STREAMING_STORES & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000800 ",
+	.desc = "STREAMING_STORES & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_miss_local_dram.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0044000800 ",
+	.desc = "STREAMING_STORES & L3_MISS_LOCAL_DRAM & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l4_hit_local_l4.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000400800 ",
+	.desc = "STREAMING_STORES & L4_HIT_LOCAL_L4 & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20001c0800 ",
+	.desc = "STREAMING_STORES & L3_HIT & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_s.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000100800 ",
+	.desc = "STREAMING_STORES & L3_HIT_S & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_e.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000080800 ",
+	.desc = "STREAMING_STORES & L3_HIT_E & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_m.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000040800 ",
+	.desc = "STREAMING_STORES & L3_HIT_M & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.streaming_stores.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020800 ",
+	.desc = "STREAMING_STORES & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3ffc000100 ",
+	.desc = "PF_L3_RFO & L3_MISS & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x203c000100 ",
+	.desc = "PF_L3_RFO & L3_MISS & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103c000100 ",
+	.desc = "PF_L3_RFO & L3_MISS & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000100 ",
+	.desc = "PF_L3_RFO & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000100 ",
+	.desc = "PF_L3_RFO & L3_MISS & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000100 ",
+	.desc = "PF_L3_RFO & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000100 ",
+	.desc = "PF_L3_RFO & L3_MISS & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x007c000100 ",
+	.desc = "PF_L3_RFO & L3_MISS & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc4000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_miss_local_dram.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0044000100 ",
+	.desc = "PF_L3_RFO & L3_MISS_LOCAL_DRAM & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l4_hit_local_l4.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000400100 ",
+	.desc = "PF_L3_RFO & L4_HIT_LOCAL_L4 & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20001c0100 ",
+	.desc = "PF_L3_RFO & L3_HIT & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_s.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000100100 ",
+	.desc = "PF_L3_RFO & L3_HIT_S & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_e.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000080100 ",
+	.desc = "PF_L3_RFO & L3_HIT_E & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_m.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000040100 ",
+	.desc = "PF_L3_RFO & L3_HIT_M & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3ffc000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x203c000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103c000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x007c000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc4000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_miss_local_dram.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0044000080 ",
+	.desc = "PF_L3_DATA_RD & L3_MISS_LOCAL_DRAM & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l4_hit_local_l4.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000400080 ",
+	.desc = "PF_L3_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20001c0080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_s.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000100080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_S & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_e.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000080080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_E & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_m.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000040080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_M & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3ffc000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x203c000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103c000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x007c000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc4000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_miss_local_dram.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0044000004 ",
+	.desc = "DEMAND_CODE_RD & L3_MISS_LOCAL_DRAM & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l4_hit_local_l4.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000400004 ",
+	.desc = "DEMAND_CODE_RD & L4_HIT_LOCAL_L4 & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20001c0004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_s.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000100004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_S & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_e.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000080004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_E & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_m.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000040004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_M & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3ffc000002 ",
+	.desc = "DEMAND_RFO & L3_MISS & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x203c000002 ",
+	.desc = "DEMAND_RFO & L3_MISS & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103c000002 ",
+	.desc = "DEMAND_RFO & L3_MISS & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000002 ",
+	.desc = "DEMAND_RFO & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000002 ",
+	.desc = "DEMAND_RFO & L3_MISS & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000002 ",
+	.desc = "DEMAND_RFO & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000002 ",
+	.desc = "DEMAND_RFO & L3_MISS & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x007c000002 ",
+	.desc = "DEMAND_RFO & L3_MISS & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc4000002 ",
+	.desc = "DEMAND_RFO & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000002 ",
+	.desc = "DEMAND_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000002 ",
+	.desc = "DEMAND_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000002 ",
+	.desc = "DEMAND_RFO & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000002 ",
+	.desc = "DEMAND_RFO & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000002 ",
+	.desc = "DEMAND_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000002 ",
+	.desc = "DEMAND_RFO & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_miss_local_dram.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0044000002 ",
+	.desc = "DEMAND_RFO & L3_MISS_LOCAL_DRAM & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l4_hit_local_l4.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000400002 ",
+	.desc = "DEMAND_RFO & L4_HIT_LOCAL_L4 & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20001c0002 ",
+	.desc = "DEMAND_RFO & L3_HIT & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_s.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000100002 ",
+	.desc = "DEMAND_RFO & L3_HIT_S & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_e.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000080002 ",
+	.desc = "DEMAND_RFO & L3_HIT_E & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_m.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000040002 ",
+	.desc = "DEMAND_RFO & L3_HIT_M & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020002 ",
+	.desc = "DEMAND_RFO & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3ffc000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x203c000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103c000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x043c000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x023c000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x013c000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00bc000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x007c000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc4000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & ANY_SNOOP",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2004000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1004000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HITM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0404000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_HIT_NO_FWD",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0204000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_MISS",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0104000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NOT_NEEDED",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0084000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SNOOP_NONE",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_miss_local_dram.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0044000001 ",
+	.desc = "DEMAND_DATA_RD & L3_MISS_LOCAL_DRAM & SPL_HIT",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l4_hit_local_l4.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000400001 ",
+	.desc = "DEMAND_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x20001c0001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_s.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000100001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_S & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_e.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000080001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_E & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_m.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000040001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_M & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_non_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2000020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_NON_DRAM",
+	.topic = "memory",
+},
+{
+	.name = "itlb.itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=100007",
+	.desc = "Flushing of the Instruction TLB (ITLB) pages, includes 4k/2M/4M pages",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of flushes of the big or small ITLB pages. Counting include both TLB Flush (covering all sets) and TLB Set Clear (set-specific)",
+},
+{
+	.name = "ept.walk_pending",
+	.event = "event=0x4F,umask=0x10,period=2000003",
+	.desc = "Counts 1 per cycle for each PMH that is busy with a EPT (Extended Page Table) walk for any request type",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.miss_causes_a_walk",
+	.event = "event=0x85,umask=0x1,period=100003",
+	.desc = "Misses at all ITLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)",
+},
+{
+	.name = "itlb_misses.walk_completed_4k",
+	.event = "event=0x85,umask=0x2,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (4K)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault",
+},
+{
+	.name = "itlb_misses.walk_completed_2m_4m",
+	.event = "event=0x85,umask=0x4,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (2M/4M)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault",
+},
+{
+	.name = "itlb_misses.walk_completed_1g",
+	.event = "event=0x85,umask=0x8,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (1G)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault",
+},
+{
+	.name = "itlb_misses.walk_pending",
+	.event = "event=0x85,umask=0x10,period=100003",
+	.desc = "Counts 1 per cycle for each PMH that is busy with a page walk for an instruction fetch request. EPT page walk duration are excluded in Skylake",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.stlb_hit",
+	.event = "event=0x85,umask=0x20,period=100003",
+	.desc = "Instruction fetch requests that miss the ITLB and hit the STLB",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.miss_causes_a_walk",
+	.event = "event=0x08,umask=0x1,period=100003",
+	.desc = "Load misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_4k",
+	.event = "event=0x08,umask=0x2,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (4K)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_2m_4m",
+	.event = "event=0x08,umask=0x4,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (2M/4M)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_1g",
+	.event = "event=0x08,umask=0x8,period=2000003",
+	.desc = "Load miss in all TLB levels causes a page walk that completes. (1G)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault",
+},
+{
+	.name = "dtlb_load_misses.walk_pending",
+	.event = "event=0x08,umask=0x10,period=2000003",
+	.desc = "Counts 1 per cycle for each PMH that is busy with a page walk for a load. EPT page walk duration are excluded in Skylake",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x08,umask=0x20,period=2000003",
+	.desc = "Loads that miss the DTLB and hit the STLB",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.miss_causes_a_walk",
+	.event = "event=0x49,umask=0x1,period=100003",
+	.desc = "Store misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_4k",
+	.event = "event=0x49,umask=0x2,period=100003",
+	.desc = "Store miss in all TLB levels causes a page walk that completes. (4K)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_2m_4m",
+	.event = "event=0x49,umask=0x4,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks (2M/4M)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_1g",
+	.event = "event=0x49,umask=0x8,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks (1G)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault",
+},
+{
+	.name = "dtlb_store_misses.walk_pending",
+	.event = "event=0x49,umask=0x10,period=2000003",
+	.desc = "Counts 1 per cycle for each PMH that is busy with a page walk for a store. EPT page walk duration are excluded in Skylake",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit",
+	.event = "event=0x49,umask=0x20,period=100003",
+	.desc = "Stores that miss the DTLB and hit the STLB",
+	.topic = "virtual memory",
+},
+{
+	.name = "tlb_flush.dtlb_thread",
+	.event = "event=0xBD,umask=0x1,period=100007",
+	.desc = "DTLB flush attempts of the thread-specific entries",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of DTLB flush attempts of the thread-specific entries",
+},
+{
+	.name = "tlb_flush.stlb_any",
+	.event = "event=0xBD,umask=0x20,period=100007",
+	.desc = "STLB flush attempts",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of any STLB flush attempts (such as entire, VPID, PCID, InvPage, CR3 write, and so on)",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0xe,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (All page sizes)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x08,umask=0xe,period=100003",
+	.desc = "Load miss in all TLB levels causes a page walk that completes. (All page sizes)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.walk_completed",
+	.event = "event=0x49,umask=0xe,period=100003",
+	.desc = "Store misses in all TLB levels causes a page walk that completes. (All page sizes)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.walk_active",
+	.event = "event=0x49,umask=0x10,period=100003,cmask=1",
+	.desc = "Cycles when at least one PMH is busy with a page walk for a store. EPT page walk duration are excluded in Skylake",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_active",
+	.event = "event=0x08,umask=0x10,period=100003,cmask=1",
+	.desc = "Cycles when at least one PMH is busy with a page walk for a load. EPT page walk duration are excluded in Skylake",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_active",
+	.event = "event=0x85,umask=0x10,period=100003,cmask=1",
+	.desc = "Cycles when at least one PMH is busy with a page walk for code (instruction fetch) request. EPT page walk duration are excluded in Skylake",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_inst_retired.stlb_miss_loads",
+	.event = "event=0xD0,umask=0x11,period=100003",
+	.desc = "Retired load instructions that miss the STLB  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.stlb_miss_stores",
+	.event = "event=0xD0,umask=0x12,period=100003",
+	.desc = "Retired store instructions that miss the STLB  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.lock_loads",
+	.event = "event=0xD0,umask=0x21,period=100007",
+	.desc = "Retired load instructions with locked access  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.split_loads",
+	.event = "event=0xD0,umask=0x41,period=100003",
+	.desc = "Retired load instructions that split across a cacheline boundary  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.split_stores",
+	.event = "event=0xD0,umask=0x42,period=100003",
+	.desc = "Retired store instructions that split across a cacheline boundary  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.all_loads",
+	.event = "event=0xD0,umask=0x81,period=2000003",
+	.desc = "All retired load instructions  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.all_stores",
+	.event = "event=0xD0,umask=0x82,period=2000003",
+	.desc = "All retired store instructions  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l1_hit",
+	.event = "event=0xD1,umask=0x1,period=2000003",
+	.desc = "Retired load instructions with L1 cache hits as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load instructions with L1 cache hits as data sources  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_retired.l2_hit",
+	.event = "event=0xD1,umask=0x2,period=100003",
+	.desc = "Retired load instructions with L2 cache hits as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load instructions with L2 cache hits as data sources  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_retired.l3_hit",
+	.event = "event=0xD1,umask=0x4,period=50021",
+	.desc = "Retired load instructions with L3 cache hits as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load instructions with L3 cache hits as data sources  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_retired.l1_miss",
+	.event = "event=0xD1,umask=0x8,period=100003",
+	.desc = "Retired load instructions missed L1 cache as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load instructions missed L1 cache as data sources  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_retired.l2_miss",
+	.event = "event=0xD1,umask=0x10,period=50021",
+	.desc = "Retired load instructions missed L2 cache as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load instructions missed L2 cache as data sources  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_retired.l3_miss",
+	.event = "event=0xD1,umask=0x20,period=100007",
+	.desc = "Retired load instructions missed L3 cache as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load instructions missed L3 cache as data sources  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_retired.fb_hit",
+	.event = "event=0xD1,umask=0x40,period=100007",
+	.desc = "Retired load instructions which data sources were load missed L1 but hit FB due to preceding miss to the same cache line with data not ready  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load instructions which data sources were load missed L1 but hit FB due to preceding miss to the same cache line with data not ready  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_l3_hit_retired.xsnp_miss",
+	.event = "event=0xD2,umask=0x1,period=20011",
+	.desc = "Retired load instructions which data sources were L3 hit and cross-core snoop missed in on-pkg core cache  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_l3_hit_retired.xsnp_hit",
+	.event = "event=0xD2,umask=0x2,period=20011",
+	.desc = "Retired load instructions which data sources were L3 and cross-core snoop hits in on-pkg core cache  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load instructions which data sources were L3 and cross-core snoop hits in on-pkg core cache  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_l3_hit_retired.xsnp_hitm",
+	.event = "event=0xD2,umask=0x4,period=20011",
+	.desc = "Retired load instructions which data sources were HitM responses from shared L3  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load instructions which data sources were HitM responses from shared L3  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_l3_hit_retired.xsnp_none",
+	.event = "event=0xD2,umask=0x8,period=100003",
+	.desc = "Retired load instructions which data sources were hits in L3 without snoops required  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load instructions which data sources were hits in L3 without snoops required  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_misc_retired.uc",
+	.event = "event=0xD4,umask=0x4,period=100007",
+	.desc = "Retired instructions with at least 1 uncacheable load or lock  Supports address when precise (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "l1d.replacement",
+	.event = "event=0x51,umask=0x1,period=2000003",
+	.desc = "L1D data line replacements",
+	.topic = "cache",
+	.long_desc = "This event counts L1D data line replacements including opportunistic replacements, and replacements that require stall-for-replace or block-for-replace",
+},
+{
+	.name = "l1d_pend_miss.pending",
+	.event = "event=0x48,umask=0x1,period=2000003",
+	.desc = "L1D miss outstandings duration in cycles",
+	.topic = "cache",
+	.long_desc = "This event counts duration of L1D miss outstanding, that is each cycle number of Fill Buffers (FB) outstanding required by Demand Reads. FB either is held by demand loads, or it is held by non-demand loads and gets hit at least once by demand. The valid outstanding interval is defined until the FB deallocation by one of the following ways: from FB allocation, if FB is allocated by demand\n from the demand Hit FB, if it is allocated by hardware or software prefetch.\nNote: In the L1D, a Demand Read contains cacheable or noncacheable demand loads, including ones causing cache-line splits and reads due to page walks resulted from any request type",
+},
+{
+	.name = "l1d_pend_miss.fb_full",
+	.event = "event=0x48,umask=0x2,period=2000003",
+	.desc = "Number of times a request needed a FB entry but there was no entry available for it. That is the FB unavailability was dominant reason for blocking the request. A request includes cacheable/uncacheable demands that is load, store or SW prefetch",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles",
+	.event = "event=0x48,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding",
+	.topic = "cache",
+	.long_desc = "This event counts duration of L1D miss outstanding in cycles",
+},
+{
+	.name = "offcore_requests.demand_data_rd",
+	.event = "event=0xB0,umask=0x1,period=100003",
+	.desc = "Demand Data Read requests sent to uncore",
+	.topic = "cache",
+	.long_desc = "This event counts the Demand Data Read requests sent to uncore. Use it in conjunction with OFFCORE_REQUESTS_OUTSTANDING to determine average latency in the uncore",
+},
+{
+	.name = "offcore_requests.demand_code_rd",
+	.event = "event=0xB0,umask=0x2,period=100003",
+	.desc = "Cacheable and noncachaeble code read requests",
+	.topic = "cache",
+	.long_desc = "This event counts both cacheable and noncachaeble code read requests",
+},
+{
+	.name = "offcore_requests.demand_rfo",
+	.event = "event=0xB0,umask=0x4,period=100003",
+	.desc = "Demand RFO requests including regular RFOs, locks, ItoM",
+	.topic = "cache",
+	.long_desc = "This event counts the demand RFO (read for ownership) requests including regular RFOs, locks, ItoM",
+},
+{
+	.name = "offcore_requests.all_data_rd",
+	.event = "event=0xB0,umask=0x8,period=100003",
+	.desc = "Demand and prefetch data reads",
+	.topic = "cache",
+	.long_desc = "This event counts the demand and prefetch data reads. All Core Data Reads include cacheable 'Demands' and L2 prefetchers (not L3 prefetchers). Counting also covers reads due to page walks resulted from any request type",
+},
+{
+	.name = "offcore_requests.all_requests",
+	.event = "event=0xB0,umask=0x80,period=100003",
+	.desc = "Any memory transaction that reached the SQ",
+	.topic = "cache",
+	.long_desc = "This event counts memory transactions reached the super queue including requests initiated by the core, all L3 prefetches, page walks, and so on",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003",
+	.desc = "Offcore outstanding Demand Data Read transactions in uncore queue",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding Demand Data Read transactions in the super queue (SQ) every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor. See the corresponding Umask under OFFCORE_REQUESTS.\nNote: A prefetch promoted to Demand is counted from the promotion point",
+},
+{
+	.name = "offcore_requests_outstanding.demand_code_rd",
+	.event = "event=0x60,umask=0x2,period=2000003",
+	.desc = "Offcore outstanding Code Reads transactions in the SuperQueue (SQ), queue to uncore, every cycle",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding Code Reads transactions in the super queue every cycle. The 'Offcore outstanding' state of the transaction lasts from the L2 miss until the sending transaction completion to requestor (SQ deallocation). See the corresponding Umask under OFFCORE_REQUESTS",
+},
+{
+	.name = "offcore_requests_outstanding.demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003",
+	.desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding RFO (store) transactions in the super queue (SQ) every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS",
+},
+{
+	.name = "offcore_requests_outstanding.all_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003",
+	.desc = "Offcore outstanding cacheable Core Data Read transactions in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding cacheable Core Data Read transactions in the super queue every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "This event counts cycles when offcore outstanding Demand Data Read transactions are present in the super queue (SQ). A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation)",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "This event counts cycles when offcore outstanding cacheable Core Data Read transactions are present in the super queue. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS",
+},
+{
+	.name = "offcore_requests_buffer.sq_full",
+	.event = "event=0xB2,umask=0x1,period=2000003",
+	.desc = "Offcore requests buffer cannot take more entries for this thread core",
+	.topic = "cache",
+	.long_desc = "This event counts the number of cases when the offcore requests buffer cannot take more entries for the core. This can happen when the superqueue does not contain eligible entries, or when L1D writeback pending FIFO requests is full.\nNote: Writeback pending FIFO has six entries",
+},
+{
+	.name = "l2_trans.l2_wb",
+	.event = "event=0xF0,umask=0x40,period=200003",
+	.desc = "L2 writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L2 writebacks that access L2 cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100003",
+	.desc = "Core-originated cacheable demand requests missed L3  Spec update: SKL057",
+	.topic = "cache",
+	.long_desc = "This event counts core-originated cacheable demand requests that miss the last level cache (LLC). Demand requests include loads, RFOs, and hardware prefetches from L1D, and instruction fetches from IFU  Spec update: SKL057",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=100003",
+	.desc = "Core-originated cacheable demand requests that refer to L3  Spec update: SKL057",
+	.topic = "cache",
+	.long_desc = "This event counts core-originated cacheable demand requests that refer to the last level cache (LLC). Demand requests include loads, RFOs, and hardware prefetches from L1D, and instruction fetches from IFU  Spec update: SKL057",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xF4,umask=0x10,period=100003",
+	.desc = "Number of cache line split locks sent to uncore",
+	.topic = "cache",
+	.long_desc = "This event counts the number of cache line split locks sent to the uncore",
+},
+{
+	.name = "offcore_response",
+	.event = "event=0xB7,umask=0x1,period=100003",
+	.desc = "Offcore response can be programmed only with a specific pair of event select and counter MSR, and with specific event codes and predefine mask bit value in a dedicated MSR to specify attributes of the offcore transaction",
+	.topic = "cache",
+	.long_desc = "Offcore response can be programmed only with a specific pair of event select and counter MSR, and with specific event codes and predefine mask bit value in a dedicated MSR to specify attributes of the offcore transaction",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_miss",
+	.event = "event=0x24,umask=0x21,period=200003",
+	.desc = "Demand Data Read miss L2, no rejects",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests that miss L2 cache. Only not rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_hit",
+	.event = "event=0x24,umask=0x41,period=200003",
+	.desc = "Demand Data Read requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests that hit L2 cache. Only not rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.all_demand_data_rd",
+	.event = "event=0x24,umask=0xe1,period=200003",
+	.desc = "Demand Data Read requests",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests (including requests from L1D hardware prefetchers). These loads may hit or miss L2 cache. Only non rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.all_rfo",
+	.event = "event=0x24,umask=0xe2,period=200003",
+	.desc = "RFO requests to L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of RFO (read for ownership) requests to L2 cache. L2 RFO requests include both L1D demand RFO misses as well as L1D RFO prefetches",
+},
+{
+	.name = "l2_rqsts.all_code_rd",
+	.event = "event=0x24,umask=0xe4,period=200003",
+	.desc = "L2 code requests",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of L2 code requests",
+},
+{
+	.name = "l2_rqsts.all_pf",
+	.event = "event=0x24,umask=0xf8,period=200003",
+	.desc = "Requests from the L1/L2/L3 hardware prefetchers or Load software prefetches",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of requests from the L2 hardware prefetchers",
+},
+{
+	.name = "l2_rqsts.pf_miss",
+	.event = "event=0x24,umask=0x38,period=200003",
+	.desc = "Requests from the L1/L2/L3 hardware prefetchers or Load software prefetches that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "Requests from the L1/L2/L3 hardware prefetchers or Load software prefetches that miss L2 cache",
+},
+{
+	.name = "l2_rqsts.pf_hit",
+	.event = "event=0x24,umask=0xd8,period=200003",
+	.desc = "Requests from the L1/L2/L3 hardware prefetchers or Load software prefetches that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "Requests from the L1/L2/L3 hardware prefetchers or Load software prefetches that hit L2 cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x42,period=200003",
+	.desc = "RFO requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "RFO requests that hit L2 cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x22,period=200003",
+	.desc = "RFO requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "RFO requests that miss L2 cache",
+},
+{
+	.name = "l2_rqsts.code_rd_hit",
+	.event = "event=0x24,umask=0x44,period=200003",
+	.desc = "L2 cache hits when fetching instructions, code reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.code_rd_miss",
+	.event = "event=0x24,umask=0x24,period=200003",
+	.desc = "L2 cache misses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "L2 cache misses when fetching instructions",
+},
+{
+	.name = "l2_rqsts.all_demand_miss",
+	.event = "event=0x24,umask=0x27,period=200003",
+	.desc = "Demand requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "Demand requests that miss L2 cache",
+},
+{
+	.name = "l2_rqsts.all_demand_references",
+	.event = "event=0x24,umask=0xe7,period=200003",
+	.desc = "Demand requests to L2 cache",
+	.topic = "cache",
+	.long_desc = "Demand requests to L2 cache",
+},
+{
+	.name = "l2_rqsts.miss",
+	.event = "event=0x24,umask=0x3f,period=200003",
+	.desc = "All requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "All requests that miss L2 cache",
+},
+{
+	.name = "l2_rqsts.references",
+	.event = "event=0x24,umask=0xff,period=200003",
+	.desc = "All L2 requests",
+	.topic = "cache",
+	.long_desc = "All L2 requests",
+},
+{
+	.name = "l2_lines_out.silent",
+	.event = "event=0xF2,umask=0x1,period=200003",
+	.desc = "Counts the number of lines that are silently dropped by L2 cache when triggered by an L2 cache fill. These lines are typically in Shared or Exclusive state. A non-threaded event",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.non_silent",
+	.event = "event=0xF2,umask=0x2,period=200003",
+	.desc = "Counts the number of lines that are evicted by L2 cache when triggered by an L2 cache fill. Those lines are in Modified state. Modified lines are written back to L3",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.useless_pref",
+	.event = "event=0xF2,umask=0x4,period=200003",
+	.desc = "Counts the number of lines that have been hardware prefetched but not used and now evicted by L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts the number of lines that have been hardware prefetched but not used and now evicted by L2 cache",
+},
+{
+	.name = "l2_lines_in.all",
+	.event = "event=0xF1,umask=0x1f,period=100003",
+	.desc = "L2 cache lines filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_code_rd",
+	.event = "event=0x60,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles with offcore outstanding Code Reads transactions in the SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding Code Reads transactions in the super queue every cycle. The 'Offcore outstanding' state of the transaction lasts from the L2 miss until the sending transaction completion to requestor (SQ deallocation). See the corresponding Umask under OFFCORE_REQUESTS",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003,cmask=1",
+	.desc = "Cycles with offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding demand rfo Reads transactions in the super queue every cycle. The 'Offcore outstanding' state of the transaction lasts from the L2 miss until the sending transaction completion to requestor (SQ deallocation). See the corresponding Umask under OFFCORE_REQUESTS",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles_any",
+	.event = "event=0x48,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd_ge_6",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=6",
+	.desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.useless_hwpf",
+	.event = "event=0xF2,umask=0x4,period=200003",
+	.desc = "Counts the number of lines that have been hardware prefetched but not used and now evicted by L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l4_hit_local_l4.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0408000 ",
+	.desc = "OTHER & L4_HIT_LOCAL_L4 & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l4_hit_local_l4.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000408000 ",
+	.desc = "OTHER & L4_HIT_LOCAL_L4 & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l4_hit_local_l4.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400408000 ",
+	.desc = "OTHER & L4_HIT_LOCAL_L4 & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l4_hit_local_l4.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200408000 ",
+	.desc = "OTHER & L4_HIT_LOCAL_L4 & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l4_hit_local_l4.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100408000 ",
+	.desc = "OTHER & L4_HIT_LOCAL_L4 & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l4_hit_local_l4.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080408000 ",
+	.desc = "OTHER & L4_HIT_LOCAL_L4 & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l4_hit_local_l4.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040408000 ",
+	.desc = "OTHER & L4_HIT_LOCAL_L4 & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc01c8000 ",
+	.desc = "OTHER & L3_HIT & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10001c8000 ",
+	.desc = "OTHER & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04001c8000 ",
+	.desc = "Counts any other requests that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02001c8000 ",
+	.desc = "Counts any other requests that hit in the L3 and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01001c8000 ",
+	.desc = "Counts any other requests that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00801c8000 ",
+	.desc = "OTHER & L3_HIT & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00401c8000 ",
+	.desc = "OTHER & L3_HIT & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_s.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0108000 ",
+	.desc = "OTHER & L3_HIT_S & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_s.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000108000 ",
+	.desc = "OTHER & L3_HIT_S & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_s.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400108000 ",
+	.desc = "OTHER & L3_HIT_S & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_s.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200108000 ",
+	.desc = "OTHER & L3_HIT_S & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_s.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100108000 ",
+	.desc = "OTHER & L3_HIT_S & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_s.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080108000 ",
+	.desc = "OTHER & L3_HIT_S & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_s.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040108000 ",
+	.desc = "OTHER & L3_HIT_S & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_e.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0088000 ",
+	.desc = "OTHER & L3_HIT_E & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_e.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000088000 ",
+	.desc = "OTHER & L3_HIT_E & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_e.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400088000 ",
+	.desc = "OTHER & L3_HIT_E & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_e.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200088000 ",
+	.desc = "OTHER & L3_HIT_E & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_e.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100088000 ",
+	.desc = "OTHER & L3_HIT_E & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_e.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080088000 ",
+	.desc = "OTHER & L3_HIT_E & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_e.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040088000 ",
+	.desc = "OTHER & L3_HIT_E & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_m.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0048000 ",
+	.desc = "OTHER & L3_HIT_M & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_m.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000048000 ",
+	.desc = "OTHER & L3_HIT_M & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_m.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400048000 ",
+	.desc = "OTHER & L3_HIT_M & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_m.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200048000 ",
+	.desc = "OTHER & L3_HIT_M & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_m.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100048000 ",
+	.desc = "OTHER & L3_HIT_M & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_m.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080048000 ",
+	.desc = "OTHER & L3_HIT_M & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.l3_hit_m.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040048000 ",
+	.desc = "OTHER & L3_HIT_M & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.supplier_none.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040028000 ",
+	.desc = "OTHER & SUPPLIER_NONE & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000018000 ",
+	.desc = "Counts any other requests that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l4_hit_local_l4.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0400800 ",
+	.desc = "STREAMING_STORES & L4_HIT_LOCAL_L4 & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l4_hit_local_l4.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000400800 ",
+	.desc = "STREAMING_STORES & L4_HIT_LOCAL_L4 & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l4_hit_local_l4.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400400800 ",
+	.desc = "STREAMING_STORES & L4_HIT_LOCAL_L4 & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l4_hit_local_l4.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200400800 ",
+	.desc = "STREAMING_STORES & L4_HIT_LOCAL_L4 & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l4_hit_local_l4.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400800 ",
+	.desc = "STREAMING_STORES & L4_HIT_LOCAL_L4 & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l4_hit_local_l4.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080400800 ",
+	.desc = "STREAMING_STORES & L4_HIT_LOCAL_L4 & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l4_hit_local_l4.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040400800 ",
+	.desc = "STREAMING_STORES & L4_HIT_LOCAL_L4 & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc01c0800 ",
+	.desc = "STREAMING_STORES & L3_HIT & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10001c0800 ",
+	.desc = "STREAMING_STORES & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04001c0800 ",
+	.desc = "Counts streaming stores that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02001c0800 ",
+	.desc = "Counts streaming stores that hit in the L3 and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01001c0800 ",
+	.desc = "Counts streaming stores that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00801c0800 ",
+	.desc = "STREAMING_STORES & L3_HIT & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00401c0800 ",
+	.desc = "STREAMING_STORES & L3_HIT & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_s.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0100800 ",
+	.desc = "STREAMING_STORES & L3_HIT_S & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_s.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000100800 ",
+	.desc = "STREAMING_STORES & L3_HIT_S & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_s.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400100800 ",
+	.desc = "STREAMING_STORES & L3_HIT_S & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_s.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200100800 ",
+	.desc = "STREAMING_STORES & L3_HIT_S & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_s.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100100800 ",
+	.desc = "STREAMING_STORES & L3_HIT_S & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_s.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080100800 ",
+	.desc = "STREAMING_STORES & L3_HIT_S & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_s.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040100800 ",
+	.desc = "STREAMING_STORES & L3_HIT_S & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_e.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0080800 ",
+	.desc = "STREAMING_STORES & L3_HIT_E & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_e.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000080800 ",
+	.desc = "STREAMING_STORES & L3_HIT_E & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_e.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400080800 ",
+	.desc = "STREAMING_STORES & L3_HIT_E & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_e.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200080800 ",
+	.desc = "STREAMING_STORES & L3_HIT_E & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_e.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100080800 ",
+	.desc = "STREAMING_STORES & L3_HIT_E & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_e.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080080800 ",
+	.desc = "STREAMING_STORES & L3_HIT_E & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_e.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040080800 ",
+	.desc = "STREAMING_STORES & L3_HIT_E & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_m.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0040800 ",
+	.desc = "STREAMING_STORES & L3_HIT_M & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_m.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000040800 ",
+	.desc = "STREAMING_STORES & L3_HIT_M & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_m.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400040800 ",
+	.desc = "STREAMING_STORES & L3_HIT_M & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_m.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200040800 ",
+	.desc = "STREAMING_STORES & L3_HIT_M & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_m.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100040800 ",
+	.desc = "STREAMING_STORES & L3_HIT_M & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_m.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080040800 ",
+	.desc = "STREAMING_STORES & L3_HIT_M & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l3_hit_m.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040040800 ",
+	.desc = "STREAMING_STORES & L3_HIT_M & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0020800 ",
+	.desc = "STREAMING_STORES & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020800 ",
+	.desc = "STREAMING_STORES & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020800 ",
+	.desc = "STREAMING_STORES & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020800 ",
+	.desc = "STREAMING_STORES & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020800 ",
+	.desc = "STREAMING_STORES & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020800 ",
+	.desc = "STREAMING_STORES & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.supplier_none.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040020800 ",
+	.desc = "STREAMING_STORES & SUPPLIER_NONE & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010800 ",
+	.desc = "Counts streaming stores that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l4_hit_local_l4.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0400100 ",
+	.desc = "PF_L3_RFO & L4_HIT_LOCAL_L4 & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l4_hit_local_l4.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000400100 ",
+	.desc = "PF_L3_RFO & L4_HIT_LOCAL_L4 & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l4_hit_local_l4.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400400100 ",
+	.desc = "PF_L3_RFO & L4_HIT_LOCAL_L4 & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l4_hit_local_l4.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200400100 ",
+	.desc = "PF_L3_RFO & L4_HIT_LOCAL_L4 & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l4_hit_local_l4.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400100 ",
+	.desc = "PF_L3_RFO & L4_HIT_LOCAL_L4 & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l4_hit_local_l4.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080400100 ",
+	.desc = "PF_L3_RFO & L4_HIT_LOCAL_L4 & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l4_hit_local_l4.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040400100 ",
+	.desc = "PF_L3_RFO & L4_HIT_LOCAL_L4 & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc01c0100 ",
+	.desc = "PF_L3_RFO & L3_HIT & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10001c0100 ",
+	.desc = "PF_L3_RFO & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04001c0100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02001c0100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the L3 and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01001c0100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00801c0100 ",
+	.desc = "PF_L3_RFO & L3_HIT & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00401c0100 ",
+	.desc = "PF_L3_RFO & L3_HIT & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_s.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0100100 ",
+	.desc = "PF_L3_RFO & L3_HIT_S & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_s.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000100100 ",
+	.desc = "PF_L3_RFO & L3_HIT_S & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_s.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400100100 ",
+	.desc = "PF_L3_RFO & L3_HIT_S & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_s.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200100100 ",
+	.desc = "PF_L3_RFO & L3_HIT_S & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_s.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100100100 ",
+	.desc = "PF_L3_RFO & L3_HIT_S & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_s.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080100100 ",
+	.desc = "PF_L3_RFO & L3_HIT_S & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_s.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040100100 ",
+	.desc = "PF_L3_RFO & L3_HIT_S & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_e.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0080100 ",
+	.desc = "PF_L3_RFO & L3_HIT_E & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_e.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000080100 ",
+	.desc = "PF_L3_RFO & L3_HIT_E & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_e.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400080100 ",
+	.desc = "PF_L3_RFO & L3_HIT_E & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_e.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200080100 ",
+	.desc = "PF_L3_RFO & L3_HIT_E & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_e.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100080100 ",
+	.desc = "PF_L3_RFO & L3_HIT_E & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_e.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080080100 ",
+	.desc = "PF_L3_RFO & L3_HIT_E & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_e.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040080100 ",
+	.desc = "PF_L3_RFO & L3_HIT_E & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_m.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0040100 ",
+	.desc = "PF_L3_RFO & L3_HIT_M & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_m.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000040100 ",
+	.desc = "PF_L3_RFO & L3_HIT_M & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_m.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400040100 ",
+	.desc = "PF_L3_RFO & L3_HIT_M & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_m.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200040100 ",
+	.desc = "PF_L3_RFO & L3_HIT_M & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_m.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100040100 ",
+	.desc = "PF_L3_RFO & L3_HIT_M & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_m.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080040100 ",
+	.desc = "PF_L3_RFO & L3_HIT_M & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.l3_hit_m.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040040100 ",
+	.desc = "PF_L3_RFO & L3_HIT_M & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.supplier_none.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040020100 ",
+	.desc = "PF_L3_RFO & SUPPLIER_NONE & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010100 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l4_hit_local_l4.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0400080 ",
+	.desc = "PF_L3_DATA_RD & L4_HIT_LOCAL_L4 & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l4_hit_local_l4.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000400080 ",
+	.desc = "PF_L3_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l4_hit_local_l4.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400400080 ",
+	.desc = "PF_L3_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l4_hit_local_l4.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200400080 ",
+	.desc = "PF_L3_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l4_hit_local_l4.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400080 ",
+	.desc = "PF_L3_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l4_hit_local_l4.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080400080 ",
+	.desc = "PF_L3_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l4_hit_local_l4.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040400080 ",
+	.desc = "PF_L3_DATA_RD & L4_HIT_LOCAL_L4 & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc01c0080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10001c0080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04001c0080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02001c0080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the L3 and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01001c0080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00801c0080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00401c0080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_s.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0100080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_S & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_s.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000100080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_S & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_s.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400100080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_S & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_s.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200100080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_S & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_s.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100100080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_S & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_s.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080100080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_S & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_s.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040100080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_S & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_e.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0080080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_E & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_e.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000080080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_E & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_e.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400080080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_E & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_e.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200080080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_E & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_e.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100080080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_E & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_e.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080080080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_E & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_e.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040080080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_E & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_m.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0040080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_M & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_m.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000040080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_M & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_m.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400040080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_M & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_m.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200040080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_M & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_m.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100040080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_M & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_m.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080040080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_M & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.l3_hit_m.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040040080 ",
+	.desc = "PF_L3_DATA_RD & L3_HIT_M & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.supplier_none.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040020080 ",
+	.desc = "PF_L3_DATA_RD & SUPPLIER_NONE & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l3_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010080 ",
+	.desc = "Counts all prefetch (that bring data to LLC only) data reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l4_hit_local_l4.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0400004 ",
+	.desc = "DEMAND_CODE_RD & L4_HIT_LOCAL_L4 & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l4_hit_local_l4.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000400004 ",
+	.desc = "DEMAND_CODE_RD & L4_HIT_LOCAL_L4 & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l4_hit_local_l4.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400400004 ",
+	.desc = "DEMAND_CODE_RD & L4_HIT_LOCAL_L4 & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l4_hit_local_l4.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200400004 ",
+	.desc = "DEMAND_CODE_RD & L4_HIT_LOCAL_L4 & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l4_hit_local_l4.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400004 ",
+	.desc = "DEMAND_CODE_RD & L4_HIT_LOCAL_L4 & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l4_hit_local_l4.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080400004 ",
+	.desc = "DEMAND_CODE_RD & L4_HIT_LOCAL_L4 & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l4_hit_local_l4.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040400004 ",
+	.desc = "DEMAND_CODE_RD & L4_HIT_LOCAL_L4 & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc01c0004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10001c0004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04001c0004 ",
+	.desc = "Counts all demand code reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02001c0004 ",
+	.desc = "Counts all demand code reads that hit in the L3 and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01001c0004 ",
+	.desc = "Counts all demand code reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00801c0004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00401c0004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_s.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0100004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_S & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_s.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000100004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_S & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_s.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400100004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_S & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_s.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200100004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_S & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_s.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100100004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_S & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_s.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080100004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_S & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_s.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040100004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_S & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_e.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0080004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_E & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_e.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000080004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_E & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_e.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400080004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_E & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_e.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200080004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_E & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_e.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100080004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_E & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_e.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080080004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_E & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_e.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040080004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_E & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_m.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0040004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_M & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_m.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000040004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_M & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_m.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400040004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_M & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_m.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200040004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_M & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_m.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100040004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_M & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_m.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080040004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_M & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l3_hit_m.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040040004 ",
+	.desc = "DEMAND_CODE_RD & L3_HIT_M & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.supplier_none.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040020004 ",
+	.desc = "DEMAND_CODE_RD & SUPPLIER_NONE & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010004 ",
+	.desc = "Counts all demand code reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l4_hit_local_l4.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0400002 ",
+	.desc = "DEMAND_RFO & L4_HIT_LOCAL_L4 & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l4_hit_local_l4.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000400002 ",
+	.desc = "DEMAND_RFO & L4_HIT_LOCAL_L4 & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l4_hit_local_l4.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400400002 ",
+	.desc = "DEMAND_RFO & L4_HIT_LOCAL_L4 & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l4_hit_local_l4.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200400002 ",
+	.desc = "DEMAND_RFO & L4_HIT_LOCAL_L4 & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l4_hit_local_l4.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400002 ",
+	.desc = "DEMAND_RFO & L4_HIT_LOCAL_L4 & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l4_hit_local_l4.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080400002 ",
+	.desc = "DEMAND_RFO & L4_HIT_LOCAL_L4 & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l4_hit_local_l4.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040400002 ",
+	.desc = "DEMAND_RFO & L4_HIT_LOCAL_L4 & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc01c0002 ",
+	.desc = "DEMAND_RFO & L3_HIT & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10001c0002 ",
+	.desc = "DEMAND_RFO & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04001c0002 ",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02001c0002 ",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01001c0002 ",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00801c0002 ",
+	.desc = "DEMAND_RFO & L3_HIT & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00401c0002 ",
+	.desc = "DEMAND_RFO & L3_HIT & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_s.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0100002 ",
+	.desc = "DEMAND_RFO & L3_HIT_S & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_s.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000100002 ",
+	.desc = "DEMAND_RFO & L3_HIT_S & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_s.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400100002 ",
+	.desc = "DEMAND_RFO & L3_HIT_S & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_s.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200100002 ",
+	.desc = "DEMAND_RFO & L3_HIT_S & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_s.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100100002 ",
+	.desc = "DEMAND_RFO & L3_HIT_S & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_s.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080100002 ",
+	.desc = "DEMAND_RFO & L3_HIT_S & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_s.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040100002 ",
+	.desc = "DEMAND_RFO & L3_HIT_S & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_e.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0080002 ",
+	.desc = "DEMAND_RFO & L3_HIT_E & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_e.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000080002 ",
+	.desc = "DEMAND_RFO & L3_HIT_E & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_e.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400080002 ",
+	.desc = "DEMAND_RFO & L3_HIT_E & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_e.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200080002 ",
+	.desc = "DEMAND_RFO & L3_HIT_E & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_e.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100080002 ",
+	.desc = "DEMAND_RFO & L3_HIT_E & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_e.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080080002 ",
+	.desc = "DEMAND_RFO & L3_HIT_E & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_e.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040080002 ",
+	.desc = "DEMAND_RFO & L3_HIT_E & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_m.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0040002 ",
+	.desc = "DEMAND_RFO & L3_HIT_M & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_m.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000040002 ",
+	.desc = "DEMAND_RFO & L3_HIT_M & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_m.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400040002 ",
+	.desc = "DEMAND_RFO & L3_HIT_M & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_m.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200040002 ",
+	.desc = "DEMAND_RFO & L3_HIT_M & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_m.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100040002 ",
+	.desc = "DEMAND_RFO & L3_HIT_M & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_m.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080040002 ",
+	.desc = "DEMAND_RFO & L3_HIT_M & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l3_hit_m.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040040002 ",
+	.desc = "DEMAND_RFO & L3_HIT_M & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0020002 ",
+	.desc = "DEMAND_RFO & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020002 ",
+	.desc = "DEMAND_RFO & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020002 ",
+	.desc = "DEMAND_RFO & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020002 ",
+	.desc = "DEMAND_RFO & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020002 ",
+	.desc = "DEMAND_RFO & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020002 ",
+	.desc = "DEMAND_RFO & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.supplier_none.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040020002 ",
+	.desc = "DEMAND_RFO & SUPPLIER_NONE & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010002 ",
+	.desc = "Counts all demand data writes (RFOs) that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l4_hit_local_l4.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0400001 ",
+	.desc = "DEMAND_DATA_RD & L4_HIT_LOCAL_L4 & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l4_hit_local_l4.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000400001 ",
+	.desc = "DEMAND_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l4_hit_local_l4.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400400001 ",
+	.desc = "DEMAND_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l4_hit_local_l4.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200400001 ",
+	.desc = "DEMAND_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l4_hit_local_l4.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100400001 ",
+	.desc = "DEMAND_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l4_hit_local_l4.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080400001 ",
+	.desc = "DEMAND_DATA_RD & L4_HIT_LOCAL_L4 & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l4_hit_local_l4.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040400001 ",
+	.desc = "DEMAND_DATA_RD & L4_HIT_LOCAL_L4 & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc01c0001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10001c0001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04001c0001 ",
+	.desc = "Counts demand data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x02001c0001 ",
+	.desc = "Counts demand data reads that hit in the L3 and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x01001c0001 ",
+	.desc = "Counts demand data reads that hit in the L3 and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00801c0001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x00401c0001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_s.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0100001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_S & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_s.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000100001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_S & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_s.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400100001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_S & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_s.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200100001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_S & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_s.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100100001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_S & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_s.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080100001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_S & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_s.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040100001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_S & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_e.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0080001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_E & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_e.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000080001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_E & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_e.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400080001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_E & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_e.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200080001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_E & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_e.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100080001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_E & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_e.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080080001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_E & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_e.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040080001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_E & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_m.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0040001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_M & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_m.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000040001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_M & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_m.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400040001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_M & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_m.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200040001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_M & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_m.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100040001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_M & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_m.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080040001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_M & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l3_hit_m.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040040001 ",
+	.desc = "DEMAND_DATA_RD & L3_HIT_M & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.any_snoop",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fc0020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & ANY_SNOOP",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1000020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_HITM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_hit_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0400020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_HIT_NO_FWD",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0200020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_MISS",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_not_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0100020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_NOT_NEEDED",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.snoop_none",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0080020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SNOOP_NONE",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.supplier_none.spl_hit",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0040020001 ",
+	.desc = "DEMAND_DATA_RD & SUPPLIER_NONE & SPL_HIT",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0000010001 ",
+	.desc = "Counts demand data reads that have any response type",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_nehalemex[] = {
+{
+	.name = "fp_assist.all",
+	.event = "event=0xF7,umask=0x1,period=20000",
+	.desc = "X87 Floating point assists (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.input",
+	.event = "event=0xF7,umask=0x4,period=20000",
+	.desc = "X87 Floating poiint assists for invalid input value (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.output",
+	.event = "event=0xF7,umask=0x2,period=20000",
+	.desc = "X87 Floating point assists for invalid output value (Precise Event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.mmx",
+	.event = "event=0x10,umask=0x2,period=2000000",
+	.desc = "MMX Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_double_precision",
+	.event = "event=0x10,umask=0x80,period=2000000",
+	.desc = "SSE* FP double precision Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp",
+	.event = "event=0x10,umask=0x4,period=2000000",
+	.desc = "SSE and SSE2 FP Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp_packed",
+	.event = "event=0x10,umask=0x10,period=2000000",
+	.desc = "SSE FP packed Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_fp_scalar",
+	.event = "event=0x10,umask=0x20,period=2000000",
+	.desc = "SSE FP scalar Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse_single_precision",
+	.event = "event=0x10,umask=0x40,period=2000000",
+	.desc = "SSE* FP single precision Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.sse2_integer",
+	.event = "event=0x10,umask=0x8,period=2000000",
+	.desc = "SSE2 integer Uops",
+	.topic = "floating point",
+},
+{
+	.name = "fp_comp_ops_exe.x87",
+	.event = "event=0x10,umask=0x1,period=2000000",
+	.desc = "Computational floating-point operations executed",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.any",
+	.event = "event=0xCC,umask=0x3,period=2000000",
+	.desc = "All Floating Point to and from MMX transitions",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.to_fp",
+	.event = "event=0xCC,umask=0x1,period=2000000",
+	.desc = "Transitions from MMX to Floating Point instructions",
+	.topic = "floating point",
+},
+{
+	.name = "fp_mmx_trans.to_mmx",
+	.event = "event=0xCC,umask=0x2,period=2000000",
+	.desc = "Transitions from Floating Point to MMX instructions",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.pack",
+	.event = "event=0x12,umask=0x4,period=200000",
+	.desc = "128 bit SIMD integer pack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_arith",
+	.event = "event=0x12,umask=0x20,period=200000",
+	.desc = "128 bit SIMD integer arithmetic operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_logical",
+	.event = "event=0x12,umask=0x10,period=200000",
+	.desc = "128 bit SIMD integer logical operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_mpy",
+	.event = "event=0x12,umask=0x1,period=200000",
+	.desc = "128 bit SIMD integer multiply operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.packed_shift",
+	.event = "event=0x12,umask=0x2,period=200000",
+	.desc = "128 bit SIMD integer shift operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.shuffle_move",
+	.event = "event=0x12,umask=0x40,period=200000",
+	.desc = "128 bit SIMD integer shuffle/move operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_128.unpack",
+	.event = "event=0x12,umask=0x8,period=200000",
+	.desc = "128 bit SIMD integer unpack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.pack",
+	.event = "event=0xFD,umask=0x4,period=200000",
+	.desc = "SIMD integer 64 bit pack operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_arith",
+	.event = "event=0xFD,umask=0x20,period=200000",
+	.desc = "SIMD integer 64 bit arithmetic operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_logical",
+	.event = "event=0xFD,umask=0x10,period=200000",
+	.desc = "SIMD integer 64 bit logical operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_mpy",
+	.event = "event=0xFD,umask=0x1,period=200000",
+	.desc = "SIMD integer 64 bit packed multiply operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.packed_shift",
+	.event = "event=0xFD,umask=0x2,period=200000",
+	.desc = "SIMD integer 64 bit shift operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.shuffle_move",
+	.event = "event=0xFD,umask=0x40,period=200000",
+	.desc = "SIMD integer 64 bit shuffle/move operations",
+	.topic = "floating point",
+},
+{
+	.name = "simd_int_64.unpack",
+	.event = "event=0xFD,umask=0x8,period=200000",
+	.desc = "SIMD integer 64 bit unpack operations",
+	.topic = "floating point",
+},
+{
+	.name = "arith.cycles_div_busy",
+	.event = "event=0x14,umask=0x1,period=2000000",
+	.desc = "Cycles the divider is busy",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.div",
+	.event = "event=0x14,inv=1,umask=0x1,period=2000000,cmask=1,edge=1",
+	.desc = "Divide Operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "arith.mul",
+	.event = "event=0x14,umask=0x2,period=2000000",
+	.desc = "Multiply operations executed",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear.bad_target",
+	.event = "event=0xE6,umask=0x2,period=2000000",
+	.desc = "BACLEAR asserted with bad target address",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear.clear",
+	.event = "event=0xE6,umask=0x1,period=2000000",
+	.desc = "BACLEAR asserted, regardless of cause ",
+	.topic = "pipeline",
+},
+{
+	.name = "baclear_force_iq",
+	.event = "event=0xA7,umask=0x1,period=2000000",
+	.desc = "Instruction queue forced BACLEAR",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_decoded",
+	.event = "event=0xE0,umask=0x1,period=2000000",
+	.desc = "Branch instructions decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.any",
+	.event = "event=0x88,umask=0x7f,period=200000",
+	.desc = "Branch instructions executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.cond",
+	.event = "event=0x88,umask=0x1,period=200000",
+	.desc = "Conditional branch instructions executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.direct",
+	.event = "event=0x88,umask=0x2,period=200000",
+	.desc = "Unconditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.direct_near_call",
+	.event = "event=0x88,umask=0x10,period=20000",
+	.desc = "Unconditional call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.indirect_near_call",
+	.event = "event=0x88,umask=0x20,period=20000",
+	.desc = "Indirect call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.indirect_non_call",
+	.event = "event=0x88,umask=0x4,period=20000",
+	.desc = "Indirect non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.near_calls",
+	.event = "event=0x88,umask=0x30,period=20000",
+	.desc = "Call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.non_calls",
+	.event = "event=0x88,umask=0x7,period=200000",
+	.desc = "All non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.return_near",
+	.event = "event=0x88,umask=0x8,period=20000",
+	.desc = "Indirect return branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.taken",
+	.event = "event=0x88,umask=0x40,period=200000",
+	.desc = "Taken branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x4,period=200000",
+	.desc = "Retired branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=200000",
+	.desc = "Retired conditional branch instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=20000",
+	.desc = "Retired near call instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.any",
+	.event = "event=0x89,umask=0x7f,period=20000",
+	.desc = "Mispredicted branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.cond",
+	.event = "event=0x89,umask=0x1,period=20000",
+	.desc = "Mispredicted conditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.direct",
+	.event = "event=0x89,umask=0x2,period=20000",
+	.desc = "Mispredicted unconditional branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.direct_near_call",
+	.event = "event=0x89,umask=0x10,period=2000",
+	.desc = "Mispredicted non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.indirect_near_call",
+	.event = "event=0x89,umask=0x20,period=2000",
+	.desc = "Mispredicted indirect call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.indirect_non_call",
+	.event = "event=0x89,umask=0x4,period=2000",
+	.desc = "Mispredicted indirect non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.near_calls",
+	.event = "event=0x89,umask=0x30,period=2000",
+	.desc = "Mispredicted call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.non_calls",
+	.event = "event=0x89,umask=0x7,period=20000",
+	.desc = "Mispredicted non call branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.return_near",
+	.event = "event=0x89,umask=0x8,period=2000",
+	.desc = "Mispredicted return branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_exec.taken",
+	.event = "event=0x89,umask=0x40,period=20000",
+	.desc = "Mispredicted taken branches executed",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_call",
+	.event = "event=0xC5,umask=0x2,period=2000",
+	.desc = "Mispredicted near retired calls (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref",
+	.event = "event=0x0,umask=0x03",
+	.desc = "Reference cycles when thread is not halted (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_p",
+	.event = "event=0x3C,umask=0x1,period=100000",
+	.desc = "Reference base clock (133 Mhz) cycles when thread is not halted (programmable counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Cycles when thread is not halted (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000000",
+	.desc = "Cycles when thread is not halted (programmable counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.total_cycles",
+	.event = "event=0x3C,inv=1,umask=0x0,period=2000000,cmask=2",
+	.desc = "Total CPU cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.any",
+	.event = "event=0x87,umask=0xf,period=2000000",
+	.desc = "Any Instruction Length Decoder stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.iq_full",
+	.event = "event=0x87,umask=0x4,period=2000000",
+	.desc = "Instruction Queue full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000000",
+	.desc = "Length Change Prefix stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.mru",
+	.event = "event=0x87,umask=0x2,period=2000000",
+	.desc = "Stall cycles due to BPU MRU bypass",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.regen",
+	.event = "event=0x87,umask=0x8,period=2000000",
+	.desc = "Regen stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_decoded.dec0",
+	.event = "event=0x18,umask=0x1,period=2000000",
+	.desc = "Instructions that must be decoded by decoder 0",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_queue_write_cycles",
+	.event = "event=0x1E,umask=0x1,period=2000000",
+	.desc = "Cycles instructions are written to the instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_queue_writes",
+	.event = "event=0x17,umask=0x1,period=2000000",
+	.desc = "Instructions written to instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (fixed counter)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Instructions retired (Programmable counter and Precise Event) (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.mmx",
+	.event = "event=0xC0,umask=0x4,period=2000000",
+	.desc = "Retired MMX instructions (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.total_cycles",
+	.event = "event=0xC0,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.x87",
+	.event = "event=0xC0,umask=0x2,period=2000000",
+	.desc = "Retired floating-point operations (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre",
+	.event = "event=0x4C,umask=0x1,period=200000",
+	.desc = "Load operations conflicting with software prefetches",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.active",
+	.event = "event=0xA8,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles when uops were delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.inactive",
+	.event = "event=0xA8,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no uops were delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd_overflow",
+	.event = "event=0x20,umask=0x1,period=2000000",
+	.desc = "Loops that can't stream from the instruction queue",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.cycles",
+	.event = "event=0xC3,umask=0x1,period=20000",
+	.desc = "Cycles machine clear asserted",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.mem_order",
+	.event = "event=0xC3,umask=0x2,period=20000",
+	.desc = "Execution pipeline restart due to Memory ordering conflicts",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=20000",
+	.desc = "Self-Modifying Code detected",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000000",
+	.desc = "Resource related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.fpcw",
+	.event = "event=0xA2,umask=0x20,period=2000000",
+	.desc = "FPU control word write stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.load",
+	.event = "event=0xA2,umask=0x2,period=2000000",
+	.desc = "Load buffer stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.mxcsr",
+	.event = "event=0xA2,umask=0x40,period=2000000",
+	.desc = "MXCSR rename stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.other",
+	.event = "event=0xA2,umask=0x80,period=2000000",
+	.desc = "Other Resource related stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rob_full",
+	.event = "event=0xA2,umask=0x10,period=2000000",
+	.desc = "ROB full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.rs_full",
+	.event = "event=0xA2,umask=0x4,period=2000000",
+	.desc = "Reservation Station full stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.store",
+	.event = "event=0xA2,umask=0x8,period=2000000",
+	.desc = "Store buffer stall cycles",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.packed_double",
+	.event = "event=0xC7,umask=0x4,period=200000",
+	.desc = "SIMD Packed-Double Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.packed_single",
+	.event = "event=0xC7,umask=0x1,period=200000",
+	.desc = "SIMD Packed-Single Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.scalar_double",
+	.event = "event=0xC7,umask=0x8,period=200000",
+	.desc = "SIMD Scalar-Double Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.scalar_single",
+	.event = "event=0xC7,umask=0x2,period=200000",
+	.desc = "SIMD Scalar-Single Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "ssex_uops_retired.vector_integer",
+	.event = "event=0xC7,umask=0x10,period=200000",
+	.desc = "SIMD Vector Integer Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uop_unfusion",
+	.event = "event=0xDB,umask=0x1,period=2000000",
+	.desc = "Uop unfusions due to FP exceptions",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.esp_folding",
+	.event = "event=0xD1,umask=0x4,period=2000000",
+	.desc = "Stack pointer instructions decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.esp_sync",
+	.event = "event=0xD1,umask=0x8,period=2000000",
+	.desc = "Stack pointer sync operations",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.ms_cycles_active",
+	.event = "event=0xD1,umask=0x2,period=2000000,cmask=1",
+	.desc = "Uops decoded by Microcode Sequencer",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_decoded.stall_cycles",
+	.event = "event=0xD1,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops are decoded",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_active_cycles",
+	.event = "event=0xB1,umask=0x3f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops executed on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_active_cycles_no_port5",
+	.event = "event=0xB1,umask=0x1f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops executed on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_count",
+	.event = "event=0xB1,inv=1,umask=0x3f,any=1,period=2000000,cmask=1,edge=1",
+	.desc = "Uops executed on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_count_no_port5",
+	.event = "event=0xB1,inv=1,umask=0x1f,any=1,period=2000000,cmask=1,edge=1",
+	.desc = "Uops executed on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x3f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on any port (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_stall_cycles_no_port5",
+	.event = "event=0xB1,inv=1,umask=0x1f,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on ports 0-4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port0",
+	.event = "event=0xB1,umask=0x1,period=2000000",
+	.desc = "Uops executed on port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port015",
+	.event = "event=0xB1,umask=0x40,period=2000000",
+	.desc = "Uops issued on ports 0, 1 or 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port015_stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x40,period=2000000,cmask=1",
+	.desc = "Cycles no Uops issued on ports 0, 1 or 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port1",
+	.event = "event=0xB1,umask=0x2,period=2000000",
+	.desc = "Uops executed on port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port2_core",
+	.event = "event=0xB1,umask=0x4,any=1,period=2000000",
+	.desc = "Uops executed on port 2 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port234_core",
+	.event = "event=0xB1,umask=0x80,any=1,period=2000000",
+	.desc = "Uops issued on ports 2, 3 or 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port3_core",
+	.event = "event=0xB1,umask=0x8,any=1,period=2000000",
+	.desc = "Uops executed on port 3 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port4_core",
+	.event = "event=0xB1,umask=0x10,any=1,period=2000000",
+	.desc = "Uops executed on port 4 (core count)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.port5",
+	.event = "event=0xB1,umask=0x20,period=2000000",
+	.desc = "Uops executed on port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0xE,umask=0x1,period=2000000",
+	.desc = "Uops issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.core_stall_cycles",
+	.event = "event=0xE,inv=1,umask=0x1,any=1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops were issued on any thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.cycles_all_threads",
+	.event = "event=0xE,umask=0x1,any=1,period=2000000,cmask=1",
+	.desc = "Cycles Uops were issued on either thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.fused",
+	.event = "event=0xE,umask=0x2,period=2000000",
+	.desc = "Fused Uops issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0xE,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles no Uops were issued",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.active_cycles",
+	.event = "event=0xC2,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles Uops are being retired (Precise event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.any",
+	.event = "event=0xC2,umask=0x1,period=2000000",
+	.desc = "Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.macro_fused",
+	.event = "event=0xC2,umask=0x4,period=2000000",
+	.desc = "Macro-fused Uops retired (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000000",
+	.desc = "Retirement slots used (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000000,cmask=1",
+	.desc = "Cycles Uops are not retiring (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles using precise uop retired event (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.total_cycles_ps",
+	.event = "event=0xC0,inv=1,umask=0x1,period=2000000,cmask=16",
+	.desc = "Total cycles (Precise Event)",
+	.topic = "pipeline",
+},
+{
+	.name = "macro_insts.decoded",
+	.event = "event=0xD0,umask=0x1,period=2000000",
+	.desc = "Instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "macro_insts.fusions_decoded",
+	.event = "event=0xA6,umask=0x1,period=2000000",
+	.desc = "Macro-fused instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "two_uop_insts_decoded",
+	.event = "event=0x19,umask=0x1,period=2000000",
+	.desc = "Two Uop instructions decoded",
+	.topic = "frontend",
+},
+{
+	.name = "bpu_clears.early",
+	.event = "event=0xE8,umask=0x1,period=2000000",
+	.desc = "Early Branch Prediciton Unit clears",
+	.topic = "other",
+},
+{
+	.name = "bpu_clears.late",
+	.event = "event=0xE8,umask=0x2,period=2000000",
+	.desc = "Late Branch Prediction Unit clears",
+	.topic = "other",
+},
+{
+	.name = "bpu_missed_call_ret",
+	.event = "event=0xE5,umask=0x1,period=2000000",
+	.desc = "Branch prediction unit missed call or return",
+	.topic = "other",
+},
+{
+	.name = "es_reg_renames",
+	.event = "event=0xD5,umask=0x1,period=2000000",
+	.desc = "ES segment renames",
+	.topic = "other",
+},
+{
+	.name = "io_transactions",
+	.event = "event=0x6C,umask=0x1,period=2000000",
+	.desc = "I/O transactions",
+	.topic = "other",
+},
+{
+	.name = "l1i.cycles_stalled",
+	.event = "event=0x80,umask=0x4,period=2000000",
+	.desc = "L1I instruction fetch stall cycles",
+	.topic = "other",
+},
+{
+	.name = "l1i.hits",
+	.event = "event=0x80,umask=0x1,period=2000000",
+	.desc = "L1I instruction fetch hits",
+	.topic = "other",
+},
+{
+	.name = "l1i.misses",
+	.event = "event=0x80,umask=0x2,period=2000000",
+	.desc = "L1I instruction fetch misses",
+	.topic = "other",
+},
+{
+	.name = "l1i.reads",
+	.event = "event=0x80,umask=0x3,period=2000000",
+	.desc = "L1I Instruction fetches",
+	.topic = "other",
+},
+{
+	.name = "large_itlb.hit",
+	.event = "event=0x82,umask=0x1,period=200000",
+	.desc = "Large ITLB hit",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.any",
+	.event = "event=0x13,umask=0x7,period=2000000",
+	.desc = "All loads dispatched",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.mob",
+	.event = "event=0x13,umask=0x4,period=2000000",
+	.desc = "Loads dispatched from the MOB",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.rs",
+	.event = "event=0x13,umask=0x1,period=2000000",
+	.desc = "Loads dispatched that bypass the MOB",
+	.topic = "other",
+},
+{
+	.name = "load_dispatch.rs_delayed",
+	.event = "event=0x13,umask=0x2,period=2000000",
+	.desc = "Loads dispatched from stage 305",
+	.topic = "other",
+},
+{
+	.name = "partial_address_alias",
+	.event = "event=0x7,umask=0x1,period=200000",
+	.desc = "False dependencies due to partial address aliasing",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.any",
+	.event = "event=0xD2,umask=0xf,period=2000000",
+	.desc = "All RAT stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.flags",
+	.event = "event=0xD2,umask=0x1,period=2000000",
+	.desc = "Flag stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.registers",
+	.event = "event=0xD2,umask=0x2,period=2000000",
+	.desc = "Partial register stall cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.rob_read_port",
+	.event = "event=0xD2,umask=0x4,period=2000000",
+	.desc = "ROB read port stalls cycles",
+	.topic = "other",
+},
+{
+	.name = "rat_stalls.scoreboard",
+	.event = "event=0xD2,umask=0x8,period=2000000",
+	.desc = "Scoreboard stall cycles",
+	.topic = "other",
+},
+{
+	.name = "sb_drain.any",
+	.event = "event=0x4,umask=0x7,period=200000",
+	.desc = "All Store buffer stall cycles",
+	.topic = "other",
+},
+{
+	.name = "seg_rename_stalls",
+	.event = "event=0xD4,umask=0x1,period=2000000",
+	.desc = "Segment rename stall cycles",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hit",
+	.event = "event=0xB8,umask=0x1,period=100000",
+	.desc = "Thread responded HIT to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hite",
+	.event = "event=0xB8,umask=0x2,period=100000",
+	.desc = "Thread responded HITE to snoop",
+	.topic = "other",
+},
+{
+	.name = "snoop_response.hitm",
+	.event = "event=0xB8,umask=0x4,period=100000",
+	.desc = "Thread responded HITM to snoop",
+	.topic = "other",
+},
+{
+	.name = "sq_full_stall_cycles",
+	.event = "event=0xF6,umask=0x1,period=2000000",
+	.desc = "Super Queue full stall cycles",
+	.topic = "other",
+},
+{
+	.name = "offcore_response.any_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6011",
+	.desc = "Offcore data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF811",
+	.desc = "Offcore data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4011",
+	.desc = "Offcore data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2011",
+	.desc = "Offcore data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6044",
+	.desc = "Offcore code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF844",
+	.desc = "Offcore code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4044",
+	.desc = "Offcore code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2044",
+	.desc = "Offcore code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x60FF",
+	.desc = "Offcore requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF8FF",
+	.desc = "Offcore requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x40FF",
+	.desc = "Offcore requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_request.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x20FF",
+	.desc = "Offcore requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6022",
+	.desc = "Offcore RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF822",
+	.desc = "Offcore RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4022",
+	.desc = "Offcore RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.any_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2022",
+	.desc = "Offcore RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6008",
+	.desc = "Offcore writebacks to any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF808",
+	.desc = "Offcore writebacks that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4008",
+	.desc = "Offcore writebacks to the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.corewb.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2008",
+	.desc = "Offcore writebacks to a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6077",
+	.desc = "Offcore code or data read requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF877",
+	.desc = "Offcore code or data read requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4077",
+	.desc = "Offcore code or data read requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2077",
+	.desc = "Offcore code or data read requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6033",
+	.desc = "Offcore request = all data, response = any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF833",
+	.desc = "Offcore request = all data, response = any LLC miss",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4033",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.data_in.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2033",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6003",
+	.desc = "Offcore demand data requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF803",
+	.desc = "Offcore demand data requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4003",
+	.desc = "Offcore demand data requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2003",
+	.desc = "Offcore demand data requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6001",
+	.desc = "Offcore demand data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF801",
+	.desc = "Offcore demand data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4001",
+	.desc = "Offcore demand data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2001",
+	.desc = "Offcore demand data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6004",
+	.desc = "Offcore demand code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF804",
+	.desc = "Offcore demand code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4004",
+	.desc = "Offcore demand code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2004",
+	.desc = "Offcore demand code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6002",
+	.desc = "Offcore demand RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF802",
+	.desc = "Offcore demand RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4002",
+	.desc = "Offcore demand RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2002",
+	.desc = "Offcore demand RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6080",
+	.desc = "Offcore other requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF880",
+	.desc = "Offcore other requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.other.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2080",
+	.desc = "Offcore other requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6030",
+	.desc = "Offcore prefetch data requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF830",
+	.desc = "Offcore prefetch data requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4030",
+	.desc = "Offcore prefetch data requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2030",
+	.desc = "Offcore prefetch data requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6010",
+	.desc = "Offcore prefetch data reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF810",
+	.desc = "Offcore prefetch data reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4010",
+	.desc = "Offcore prefetch data reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2010",
+	.desc = "Offcore prefetch data reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6040",
+	.desc = "Offcore prefetch code reads satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF840",
+	.desc = "Offcore prefetch code reads that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4040",
+	.desc = "Offcore prefetch code reads satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2040",
+	.desc = "Offcore prefetch code reads satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6020",
+	.desc = "Offcore prefetch RFO requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF820",
+	.desc = "Offcore prefetch RFO requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4020",
+	.desc = "Offcore prefetch RFO requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2020",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x6070",
+	.desc = "Offcore prefetch requests satisfied by any DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.any_llc_miss",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xF870",
+	.desc = "Offcore prefetch requests that missed the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4070",
+	.desc = "Offcore prefetch requests satisfied by the local DRAM",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.prefetch.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2070",
+	.desc = "Offcore prefetch requests satisfied by a remote DRAM",
+	.topic = "memory",
+},
+{
+	.name = "dtlb_load_misses.any",
+	.event = "event=0x8,umask=0x1,period=200000",
+	.desc = "DTLB load misses",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.pde_miss",
+	.event = "event=0x8,umask=0x20,period=200000",
+	.desc = "DTLB load miss caused by low part of address",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x8,umask=0x10,period=2000000",
+	.desc = "DTLB second level hit",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x8,umask=0x2,period=200000",
+	.desc = "DTLB load miss page walks complete",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.any",
+	.event = "event=0x49,umask=0x1,period=200000",
+	.desc = "DTLB misses",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.stlb_hit",
+	.event = "event=0x49,umask=0x10,period=200000",
+	.desc = "DTLB first level misses but second level hit",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_misses.walk_completed",
+	.event = "event=0x49,umask=0x2,period=200000",
+	.desc = "DTLB miss page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=2000000",
+	.desc = "ITLB flushes",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_miss_retired",
+	.event = "event=0xC8,umask=0x20,period=200000",
+	.desc = "Retired instructions that missed the ITLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.any",
+	.event = "event=0x85,umask=0x1,period=200000",
+	.desc = "ITLB miss",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0x2,period=200000",
+	.desc = "ITLB miss page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_load_retired.dtlb_miss",
+	.event = "event=0xCB,umask=0x80,period=200000",
+	.desc = "Retired loads that miss the DTLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "mem_store_retired.dtlb_miss",
+	.event = "event=0xC,umask=0x1,period=200000",
+	.desc = "Retired stores that miss the DTLB (Precise Event)",
+	.topic = "virtual memory",
+},
+{
+	.name = "cache_lock_cycles.l1d",
+	.event = "event=0x63,umask=0x2,period=2000000",
+	.desc = "Cycles L1D locked",
+	.topic = "cache",
+},
+{
+	.name = "cache_lock_cycles.l1d_l2",
+	.event = "event=0x63,umask=0x1,period=2000000",
+	.desc = "Cycles L1D and L2 locked",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_evict",
+	.event = "event=0x51,umask=0x4,period=2000000",
+	.desc = "L1D cache lines replaced in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_repl",
+	.event = "event=0x51,umask=0x2,period=2000000",
+	.desc = "L1D cache lines allocated in the M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.m_snoop_evict",
+	.event = "event=0x51,umask=0x8,period=2000000",
+	.desc = "L1D snoop eviction of cache lines in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d.repl",
+	.event = "event=0x51,umask=0x1,period=2000000",
+	.desc = "L1 data cache lines allocated",
+	.topic = "cache",
+},
+{
+	.name = "l1d_all_ref.any",
+	.event = "event=0x43,umask=0x1,period=2000000",
+	.desc = "All references to the L1 data cache",
+	.topic = "cache",
+},
+{
+	.name = "l1d_all_ref.cacheable",
+	.event = "event=0x43,umask=0x2,period=2000000",
+	.desc = "L1 data cacheable reads and writes",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_ld.e_state",
+	.event = "event=0x40,umask=0x4,period=2000000",
+	.desc = "L1 data cache read in E state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_ld.i_state",
+	.event = "event=0x40,umask=0x1,period=2000000",
+	.desc = "L1 data cache read in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_ld.m_state",
+	.event = "event=0x40,umask=0x8,period=2000000",
+	.desc = "L1 data cache read in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_ld.mesi",
+	.event = "event=0x40,umask=0xf,period=2000000",
+	.desc = "L1 data cache reads",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_ld.s_state",
+	.event = "event=0x40,umask=0x2,period=2000000",
+	.desc = "L1 data cache read in S state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_lock.e_state",
+	.event = "event=0x42,umask=0x4,period=2000000",
+	.desc = "L1 data cache load locks in E state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_lock.hit",
+	.event = "event=0x42,umask=0x1,period=2000000",
+	.desc = "L1 data cache load lock hits",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_lock.m_state",
+	.event = "event=0x42,umask=0x8,period=2000000",
+	.desc = "L1 data cache load locks in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_lock.s_state",
+	.event = "event=0x42,umask=0x2,period=2000000",
+	.desc = "L1 data cache load locks in S state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_lock_fb_hit",
+	.event = "event=0x53,umask=0x1,period=2000000",
+	.desc = "L1D load lock accepted in fill buffer",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_prefetch_lock_fb_hit",
+	.event = "event=0x52,umask=0x1,period=2000000",
+	.desc = "L1D prefetch load lock accepted in fill buffer",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_st.e_state",
+	.event = "event=0x41,umask=0x4,period=2000000",
+	.desc = "L1 data cache stores in E state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_st.m_state",
+	.event = "event=0x41,umask=0x8,period=2000000",
+	.desc = "L1 data cache stores in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_cache_st.s_state",
+	.event = "event=0x41,umask=0x2,period=2000000",
+	.desc = "L1 data cache stores in S state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.miss",
+	.event = "event=0x4E,umask=0x2,period=200000",
+	.desc = "L1D hardware prefetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.requests",
+	.event = "event=0x4E,umask=0x1,period=200000",
+	.desc = "L1D hardware prefetch requests",
+	.topic = "cache",
+},
+{
+	.name = "l1d_prefetch.triggers",
+	.event = "event=0x4E,umask=0x4,period=200000",
+	.desc = "L1D hardware prefetch requests triggered",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.e_state",
+	.event = "event=0x28,umask=0x4,period=100000",
+	.desc = "L1 writebacks to L2 in E state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.i_state",
+	.event = "event=0x28,umask=0x1,period=100000",
+	.desc = "L1 writebacks to L2 in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.m_state",
+	.event = "event=0x28,umask=0x8,period=100000",
+	.desc = "L1 writebacks to L2 in M state",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.mesi",
+	.event = "event=0x28,umask=0xf,period=100000",
+	.desc = "All L1 writebacks to L2",
+	.topic = "cache",
+},
+{
+	.name = "l1d_wb_l2.s_state",
+	.event = "event=0x28,umask=0x2,period=100000",
+	.desc = "L1 writebacks to L2 in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.any",
+	.event = "event=0x26,umask=0xff,period=200000",
+	.desc = "All L2 data requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.e_state",
+	.event = "event=0x26,umask=0x4,period=200000",
+	.desc = "L2 data demand loads in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.i_state",
+	.event = "event=0x26,umask=0x1,period=200000",
+	.desc = "L2 data demand loads in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.m_state",
+	.event = "event=0x26,umask=0x8,period=200000",
+	.desc = "L2 data demand loads in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.mesi",
+	.event = "event=0x26,umask=0xf,period=200000",
+	.desc = "L2 data demand requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.demand.s_state",
+	.event = "event=0x26,umask=0x2,period=200000",
+	.desc = "L2 data demand loads in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.e_state",
+	.event = "event=0x26,umask=0x40,period=200000",
+	.desc = "L2 data prefetches in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.i_state",
+	.event = "event=0x26,umask=0x10,period=200000",
+	.desc = "L2 data prefetches in the I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.m_state",
+	.event = "event=0x26,umask=0x80,period=200000",
+	.desc = "L2 data prefetches in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.mesi",
+	.event = "event=0x26,umask=0xf0,period=200000",
+	.desc = "All L2 data prefetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_data_rqsts.prefetch.s_state",
+	.event = "event=0x26,umask=0x20,period=200000",
+	.desc = "L2 data prefetches in the S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.any",
+	.event = "event=0xF1,umask=0x7,period=100000",
+	.desc = "L2 lines alloacated",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.e_state",
+	.event = "event=0xF1,umask=0x4,period=100000",
+	.desc = "L2 lines allocated in the E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_in.s_state",
+	.event = "event=0xF1,umask=0x2,period=100000",
+	.desc = "L2 lines allocated in the S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.any",
+	.event = "event=0xF2,umask=0xf,period=100000",
+	.desc = "L2 lines evicted",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x1,period=100000",
+	.desc = "L2 lines evicted by a demand request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.demand_dirty",
+	.event = "event=0xF2,umask=0x2,period=100000",
+	.desc = "L2 modified lines evicted by a demand request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.prefetch_clean",
+	.event = "event=0xF2,umask=0x4,period=100000",
+	.desc = "L2 lines evicted by a prefetch request",
+	.topic = "cache",
+},
+{
+	.name = "l2_lines_out.prefetch_dirty",
+	.event = "event=0xF2,umask=0x8,period=100000",
+	.desc = "L2 modified lines evicted by a prefetch request",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetch_hit",
+	.event = "event=0x24,umask=0x10,period=200000",
+	.desc = "L2 instruction fetch hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetch_miss",
+	.event = "event=0x24,umask=0x20,period=200000",
+	.desc = "L2 instruction fetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ifetches",
+	.event = "event=0x24,umask=0x30,period=200000",
+	.desc = "L2 instruction fetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ld_hit",
+	.event = "event=0x24,umask=0x1,period=200000",
+	.desc = "L2 load hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.ld_miss",
+	.event = "event=0x24,umask=0x2,period=200000",
+	.desc = "L2 load misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.loads",
+	.event = "event=0x24,umask=0x3,period=200000",
+	.desc = "L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.miss",
+	.event = "event=0x24,umask=0xaa,period=200000",
+	.desc = "All L2 misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetch_hit",
+	.event = "event=0x24,umask=0x40,period=200000",
+	.desc = "L2 prefetch hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetch_miss",
+	.event = "event=0x24,umask=0x80,period=200000",
+	.desc = "L2 prefetch misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.prefetches",
+	.event = "event=0x24,umask=0xc0,period=200000",
+	.desc = "All L2 prefetches",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.references",
+	.event = "event=0x24,umask=0xff,period=200000",
+	.desc = "All L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x4,period=200000",
+	.desc = "L2 RFO hits",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x8,period=200000",
+	.desc = "L2 RFO misses",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfos",
+	.event = "event=0x24,umask=0xc,period=200000",
+	.desc = "L2 RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.any",
+	.event = "event=0xF0,umask=0x80,period=200000",
+	.desc = "All L2 transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.fill",
+	.event = "event=0xF0,umask=0x20,period=200000",
+	.desc = "L2 fill transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.ifetch",
+	.event = "event=0xF0,umask=0x4,period=200000",
+	.desc = "L2 instruction fetch transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200000",
+	.desc = "L1D writeback to L2 transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.load",
+	.event = "event=0xF0,umask=0x1,period=200000",
+	.desc = "L2 Load transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.prefetch",
+	.event = "event=0xF0,umask=0x8,period=200000",
+	.desc = "L2 prefetch transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.rfo",
+	.event = "event=0xF0,umask=0x2,period=200000",
+	.desc = "L2 RFO transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_transactions.wb",
+	.event = "event=0xF0,umask=0x40,period=200000",
+	.desc = "L2 writeback to LLC transactions",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.e_state",
+	.event = "event=0x27,umask=0x40,period=100000",
+	.desc = "L2 demand lock RFOs in E state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.hit",
+	.event = "event=0x27,umask=0xe0,period=100000",
+	.desc = "All demand L2 lock RFOs that hit the cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.i_state",
+	.event = "event=0x27,umask=0x10,period=100000",
+	.desc = "L2 demand lock RFOs in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.m_state",
+	.event = "event=0x27,umask=0x80,period=100000",
+	.desc = "L2 demand lock RFOs in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.mesi",
+	.event = "event=0x27,umask=0xf0,period=100000",
+	.desc = "All demand L2 lock RFOs",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.lock.s_state",
+	.event = "event=0x27,umask=0x20,period=100000",
+	.desc = "L2 demand lock RFOs in S state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.hit",
+	.event = "event=0x27,umask=0xe,period=100000",
+	.desc = "All L2 demand store RFOs that hit the cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.i_state",
+	.event = "event=0x27,umask=0x1,period=100000",
+	.desc = "L2 demand store RFOs in I state (misses)",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.m_state",
+	.event = "event=0x27,umask=0x8,period=100000",
+	.desc = "L2 demand store RFOs in M state",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.mesi",
+	.event = "event=0x27,umask=0xf,period=100000",
+	.desc = "All L2 demand store RFOs",
+	.topic = "cache",
+},
+{
+	.name = "l2_write.rfo.s_state",
+	.event = "event=0x27,umask=0x2,period=100000",
+	.desc = "L2 demand store RFOs in S state",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100000",
+	.desc = "Longest latency cache miss",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=200000",
+	.desc = "Longest latency cache reference",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.loads",
+	.event = "event=0xB,umask=0x1,period=2000000",
+	.desc = "Instructions retired which contains a load (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.stores",
+	.event = "event=0xB,umask=0x2,period=2000000",
+	.desc = "Instructions retired which contains a store (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.hit_lfb",
+	.event = "event=0xCB,umask=0x40,period=200000",
+	.desc = "Retired loads that miss L1D and hit an previously allocated LFB (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l1d_hit",
+	.event = "event=0xCB,umask=0x1,period=2000000",
+	.desc = "Retired loads that hit the L1 data cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.l2_hit",
+	.event = "event=0xCB,umask=0x2,period=200000",
+	.desc = "Retired loads that hit the L2 cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.llc_miss",
+	.event = "event=0xCB,umask=0x10,period=10000",
+	.desc = "Retired loads that miss the LLC cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.llc_unshared_hit",
+	.event = "event=0xCB,umask=0x4,period=40000",
+	.desc = "Retired loads that hit valid versions in the LLC cache (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_retired.other_core_l2_hit_hitm",
+	.event = "event=0xCB,umask=0x8,period=40000",
+	.desc = "Retired loads that hit sibling core's L2 in modified or unmodified states (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests.l1d_writeback",
+	.event = "event=0xB0,umask=0x40,period=100000",
+	.desc = "Offcore L1 data cache writebacks",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_sq_full",
+	.event = "event=0xB2,umask=0x1,period=100000",
+	.desc = "Offcore requests blocked due to Super Queue full",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xF4,umask=0x10,period=2000000",
+	.desc = "Super Queue lock splits across a cache line",
+	.topic = "cache",
+},
+{
+	.name = "store_blocks.at_ret",
+	.event = "event=0x6,umask=0x4,period=200000",
+	.desc = "Loads delayed with at-Retirement block code",
+	.topic = "cache",
+},
+{
+	.name = "store_blocks.l1d_block",
+	.event = "event=0x6,umask=0x8,period=200000",
+	.desc = "Cacheable loads delayed with L1D block code",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_0",
+	.event = "event=0xB,umask=0x10,period=2000000,ldlat=0x0",
+	.desc = "Memory instructions retired above 0 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_1024",
+	.event = "event=0xB,umask=0x10,period=100,ldlat=0x400",
+	.desc = "Memory instructions retired above 1024 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_128",
+	.event = "event=0xB,umask=0x10,period=1000,ldlat=0x80",
+	.desc = "Memory instructions retired above 128 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_16",
+	.event = "event=0xB,umask=0x10,period=10000,ldlat=0x10",
+	.desc = "Memory instructions retired above 16 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_16384",
+	.event = "event=0xB,umask=0x10,period=5,ldlat=0x4000",
+	.desc = "Memory instructions retired above 16384 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_2048",
+	.event = "event=0xB,umask=0x10,period=50,ldlat=0x800",
+	.desc = "Memory instructions retired above 2048 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_256",
+	.event = "event=0xB,umask=0x10,period=500,ldlat=0x100",
+	.desc = "Memory instructions retired above 256 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_32",
+	.event = "event=0xB,umask=0x10,period=5000,ldlat=0x20",
+	.desc = "Memory instructions retired above 32 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_32768",
+	.event = "event=0xB,umask=0x10,period=3,ldlat=0x8000",
+	.desc = "Memory instructions retired above 32768 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_4",
+	.event = "event=0xB,umask=0x10,period=50000,ldlat=0x4",
+	.desc = "Memory instructions retired above 4 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_4096",
+	.event = "event=0xB,umask=0x10,period=20,ldlat=0x1000",
+	.desc = "Memory instructions retired above 4096 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_512",
+	.event = "event=0xB,umask=0x10,period=200,ldlat=0x200",
+	.desc = "Memory instructions retired above 512 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_64",
+	.event = "event=0xB,umask=0x10,period=2000,ldlat=0x40",
+	.desc = "Memory instructions retired above 64 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_8",
+	.event = "event=0xB,umask=0x10,period=20000,ldlat=0x8",
+	.desc = "Memory instructions retired above 8 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_inst_retired.latency_above_threshold_8192",
+	.event = "event=0xB,umask=0x10,period=10,ldlat=0x2000",
+	.desc = "Memory instructions retired above 8192 clocks (Precise Event)",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F11",
+	.desc = "Offcore data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF11",
+	.desc = "All offcore data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8011",
+	.desc = "Offcore data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x111",
+	.desc = "Offcore data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x211",
+	.desc = "Offcore data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x411",
+	.desc = "Offcore data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x711",
+	.desc = "Offcore data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4711",
+	.desc = "Offcore data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1811",
+	.desc = "Offcore data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3811",
+	.desc = "Offcore data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1011",
+	.desc = "Offcore data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x811",
+	.desc = "Offcore data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F44",
+	.desc = "Offcore code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF44",
+	.desc = "All offcore code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8044",
+	.desc = "Offcore code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x144",
+	.desc = "Offcore code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x244",
+	.desc = "Offcore code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x444",
+	.desc = "Offcore code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x744",
+	.desc = "Offcore code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4744",
+	.desc = "Offcore code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1844",
+	.desc = "Offcore code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3844",
+	.desc = "Offcore code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1044",
+	.desc = "Offcore code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x844",
+	.desc = "Offcore code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7FFF",
+	.desc = "Offcore requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFFFF",
+	.desc = "All offcore requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x80FF",
+	.desc = "Offcore requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1FF",
+	.desc = "Offcore requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x2FF",
+	.desc = "Offcore requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4FF",
+	.desc = "Offcore requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7FF",
+	.desc = "Offcore requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x47FF",
+	.desc = "Offcore requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x18FF",
+	.desc = "Offcore requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x38FF",
+	.desc = "Offcore requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x10FF",
+	.desc = "Offcore requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8FF",
+	.desc = "Offcore requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F22",
+	.desc = "Offcore RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF22",
+	.desc = "All offcore RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8022",
+	.desc = "Offcore RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x122",
+	.desc = "Offcore RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x222",
+	.desc = "Offcore RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x422",
+	.desc = "Offcore RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x722",
+	.desc = "Offcore RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4722",
+	.desc = "Offcore RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1822",
+	.desc = "Offcore RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3822",
+	.desc = "Offcore RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1022",
+	.desc = "Offcore RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x822",
+	.desc = "Offcore RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F08",
+	.desc = "Offcore writebacks to any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF08",
+	.desc = "All offcore writebacks",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8008",
+	.desc = "Offcore writebacks to the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x108",
+	.desc = "Offcore writebacks to the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x408",
+	.desc = "Offcore writebacks to the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x708",
+	.desc = "Offcore writebacks to the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4708",
+	.desc = "Offcore writebacks to the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1808",
+	.desc = "Offcore writebacks to a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3808",
+	.desc = "Offcore writebacks to a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1008",
+	.desc = "Offcore writebacks that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x808",
+	.desc = "Offcore writebacks that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F77",
+	.desc = "Offcore code or data read requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF77",
+	.desc = "All offcore code or data read requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8077",
+	.desc = "Offcore code or data read requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x177",
+	.desc = "Offcore code or data read requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x277",
+	.desc = "Offcore code or data read requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x477",
+	.desc = "Offcore code or data read requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x777",
+	.desc = "Offcore code or data read requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4777",
+	.desc = "Offcore code or data read requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1877",
+	.desc = "Offcore code or data read requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3877",
+	.desc = "Offcore code or data read requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1077",
+	.desc = "Offcore code or data read requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x877",
+	.desc = "Offcore code or data read requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F33",
+	.desc = "Offcore request = all data, response = any cache_dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF33",
+	.desc = "Offcore request = all data, response = any location",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8033",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x133",
+	.desc = "Offcore data reads, RFO's and prefetches statisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x233",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x433",
+	.desc = "Offcore data reads, RFO's and prefetches satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x733",
+	.desc = "Offcore request = all data, response = local cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4733",
+	.desc = "Offcore request = all data, response = local cache or dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1833",
+	.desc = "Offcore request = all data, response = remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3833",
+	.desc = "Offcore request = all data, response = remote cache or dram",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1033",
+	.desc = "Offcore data reads, RFO's and prefetches that HIT in a remote cache ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.data_in.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x833",
+	.desc = "Offcore data reads, RFO's and prefetches that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F03",
+	.desc = "Offcore demand data requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF03",
+	.desc = "All offcore demand data requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8003",
+	.desc = "Offcore demand data requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x103",
+	.desc = "Offcore demand data requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x203",
+	.desc = "Offcore demand data requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x403",
+	.desc = "Offcore demand data requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x703",
+	.desc = "Offcore demand data requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4703",
+	.desc = "Offcore demand data requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1803",
+	.desc = "Offcore demand data requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3803",
+	.desc = "Offcore demand data requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1003",
+	.desc = "Offcore demand data requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x803",
+	.desc = "Offcore demand data requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F01",
+	.desc = "Offcore demand data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF01",
+	.desc = "All offcore demand data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8001",
+	.desc = "Offcore demand data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x101",
+	.desc = "Offcore demand data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x201",
+	.desc = "Offcore demand data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x401",
+	.desc = "Offcore demand data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x701",
+	.desc = "Offcore demand data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4701",
+	.desc = "Offcore demand data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1801",
+	.desc = "Offcore demand data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3801",
+	.desc = "Offcore demand data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1001",
+	.desc = "Offcore demand data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x801",
+	.desc = "Offcore demand data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F04",
+	.desc = "Offcore demand code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF04",
+	.desc = "All offcore demand code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8004",
+	.desc = "Offcore demand code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x104",
+	.desc = "Offcore demand code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x204",
+	.desc = "Offcore demand code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x404",
+	.desc = "Offcore demand code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x704",
+	.desc = "Offcore demand code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4704",
+	.desc = "Offcore demand code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1804",
+	.desc = "Offcore demand code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3804",
+	.desc = "Offcore demand code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1004",
+	.desc = "Offcore demand code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x804",
+	.desc = "Offcore demand code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F02",
+	.desc = "Offcore demand RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF02",
+	.desc = "All offcore demand RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8002",
+	.desc = "Offcore demand RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x102",
+	.desc = "Offcore demand RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x202",
+	.desc = "Offcore demand RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x402",
+	.desc = "Offcore demand RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x702",
+	.desc = "Offcore demand RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4702",
+	.desc = "Offcore demand RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1802",
+	.desc = "Offcore demand RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3802",
+	.desc = "Offcore demand RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1002",
+	.desc = "Offcore demand RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x802",
+	.desc = "Offcore demand RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F80",
+	.desc = "Offcore other requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF80",
+	.desc = "All offcore other requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8080",
+	.desc = "Offcore other requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x180",
+	.desc = "Offcore other requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x280",
+	.desc = "Offcore other requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x480",
+	.desc = "Offcore other requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x780",
+	.desc = "Offcore other requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4780",
+	.desc = "Offcore other requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1880",
+	.desc = "Offcore other requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3880",
+	.desc = "Offcore other requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1080",
+	.desc = "Offcore other requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x880",
+	.desc = "Offcore other requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F30",
+	.desc = "Offcore prefetch data requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF30",
+	.desc = "All offcore prefetch data requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8030",
+	.desc = "Offcore prefetch data requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x130",
+	.desc = "Offcore prefetch data requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x230",
+	.desc = "Offcore prefetch data requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x430",
+	.desc = "Offcore prefetch data requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x730",
+	.desc = "Offcore prefetch data requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4730",
+	.desc = "Offcore prefetch data requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1830",
+	.desc = "Offcore prefetch data requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3830",
+	.desc = "Offcore prefetch data requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1030",
+	.desc = "Offcore prefetch data requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x830",
+	.desc = "Offcore prefetch data requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F10",
+	.desc = "Offcore prefetch data reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF10",
+	.desc = "All offcore prefetch data reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8010",
+	.desc = "Offcore prefetch data reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x110",
+	.desc = "Offcore prefetch data reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x210",
+	.desc = "Offcore prefetch data reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x410",
+	.desc = "Offcore prefetch data reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x710",
+	.desc = "Offcore prefetch data reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4710",
+	.desc = "Offcore prefetch data reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1810",
+	.desc = "Offcore prefetch data reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3810",
+	.desc = "Offcore prefetch data reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1010",
+	.desc = "Offcore prefetch data reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_data_rd.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x810",
+	.desc = "Offcore prefetch data reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F40",
+	.desc = "Offcore prefetch code reads satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF40",
+	.desc = "All offcore prefetch code reads",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8040",
+	.desc = "Offcore prefetch code reads satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x140",
+	.desc = "Offcore prefetch code reads satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x240",
+	.desc = "Offcore prefetch code reads satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x440",
+	.desc = "Offcore prefetch code reads satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x740",
+	.desc = "Offcore prefetch code reads satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4740",
+	.desc = "Offcore prefetch code reads satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1840",
+	.desc = "Offcore prefetch code reads satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3840",
+	.desc = "Offcore prefetch code reads satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1040",
+	.desc = "Offcore prefetch code reads that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_ifetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x840",
+	.desc = "Offcore prefetch code reads that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F20",
+	.desc = "Offcore prefetch RFO requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF20",
+	.desc = "All offcore prefetch RFO requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8020",
+	.desc = "Offcore prefetch RFO requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x120",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x220",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x420",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x720",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4720",
+	.desc = "Offcore prefetch RFO requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1820",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3820",
+	.desc = "Offcore prefetch RFO requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1020",
+	.desc = "Offcore prefetch RFO requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_rfo.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x820",
+	.desc = "Offcore prefetch RFO requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.any_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x7F70",
+	.desc = "Offcore prefetch requests satisfied by any cache or DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.any_location",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0xFF70",
+	.desc = "All offcore prefetch requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.io_csr_mmio",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x8070",
+	.desc = "Offcore prefetch requests satisfied by the IO, CSR, MMIO unit",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_no_other_core",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x170",
+	.desc = "Offcore prefetch requests satisfied by the LLC and not found in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_other_core_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x270",
+	.desc = "Offcore prefetch requests satisfied by the LLC and HIT in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.llc_hit_other_core_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x470",
+	.desc = "Offcore prefetch requests satisfied by the LLC  and HITM in a sibling core",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.local_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x770",
+	.desc = "Offcore prefetch requests satisfied by the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.local_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x4770",
+	.desc = "Offcore prefetch requests satisfied by the LLC or local DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1870",
+	.desc = "Offcore prefetch requests satisfied by a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_dram",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x3870",
+	.desc = "Offcore prefetch requests satisfied by a remote cache or remote DRAM",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_hit",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x1070",
+	.desc = "Offcore prefetch requests that HIT in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.prefetch.remote_cache_hitm",
+	.event = "event=0xB7,umask=0x1,period=100000,offcore_rsp=0x870",
+	.desc = "Offcore prefetch requests that HITM in a remote cache",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_ivytown[] = {
+{
+	.name = "fp_comp_ops_exe.x87",
+	.event = "event=0x10,umask=0x1,period=2000003",
+	.desc = "Number of FP Computational Uops Executed this cycle. The number of FADD, FSUB, FCOM, FMULs, integer MULsand IMULs, FDIVs, FPREMs, FSQRTS, integer DIVs, and IDIVs. This event does not distinguish an FADD used in the middle of a transcendental flow from a s",
+	.topic = "floating point",
+	.long_desc = "Counts number of X87 uops executed",
+},
+{
+	.name = "fp_comp_ops_exe.sse_packed_double",
+	.event = "event=0x10,umask=0x10,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational packed double-precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Number of SSE* or AVX-128 FP Computational packed double-precision uops issued this cycle",
+},
+{
+	.name = "fp_comp_ops_exe.sse_scalar_single",
+	.event = "event=0x10,umask=0x20,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational scalar single-precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Number of SSE* or AVX-128 FP Computational scalar single-precision uops issued this cycle",
+},
+{
+	.name = "fp_comp_ops_exe.sse_packed_single",
+	.event = "event=0x10,umask=0x40,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational packed single-precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Number of SSE* or AVX-128 FP Computational packed single-precision uops issued this cycle",
+},
+{
+	.name = "fp_comp_ops_exe.sse_scalar_double",
+	.event = "event=0x10,umask=0x80,period=2000003",
+	.desc = "Number of SSE* or AVX-128 FP Computational scalar double-precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Counts number of SSE* or AVX-128 double precision FP scalar uops executed",
+},
+{
+	.name = "simd_fp_256.packed_single",
+	.event = "event=0x11,umask=0x1,period=2000003",
+	.desc = "number of GSSE-256 Computational FP single precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Counts 256-bit packed single-precision floating-point instructions",
+},
+{
+	.name = "simd_fp_256.packed_double",
+	.event = "event=0x11,umask=0x2,period=2000003",
+	.desc = "number of AVX-256 Computational FP double precision uops issued this cycle",
+	.topic = "floating point",
+	.long_desc = "Counts 256-bit packed double-precision floating-point instructions",
+},
+{
+	.name = "other_assists.avx_store",
+	.event = "event=0xC1,umask=0x8,period=100003",
+	.desc = "Number of GSSE memory assist for stores. GSSE microcode assist is being invoked whenever the hardware is unable to properly handle GSSE-256b operations",
+	.topic = "floating point",
+	.long_desc = "Number of assists associated with 256-bit AVX store operations",
+},
+{
+	.name = "other_assists.avx_to_sse",
+	.event = "event=0xC1,umask=0x10,period=100003",
+	.desc = "Number of transitions from AVX-256 to legacy SSE when penalty applicable",
+	.topic = "floating point",
+},
+{
+	.name = "other_assists.sse_to_avx",
+	.event = "event=0xC1,umask=0x20,period=100003",
+	.desc = "Number of transitions from SSE to AVX-256 when penalty applicable",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.x87_output",
+	.event = "event=0xCA,umask=0x2,period=100003",
+	.desc = "Number of X87 assists due to output value",
+	.topic = "floating point",
+	.long_desc = "Number of X87 FP assists due to output values",
+},
+{
+	.name = "fp_assist.x87_input",
+	.event = "event=0xCA,umask=0x4,period=100003",
+	.desc = "Number of X87 assists due to input value",
+	.topic = "floating point",
+	.long_desc = "Number of X87 FP assists due to input values",
+},
+{
+	.name = "fp_assist.simd_output",
+	.event = "event=0xCA,umask=0x8,period=100003",
+	.desc = "Number of SIMD FP assists due to Output values",
+	.topic = "floating point",
+	.long_desc = "Number of SIMD FP assists due to output values",
+},
+{
+	.name = "fp_assist.simd_input",
+	.event = "event=0xCA,umask=0x10,period=100003",
+	.desc = "Number of SIMD FP assists due to input values",
+	.topic = "floating point",
+	.long_desc = "Number of SIMD FP assists due to input values",
+},
+{
+	.name = "fp_assist.any",
+	.event = "event=0xCA,umask=0x1e,period=100003,cmask=1",
+	.desc = "Cycles with any input/output SSE or FP assist",
+	.topic = "floating point",
+	.long_desc = "Cycles with any input/output SSE* or FP assists",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired from execution",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Core cycles when the thread is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Reference cycles when the core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks.store_forward",
+	.event = "event=0x03,umask=0x2,period=100003",
+	.desc = "Cases when loads get true Block-on-Store blocking code preventing store forwarding",
+	.topic = "pipeline",
+	.long_desc = "Loads blocked by overlapping with store buffer that cannot be forwarded",
+},
+{
+	.name = "ld_blocks.no_sr",
+	.event = "event=0x03,umask=0x8,period=100003",
+	.desc = "This event counts the number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+	.topic = "pipeline",
+	.long_desc = "The number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+},
+{
+	.name = "ld_blocks_partial.address_alias",
+	.event = "event=0x07,umask=0x1,period=100003",
+	.desc = "False dependencies in MOB due to partial compare on address",
+	.topic = "pipeline",
+	.long_desc = "False dependencies in MOB due to partial compare on address",
+},
+{
+	.name = "int_misc.recovery_cycles",
+	.event = "event=0x0D,umask=0x3,period=2000003,cmask=1",
+	.desc = "Number of cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc.)",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_stalls_count",
+	.event = "event=0x0D,umask=0x3,edge=1,period=2000003,cmask=1",
+	.desc = "Number of occurences waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc.)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0x0E,umask=0x1,period=2000003",
+	.desc = "Uops that Resource Allocation Table (RAT) issues to Reservation Station (RS)",
+	.topic = "pipeline",
+	.long_desc = "Increments each cycle the # of Uops issued by the RAT to RS. Set Cmask = 1, Inv = 1, Any= 1to count stalled cycles of this core",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+},
+{
+	.name = "uops_issued.core_stall_cycles",
+	.event = "event=0x0E,inv=1,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for all threads",
+	.topic = "pipeline",
+	.long_desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for all threads",
+},
+{
+	.name = "uops_issued.flags_merge",
+	.event = "event=0x0E,umask=0x10,period=2000003",
+	.desc = "Number of flags-merge uops being allocated",
+	.topic = "pipeline",
+	.long_desc = "Number of flags-merge uops allocated. Such uops adds delay",
+},
+{
+	.name = "uops_issued.slow_lea",
+	.event = "event=0x0E,umask=0x20,period=2000003",
+	.desc = "Number of slow LEA uops being allocated. A uop is generally considered SlowLea if it has 3 sources (e.g. 2 sources + immediate) regardless if as a result of LEA instruction or not",
+	.topic = "pipeline",
+	.long_desc = "Number of slow LEA or similar uops allocated. Such uop has 3 sources (e.g. 2 sources + immediate) regardless if as a result of LEA instruction or not",
+},
+{
+	.name = "uops_issued.single_mul",
+	.event = "event=0x0E,umask=0x40,period=2000003",
+	.desc = "Number of Multiply packed/scalar single precision uops allocated",
+	.topic = "pipeline",
+	.long_desc = "Number of multiply packed/scalar single precision uops allocated",
+},
+{
+	.name = "arith.fpu_div_active",
+	.event = "event=0x14,umask=0x1,period=2000003",
+	.desc = "Cycles when divider is busy executing divide operations",
+	.topic = "pipeline",
+	.long_desc = "Cycles that the divider is active, includes INT and FP. Set 'edge =1, cmask=1' to count the number of divides",
+},
+{
+	.name = "arith.fpu_div",
+	.event = "event=0x14,umask=0x4,edge=1,period=100003,cmask=1",
+	.desc = "Divide operations executed",
+	.topic = "pipeline",
+	.long_desc = "Divide operations executed",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Thread cycles when thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of thread cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. The core frequency may change from time to time due to power or thermal throttling",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Increments at the frequency of XCLK (100 MHz) when not halted",
+},
+{
+	.name = "cpu_clk_thread_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.sw_pf",
+	.event = "event=0x4C,umask=0x1,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for software prefetch",
+	.topic = "pipeline",
+	.long_desc = "Non-SW-prefetch load dispatches that hit fill buffer allocated for S/W prefetch",
+},
+{
+	.name = "load_hit_pre.hw_pf",
+	.event = "event=0x4C,umask=0x2,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for hardware prefetch",
+	.topic = "pipeline",
+	.long_desc = "Non-SW-prefetch load dispatches that hit fill buffer allocated for H/W prefetch",
+},
+{
+	.name = "move_elimination.int_not_eliminated",
+	.event = "event=0x58,umask=0x4,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.simd_not_eliminated",
+	.event = "event=0x58,umask=0x8,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.int_eliminated",
+	.event = "event=0x58,umask=0x1,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.simd_eliminated",
+	.event = "event=0x58,umask=0x2,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_cycles",
+	.event = "event=0x5E,umask=0x1,period=2000003",
+	.desc = "Cycles when Reservation Station (RS) is empty for the thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles the RS is empty for the thread",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000003",
+	.desc = "Stalls caused by changing prefix length of the instruction",
+	.topic = "pipeline",
+},
+{
+	.name = "ild_stall.iq_full",
+	.event = "event=0x87,umask=0x4,period=2000003",
+	.desc = "Stall cycles because IQ is full",
+	.topic = "pipeline",
+	.long_desc = "Stall cycles due to IQ is full",
+},
+{
+	.name = "br_inst_exec.nontaken_conditional",
+	.event = "event=0x88,umask=0x41,period=200003",
+	.desc = "Not taken macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Not taken macro-conditional branches",
+},
+{
+	.name = "br_inst_exec.taken_conditional",
+	.event = "event=0x88,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired macro-conditional branches",
+},
+{
+	.name = "br_inst_exec.taken_direct_jump",
+	.event = "event=0x88,umask=0x82,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branch instructions excluding calls and indirects",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired macro-conditional branch instructions excluding calls and indirects",
+},
+{
+	.name = "br_inst_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired indirect branches excluding calls and returns",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_return",
+	.event = "event=0x88,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired indirect branches with return mnemonic",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired indirect branches with return mnemonic",
+},
+{
+	.name = "br_inst_exec.taken_direct_near_call",
+	.event = "event=0x88,umask=0x90,period=200003",
+	.desc = "Taken speculative and retired direct near calls",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired direct near calls",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_call",
+	.event = "event=0x88,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired indirect calls",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired indirect calls",
+},
+{
+	.name = "br_inst_exec.all_conditional",
+	.event = "event=0x88,umask=0xc1,period=200003",
+	.desc = "Speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Speculative and retired macro-conditional branches",
+},
+{
+	.name = "br_inst_exec.all_direct_jmp",
+	.event = "event=0x88,umask=0xc2,period=200003",
+	.desc = "Speculative and retired macro-unconditional branches excluding calls and indirects",
+	.topic = "pipeline",
+	.long_desc = "Speculative and retired macro-unconditional branches excluding calls and indirects",
+},
+{
+	.name = "br_inst_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0xc4,period=200003",
+	.desc = "Speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "Speculative and retired indirect branches excluding calls and returns",
+},
+{
+	.name = "br_inst_exec.all_indirect_near_return",
+	.event = "event=0x88,umask=0xc8,period=200003",
+	.desc = "Speculative and retired indirect return branches",
+	.topic = "pipeline",
+},
+{
+	.name = "br_inst_exec.all_direct_near_call",
+	.event = "event=0x88,umask=0xd0,period=200003",
+	.desc = "Speculative and retired direct near calls",
+	.topic = "pipeline",
+	.long_desc = "Speculative and retired direct near calls",
+},
+{
+	.name = "br_inst_exec.all_branches",
+	.event = "event=0x88,umask=0xff,period=200003",
+	.desc = "Speculative and retired  branches",
+	.topic = "pipeline",
+	.long_desc = "Counts all near executed branches (not necessarily retired)",
+},
+{
+	.name = "br_misp_exec.nontaken_conditional",
+	.event = "event=0x89,umask=0x41,period=200003",
+	.desc = "Not taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Not taken speculative and retired mispredicted macro conditional branches",
+},
+{
+	.name = "br_misp_exec.taken_conditional",
+	.event = "event=0x89,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired mispredicted macro conditional branches",
+},
+{
+	.name = "br_misp_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired mispredicted indirect branches excluding calls and returns",
+},
+{
+	.name = "br_misp_exec.taken_return_near",
+	.event = "event=0x89,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches with return mnemonic",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired mispredicted indirect branches with return mnemonic",
+},
+{
+	.name = "br_misp_exec.taken_indirect_near_call",
+	.event = "event=0x89,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect calls",
+	.topic = "pipeline",
+	.long_desc = "Taken speculative and retired mispredicted indirect calls",
+},
+{
+	.name = "br_misp_exec.all_conditional",
+	.event = "event=0x89,umask=0xc1,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Speculative and retired mispredicted macro conditional branches",
+},
+{
+	.name = "br_misp_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0xc4,period=200003",
+	.desc = "Mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted indirect branches excluding calls and returns",
+},
+{
+	.name = "br_misp_exec.all_branches",
+	.event = "event=0x89,umask=0xff,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "Counts all near executed branches (not necessarily retired)",
+},
+{
+	.name = "uops_dispatched_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 0",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 0",
+},
+{
+	.name = "uops_dispatched_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 1",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 1",
+},
+{
+	.name = "uops_dispatched_port.port_4",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 4",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 4",
+},
+{
+	.name = "uops_dispatched_port.port_5",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are dispatched to port 5",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 5",
+},
+{
+	.name = "uops_dispatched_port.port_0_core",
+	.event = "event=0xA1,umask=0x1,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 0",
+	.topic = "pipeline",
+	.long_desc = "Cycles per core when uops are dispatched to port 0",
+},
+{
+	.name = "uops_dispatched_port.port_1_core",
+	.event = "event=0xA1,umask=0x2,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 1",
+	.topic = "pipeline",
+	.long_desc = "Cycles per core when uops are dispatched to port 1",
+},
+{
+	.name = "uops_dispatched_port.port_4_core",
+	.event = "event=0xA1,umask=0x40,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 4",
+	.topic = "pipeline",
+	.long_desc = "Cycles per core when uops are dispatched to port 4",
+},
+{
+	.name = "uops_dispatched_port.port_5_core",
+	.event = "event=0xA1,umask=0x80,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 5",
+	.topic = "pipeline",
+	.long_desc = "Cycles per core when uops are dispatched to port 5",
+},
+{
+	.name = "uops_dispatched_port.port_2",
+	.event = "event=0xA1,umask=0xc,period=2000003",
+	.desc = "Cycles per thread when load or STA uops are dispatched to port 2",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 2",
+},
+{
+	.name = "uops_dispatched_port.port_3",
+	.event = "event=0xA1,umask=0x30,period=2000003",
+	.desc = "Cycles per thread when load or STA uops are dispatched to port 3",
+	.topic = "pipeline",
+	.long_desc = "Cycles which a Uop is dispatched on port 3",
+},
+{
+	.name = "uops_dispatched_port.port_2_core",
+	.event = "event=0xA1,umask=0xc,any=1,period=2000003",
+	.desc = "Uops dispatched to port 2, loads and stores per core (speculative and retired)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_dispatched_port.port_3_core",
+	.event = "event=0xA1,umask=0x30,any=1,period=2000003",
+	.desc = "Cycles per core when load or STA uops are dispatched to port 3",
+	.topic = "pipeline",
+	.long_desc = "Cycles per core when load or STA uops are dispatched to port 3",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000003",
+	.desc = "Resource-related stall cycles",
+	.topic = "pipeline",
+	.long_desc = "Cycles Allocation is stalled due to Resource Related reason",
+},
+{
+	.name = "resource_stalls.rs",
+	.event = "event=0xA2,umask=0x4,period=2000003",
+	.desc = "Cycles stalled due to no eligible RS entry available",
+	.topic = "pipeline",
+},
+{
+	.name = "resource_stalls.sb",
+	.event = "event=0xA2,umask=0x8,period=2000003",
+	.desc = "Cycles stalled due to no store buffers available. (not including draining form sync)",
+	.topic = "pipeline",
+	.long_desc = "Cycles stalled due to no store buffers available (not including draining form sync)",
+},
+{
+	.name = "resource_stalls.rob",
+	.event = "event=0xA2,umask=0x10,period=2000003",
+	.desc = "Cycles stalled due to re-order buffer full",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l2_pending",
+	.event = "event=0xA3,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with pending L2 cache miss loads",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending L2 miss loads. Set AnyThread to count per core",
+},
+{
+	.name = "cycle_activity.cycles_l1d_pending",
+	.event = "event=0xA3,umask=0x8,period=2000003,cmask=8",
+	.desc = "Cycles with pending L1 cache miss loads",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending L1 cache miss loads. Set AnyThread to count per core",
+},
+{
+	.name = "cycle_activity.cycles_ldm_pending",
+	.event = "event=0xA3,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles with pending memory loads",
+	.topic = "pipeline",
+	.long_desc = "Cycles with pending memory loads. Set AnyThread to count per core",
+},
+{
+	.name = "cycle_activity.cycles_no_execute",
+	.event = "event=0xA3,umask=0x4,period=2000003,cmask=4",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+	.long_desc = "Total execution stalls",
+},
+{
+	.name = "cycle_activity.stalls_l2_pending",
+	.event = "event=0xA3,umask=0x5,period=2000003,cmask=5",
+	.desc = "Execution stalls due to L2 cache misses",
+	.topic = "pipeline",
+	.long_desc = "Number of loads missed L2",
+},
+{
+	.name = "cycle_activity.stalls_ldm_pending",
+	.event = "event=0xA3,umask=0x6,period=2000003,cmask=6",
+	.desc = "Execution stalls due to memory subsystem",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l1d_pending",
+	.event = "event=0xA3,umask=0xc,period=2000003,cmask=12",
+	.desc = "Execution stalls due to L1 data cache misses",
+	.topic = "pipeline",
+	.long_desc = "Execution stalls due to L1 data cache miss loads. Set Cmask=0CH",
+},
+{
+	.name = "lsd.uops",
+	.event = "event=0xA8,umask=0x1,period=2000003",
+	.desc = "Number of Uops delivered by the LSD",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_active",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+	.long_desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+},
+{
+	.name = "uops_executed.thread",
+	.event = "event=0xB1,umask=0x1,period=2000003",
+	.desc = "Counts the number of uops to be executed per-thread each cycle",
+	.topic = "pipeline",
+	.long_desc = "Counts total number of uops to be executed per-thread each cycle. Set Cmask = 1, INV =1 to count stall cycles",
+},
+{
+	.name = "uops_executed.core",
+	.event = "event=0xB1,umask=0x2,period=2000003",
+	.desc = "Number of uops executed on the core",
+	.topic = "pipeline",
+	.long_desc = "Counts total number of uops to be executed per-core each cycle",
+},
+{
+	.name = "uops_executed.stall_cycles",
+	.event = "event=0xB1,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts number of cycles no uops were dispatched to be executed on this thread",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Number of instructions retired. General Counter   - architectural event",
+	.topic = "pipeline",
+	.long_desc = "Number of instructions at retirement",
+},
+{
+	.name = "inst_retired.prec_dist",
+	.event = "event=0xC0,umask=0x1,period=2000003",
+	.desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution (Must be precise)",
+},
+{
+	.name = "other_assists.any_wb_assist",
+	.event = "event=0xC1,umask=0x80,period=100003",
+	.desc = "Number of times any microcode assist is invoked by HW upon uop writeback",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.all",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "Actually retired uops (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of micro-ops retired, Use cmask=1 and invert to count active cycles or stalled cycles (Precise event)",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000003",
+	.desc = "Retirement slots used (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of retirement slots used each cycle (Precise event)",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,period=2000003,cmask=10",
+	.desc = "Cycles with less than 10 actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.core_stall_cycles",
+	.event = "event=0xC2,inv=1,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=100003",
+	.desc = "Self-modifying code (SMC) detected",
+	.topic = "pipeline",
+	.long_desc = "Number of self-modifying-code machine clears detected",
+},
+{
+	.name = "machine_clears.maskmov",
+	.event = "event=0xC3,umask=0x20,period=100003",
+	.desc = "This event counts the number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of executed AVX masked load operations that refer to an illegal address range with the mask bits set to 0",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=400009",
+	.desc = "Conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=100007",
+	.desc = "Direct and indirect near call instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Direct and indirect near call instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=400009",
+	.desc = "All (macro) branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Branch instructions at retirement",
+},
+{
+	.name = "br_inst_retired.near_return",
+	.event = "event=0xC4,umask=0x8,period=100007",
+	.desc = "Return instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of near return instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.not_taken",
+	.event = "event=0xC4,umask=0x10,period=400009",
+	.desc = "Not taken branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of not taken branch instructions retired",
+},
+{
+	.name = "br_inst_retired.near_taken",
+	.event = "event=0xC4,umask=0x20,period=400009",
+	.desc = "Taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Number of near taken branches retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0x40,period=100007",
+	.desc = "Far branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Number of far branches retired",
+},
+{
+	.name = "br_inst_retired.all_branches_pebs",
+	.event = "event=0xC4,umask=0x4,period=400009",
+	.desc = "All (macro) branch instructions retired (Must be precise)",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=400009",
+	.desc = "Mispredicted conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=400009",
+	.desc = "All mispredicted macro branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted branch instructions at retirement",
+},
+{
+	.name = "br_misp_retired.near_taken",
+	.event = "event=0xC5,umask=0x20,period=400009",
+	.desc = "number of near branch instructions retired that were mispredicted and taken (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Mispredicted taken branch instructions retired (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches_pebs",
+	.event = "event=0xC5,umask=0x4,period=400009",
+	.desc = "Mispredicted macro branch instructions retired (Must be precise)",
+	.topic = "pipeline",
+},
+{
+	.name = "rob_misc_events.lbr_inserts",
+	.event = "event=0xCC,umask=0x20,period=2000003",
+	.desc = "Count cases of saving new LBR",
+	.topic = "pipeline",
+	.long_desc = "Count cases of saving new LBR records by hardware",
+},
+{
+	.name = "baclears.any",
+	.event = "event=0xE6,umask=0x1f,period=100003",
+	.desc = "Counts the total number when the front end is resteered, mainly when the BPU cannot provide a correct prediction and this is corrected by other branch handling mechanisms at the front end",
+	.topic = "pipeline",
+	.long_desc = "Number of front end re-steers due to BPU misprediction",
+},
+{
+	.name = "uops_executed.cycles_ge_1_uop_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles where at least 1 uop was executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 1 uop was executed per-thread",
+},
+{
+	.name = "uops_executed.cycles_ge_2_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles where at least 2 uops were executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 2 uops were executed per-thread",
+},
+{
+	.name = "uops_executed.cycles_ge_3_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles where at least 3 uops were executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 3 uops were executed per-thread",
+},
+{
+	.name = "uops_executed.cycles_ge_4_uops_exec",
+	.event = "event=0xB1,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles where at least 4 uops were executed per-thread",
+	.topic = "pipeline",
+	.long_desc = "Cycles where at least 4 uops were executed per-thread",
+},
+{
+	.name = "rs_events.empty_end",
+	.event = "event=0x5E,inv=1,umask=0x1,edge=1,period=200003,cmask=1",
+	.desc = "Counts end of periods where the Reservation Station (RS) was empty. Could be useful to precisely locate Frontend Latency Bound issues",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.count",
+	.event = "event=0xC3,umask=0x1,edge=1,period=100003,cmask=1",
+	.desc = "Number of machine clears (nukes) of any type",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_4_uops",
+	.event = "event=0xA8,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+	.long_desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+},
+{
+	.name = "cycle_activity.cycles_l1d_miss",
+	.event = "event=0xA3,umask=0x8,period=2000003,cmask=8",
+	.desc = "Cycles while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l2_miss",
+	.event = "event=0xA3,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles while L2 cache miss load* is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_mem_any",
+	.event = "event=0xA3,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_total",
+	.event = "event=0xA3,umask=0x4,period=2000003,cmask=4",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l1d_miss",
+	.event = "event=0xA3,umask=0xc,period=2000003,cmask=12",
+	.desc = "Execution stalls while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l2_miss",
+	.event = "event=0xA3,umask=0x5,period=2000003,cmask=5",
+	.desc = "Execution stalls while L2 cache miss load* is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_mem_any",
+	.event = "event=0xA3,umask=0x6,period=2000003,cmask=6",
+	.desc = "Execution stalls while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_any",
+	.event = "event=0x3c,any=1",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "Core cycles when at least one thread on the physical core is not in halt state",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p_any",
+	.event = "event=0x3C,umask=0x0,any=1,period=2000003",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "Core cycles when at least one thread on the physical core is not in halt state",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted. (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_cycles_any",
+	.event = "event=0x0D,umask=0x3,any=1,period=2000003,cmask=1",
+	.desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_1",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles at least 1 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+	.long_desc = "Cycles at least 1 micro-op is executed from any thread on physical core",
+},
+{
+	.name = "uops_executed.core_cycles_ge_2",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=2",
+	.desc = "Cycles at least 2 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+	.long_desc = "Cycles at least 2 micro-op is executed from any thread on physical core",
+},
+{
+	.name = "uops_executed.core_cycles_ge_3",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=3",
+	.desc = "Cycles at least 3 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+	.long_desc = "Cycles at least 3 micro-op is executed from any thread on physical core",
+},
+{
+	.name = "uops_executed.core_cycles_ge_4",
+	.event = "event=0xB1,umask=0x2,period=2000003,cmask=4",
+	.desc = "Cycles at least 4 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+	.long_desc = "Cycles at least 4 micro-op is executed from any thread on physical core",
+},
+{
+	.name = "uops_executed.core_cycles_none",
+	.event = "event=0xB1,inv=1,umask=0x2,period=2000003",
+	.desc = "Cycles with no micro-ops executed from any thread on physical core",
+	.topic = "pipeline",
+	.long_desc = "Cycles with no micro-ops executed from any thread on physical core",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the thread is unhalted. (counts at 100 MHz rate)",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted. (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "idq.empty",
+	.event = "event=0x79,umask=0x2,period=2000003",
+	.desc = "Instruction Decode Queue (IDQ) empty cycles",
+	.topic = "frontend",
+	.long_desc = "Counts cycles the IDQ is empty",
+},
+{
+	.name = "idq.mite_uops",
+	.event = "event=0x79,umask=0x4,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ from MITE path. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.dsb_uops",
+	.event = "event=0x79,umask=0x8,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle. # of uops delivered to IDQ from DSB path. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.ms_dsb_uops",
+	.event = "event=0x79,umask=0x10,period=2000003",
+	.desc = "Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ when MS_busy by DSB. Set Cmask = 1 to count cycles. Add Edge=1 to count # of delivery",
+},
+{
+	.name = "idq.ms_mite_uops",
+	.event = "event=0x79,umask=0x20,period=2000003",
+	.desc = "Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ when MS_busy by MITE. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.ms_uops",
+	.event = "event=0x79,umask=0x30,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Increment each cycle # of uops delivered to IDQ from MS by either DSB or MITE. Set Cmask = 1 to count cycles",
+},
+{
+	.name = "idq.ms_cycles",
+	.event = "event=0x79,umask=0x30,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+},
+{
+	.name = "idq.mite_cycles",
+	.event = "event=0x79,umask=0x4,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+},
+{
+	.name = "idq.dsb_cycles",
+	.event = "event=0x79,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+},
+{
+	.name = "idq.ms_dsb_cycles",
+	.event = "event=0x79,umask=0x10,period=2000003,cmask=1",
+	.desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+},
+{
+	.name = "idq.ms_dsb_occur",
+	.event = "event=0x79,umask=0x10,edge=1,period=2000003,cmask=1",
+	.desc = "Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequenser (MS) is busy",
+},
+{
+	.name = "idq.all_dsb_cycles_4_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=4",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "Counts cycles DSB is delivered four uops. Set Cmask = 4",
+},
+{
+	.name = "idq.all_dsb_cycles_any_uops",
+	.event = "event=0x79,umask=0x18,period=2000003,cmask=1",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "Counts cycles DSB is delivered at least one uops. Set Cmask = 1",
+},
+{
+	.name = "idq.all_mite_cycles_4_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=4",
+	.desc = "Cycles MITE is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "Counts cycles MITE is delivered four uops. Set Cmask = 4",
+},
+{
+	.name = "idq.all_mite_cycles_any_uops",
+	.event = "event=0x79,umask=0x24,period=2000003,cmask=1",
+	.desc = "Cycles MITE is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "Counts cycles MITE is delivered at least one uops. Set Cmask = 1",
+},
+{
+	.name = "idq.mite_all_uops",
+	.event = "event=0x79,umask=0x3c,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "Number of uops delivered to IDQ from any path",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=2000003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Reads. both cacheable and noncacheable, including UC fetches",
+	.topic = "frontend",
+	.long_desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Reads. both cacheable and noncacheable, including UC fetches",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "Instruction cache, streaming buffer and victim cache misses",
+	.topic = "frontend",
+	.long_desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Misses. Includes UC accesses",
+},
+{
+	.name = "icache.ifetch_stall",
+	.event = "event=0x80,umask=0x4,period=2000003",
+	.desc = "Cycles where a code-fetch stalled due to L1 instruction-cache miss or an iTLB miss",
+	.topic = "frontend",
+	.long_desc = "Cycles where a code-fetch stalled due to L1 instruction-cache miss or an iTLB miss",
+},
+{
+	.name = "idq_uops_not_delivered.core",
+	.event = "event=0x9C,umask=0x1,period=2000003",
+	.desc = "Uops not delivered to Resource Allocation Table (RAT) per thread when backend of the machine is not stalled ",
+	.topic = "frontend",
+	.long_desc = "Count issue pipeline slots where no uop was delivered from the front end to the back end when there is no back-end stall",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_0_uops_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=4",
+	.desc = "Cycles per thread when 4 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_1_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=3",
+	.desc = "Cycles per thread when 3 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_2_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=2",
+	.desc = "Cycles with less than 2 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_3_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with less than 3 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_fe_was_ok",
+	.event = "event=0x9C,inv=1,umask=0x1,period=2000003,cmask=1",
+	.desc = "Counts cycles FE delivered 4 uops or Resource Allocation Table (RAT) was stalling FE",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.count",
+	.event = "event=0xAB,umask=0x1,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switches",
+	.topic = "frontend",
+	.long_desc = "Number of DSB to MITE switches",
+},
+{
+	.name = "dsb2mite_switches.penalty_cycles",
+	.event = "event=0xAB,umask=0x2,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles",
+	.topic = "frontend",
+	.long_desc = "Cycles DSB to MITE switches caused delay",
+},
+{
+	.name = "dsb_fill.exceed_dsb_lines",
+	.event = "event=0xAC,umask=0x8,period=2000003",
+	.desc = "Cycles when Decode Stream Buffer (DSB) fill encounter more than 3 Decode Stream Buffer (DSB) lines",
+	.topic = "frontend",
+	.long_desc = "DSB Fill encountered > 3 DSB lines",
+},
+{
+	.name = "idq.ms_switches",
+	.event = "event=0x79,umask=0x30,edge=1,period=2000003,cmask=1",
+	.desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+	.topic = "frontend",
+	.long_desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+},
+{
+	.name = "cpl_cycles.ring0",
+	.event = "event=0x5C,umask=0x1,period=2000003",
+	.desc = "Unhalted core cycles when the thread is in ring 0",
+	.topic = "other",
+	.long_desc = "Unhalted core cycles when the thread is in ring 0",
+},
+{
+	.name = "cpl_cycles.ring123",
+	.event = "event=0x5C,umask=0x2,period=2000003",
+	.desc = "Unhalted core cycles when thread is in rings 1, 2, or 3",
+	.topic = "other",
+	.long_desc = "Unhalted core cycles when the thread is not in ring 0",
+},
+{
+	.name = "cpl_cycles.ring0_trans",
+	.event = "event=0x5C,umask=0x1,edge=1,period=100007,cmask=1",
+	.desc = "Number of intervals between processor halts while thread is in ring 0",
+	.topic = "other",
+	.long_desc = "Number of intervals between processor halts while thread is in ring 0",
+},
+{
+	.name = "lock_cycles.split_lock_uc_lock_duration",
+	.event = "event=0x63,umask=0x1,period=2000003",
+	.desc = "Cycles when L1 and L2 are locked due to UC or split lock",
+	.topic = "other",
+	.long_desc = "Cycles in which the L1D and L2 are locked, due to a UC lock or split lock",
+},
+{
+	.name = "misalign_mem_ref.loads",
+	.event = "event=0x05,umask=0x1,period=2000003",
+	.desc = "Speculative cache line split load uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "Speculative cache-line split load uops dispatched to L1D",
+},
+{
+	.name = "misalign_mem_ref.stores",
+	.event = "event=0x05,umask=0x2,period=2000003",
+	.desc = "Speculative cache line split STA uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "Speculative cache-line split Store-address uops dispatched to L1D",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=100003",
+	.desc = "Counts the number of machine clears due to memory order conflicts",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.precise_store",
+	.event = "event=0xCD,umask=0x2,period=2000003",
+	.desc = "Sample stores and collect precise store operation via PEBS record. PMC3 only (Must be precise)",
+	.topic = "memory",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_4",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x4",
+	.desc = "Loads with latency value being above 4 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 4 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_8",
+	.event = "event=0xCD,umask=0x1,period=50021,ldlat=0x8",
+	.desc = "Loads with latency value being above 8 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 8 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_16",
+	.event = "event=0xCD,umask=0x1,period=20011,ldlat=0x10",
+	.desc = "Loads with latency value being above 16 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 16 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_32",
+	.event = "event=0xCD,umask=0x1,period=100007,ldlat=0x20",
+	.desc = "Loads with latency value being above 32 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 32 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_64",
+	.event = "event=0xCD,umask=0x1,period=2003,ldlat=0x40",
+	.desc = "Loads with latency value being above 64 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 64 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_128",
+	.event = "event=0xCD,umask=0x1,period=1009,ldlat=0x80",
+	.desc = "Loads with latency value being above 128 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 128 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_256",
+	.event = "event=0xCD,umask=0x1,period=503,ldlat=0x100",
+	.desc = "Loads with latency value being above 256 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 256 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_512",
+	.event = "event=0xCD,umask=0x1,period=101,ldlat=0x200",
+	.desc = "Loads with latency value being above 512 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "Loads with latency value being above 512 (Must be precise)",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc00244",
+	.desc = "Counts all demand & prefetch code reads that miss the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_miss.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x67f800244",
+	.desc = "Counts all demand & prefetch code reads that miss the LLC  and the data returned from remote dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x87f800244",
+	.desc = "Counts all demand & prefetch code reads that miss the LLC  and the data forwarded from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20091",
+	.desc = "Counts all demand & prefetch data reads that hits the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc203f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x6004003f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the LLC  and the data returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x87f8203f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the LLC  and the data forwarded from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x107fc003f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the LLC  the data is found in M state in remote cache and forwarded from there",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20004",
+	.desc = "Counts all demand code reads that miss the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x600400004",
+	.desc = "Counts all demand code reads that miss the LLC  and the data returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x67f800004",
+	.desc = "Counts all demand code reads that miss the LLC  and the data returned from remote dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x87f820004",
+	.desc = "Counts all demand code reads that miss the LLC  and the data forwarded from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x107fc00004",
+	.desc = "Counts all demand code reads that miss the LLC  the data is found in M state in remote cache and forwarded from there",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x67fc00001",
+	.desc = "Counts demand data reads that miss the LLC  and the data returned from remote & local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20001",
+	.desc = "Counts demand data reads that miss in the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x600400001",
+	.desc = "Counts demand data reads that miss the LLC  and the data returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x67f800001",
+	.desc = "Counts demand data reads that miss the LLC  and the data returned from remote dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x87f820001",
+	.desc = "Counts demand data reads that miss the LLC  and the data forwarded from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x107fc00001",
+	.desc = "Counts demand data reads that miss the LLC  the data is found in M state in remote cache and forwarded from there",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x107fc20002",
+	.desc = "Counts all demand data writes (RFOs) that miss the LLC and the data is found in M state in remote cache and forwarded from there",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20040",
+	.desc = "Counts all prefetch (that bring data to L2) code reads that miss the LLC  and the data returned from remote & local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.any_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x67fc00010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss the LLC  and the data returned from remote & local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss in the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x600400010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss the LLC  and the data returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x67f800010",
+	.desc = "Counts prefetch (that bring data to L2) data reads  that miss the LLC  and the data returned from remote dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x87f820010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss the LLC  and the data forwarded from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x107fc00010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that miss the LLC  the data is found in M state in remote cache and forwarded from there",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20200",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that miss in the LLC",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fffc20080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that miss in the LLC",
+	.topic = "memory",
+},
+{
+	.name = "dtlb_load_misses.demand_ld_walk_completed",
+	.event = "event=0x08,umask=0x82,period=100003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes of any page size",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.demand_ld_walk_duration",
+	.event = "event=0x08,umask=0x84,period=2000003",
+	.desc = "Demand load cycles page miss handler (PMH) is busy with this walk",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.large_page_walk_completed",
+	.event = "event=0x08,umask=0x88,period=100003",
+	.desc = "Page walk for a large page completed for Demand load",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.miss_causes_a_walk",
+	.event = "event=0x49,umask=0x1,period=100003",
+	.desc = "Store misses in all DTLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Miss in all TLB levels causes a page walk of any page size (4K/2M/4M/1G)",
+},
+{
+	.name = "dtlb_store_misses.walk_completed",
+	.event = "event=0x49,umask=0x2,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks",
+	.topic = "virtual memory",
+	.long_desc = "Miss in all TLB levels causes a page walk that completes of any page size (4K/2M/4M/1G)",
+},
+{
+	.name = "dtlb_store_misses.walk_duration",
+	.event = "event=0x49,umask=0x4,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "Cycles PMH is busy with this walk",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit",
+	.event = "event=0x49,umask=0x10,period=100003",
+	.desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4F,umask=0x10,period=2000003",
+	.desc = "Cycle count for an Extended Page table walk.  The Extended Page Directory cache is used by Virtual Machine operating systems while the guest operating systems use the standard TLB caches",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x5F,umask=0x4,period=100003",
+	.desc = "Load operations that miss the first DTLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Counts load operations that missed 1st level DTLB but hit the 2nd level",
+},
+{
+	.name = "itlb_misses.miss_causes_a_walk",
+	.event = "event=0x85,umask=0x1,period=100003",
+	.desc = "Misses at all ITLB levels that cause page walks",
+	.topic = "virtual memory",
+	.long_desc = "Misses in all ITLB levels that cause page walks",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0x2,period=100003",
+	.desc = "Misses in all ITLB levels that cause completed page walks",
+	.topic = "virtual memory",
+	.long_desc = "Misses in all ITLB levels that cause completed page walks",
+},
+{
+	.name = "itlb_misses.walk_duration",
+	.event = "event=0x85,umask=0x4,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks",
+	.topic = "virtual memory",
+	.long_desc = "Cycle PMH is busy with a walk",
+},
+{
+	.name = "itlb_misses.stlb_hit",
+	.event = "event=0x85,umask=0x10,period=100003",
+	.desc = "Operations that miss the first ITLB level but hit the second and do not cause any page walks",
+	.topic = "virtual memory",
+	.long_desc = "Number of cache load STLB hits. No page walk",
+},
+{
+	.name = "itlb_misses.large_page_walk_completed",
+	.event = "event=0x85,umask=0x80,period=100003",
+	.desc = "Completed page walks in ITLB due to STLB load misses for large pages",
+	.topic = "virtual memory",
+	.long_desc = "Completed page walks in ITLB due to STLB load misses for large pages",
+},
+{
+	.name = "itlb.itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=100007",
+	.desc = "Flushing of the Instruction TLB (ITLB) pages, includes 4k/2M/4M pages",
+	.topic = "virtual memory",
+	.long_desc = "Counts the number of ITLB flushes, includes 4k/2M/4M pages",
+},
+{
+	.name = "tlb_flush.dtlb_thread",
+	.event = "event=0xBD,umask=0x1,period=100007",
+	.desc = "DTLB flush attempts of the thread-specific entries",
+	.topic = "virtual memory",
+	.long_desc = "DTLB flush attempts of the thread-specific entries",
+},
+{
+	.name = "tlb_flush.stlb_any",
+	.event = "event=0xBD,umask=0x20,period=100007",
+	.desc = "STLB flush attempts",
+	.topic = "virtual memory",
+	.long_desc = "Count number of STLB flush attempts",
+},
+{
+	.name = "dtlb_load_misses.miss_causes_a_walk",
+	.event = "event=0x08,umask=0x81,period=100003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes an page walk of any page size",
+	.topic = "virtual memory",
+	.long_desc = "Misses in all TLB levels that cause a page walk of any page size from demand loads",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x08,umask=0x82,period=100003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes of any page size",
+	.topic = "virtual memory",
+	.long_desc = "Misses in all TLB levels that caused page walk completed of any size by demand loads",
+},
+{
+	.name = "dtlb_load_misses.walk_duration",
+	.event = "event=0x08,umask=0x84,period=2000003",
+	.desc = "Demand load cycles page miss handler (PMH) is busy with this walk",
+	.topic = "virtual memory",
+	.long_desc = "Cycle PMH is busy with a walk due to demand loads",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_hit",
+	.event = "event=0x24,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "Demand Data Read requests that hit L2 cache",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x4,period=200003",
+	.desc = "RFO requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "RFO requests that hit L2 cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x8,period=200003",
+	.desc = "RFO requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts the number of store RFO requests that miss the L2 cache",
+},
+{
+	.name = "l2_rqsts.code_rd_hit",
+	.event = "event=0x24,umask=0x10,period=200003",
+	.desc = "L2 cache hits when fetching instructions, code reads",
+	.topic = "cache",
+	.long_desc = "Number of instruction fetches that hit the L2 cache",
+},
+{
+	.name = "l2_rqsts.code_rd_miss",
+	.event = "event=0x24,umask=0x20,period=200003",
+	.desc = "L2 cache misses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "Number of instruction fetches that missed the L2 cache",
+},
+{
+	.name = "l2_rqsts.pf_hit",
+	.event = "event=0x24,umask=0x40,period=200003",
+	.desc = "Requests from the L2 hardware prefetchers that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests that hit L2",
+},
+{
+	.name = "l2_rqsts.pf_miss",
+	.event = "event=0x24,umask=0x80,period=200003",
+	.desc = "Requests from the L2 hardware prefetchers that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests that missed L2",
+},
+{
+	.name = "l2_rqsts.all_demand_data_rd",
+	.event = "event=0x24,umask=0x3,period=200003",
+	.desc = "Demand Data Read requests",
+	.topic = "cache",
+	.long_desc = "Counts any demand and L1 HW prefetch data load requests to L2",
+},
+{
+	.name = "l2_rqsts.all_rfo",
+	.event = "event=0x24,umask=0xc,period=200003",
+	.desc = "RFO requests to L2 cache",
+	.topic = "cache",
+	.long_desc = "Counts all L2 store RFO requests",
+},
+{
+	.name = "l2_rqsts.all_code_rd",
+	.event = "event=0x24,umask=0x30,period=200003",
+	.desc = "L2 code requests",
+	.topic = "cache",
+	.long_desc = "Counts all L2 code requests",
+},
+{
+	.name = "l2_rqsts.all_pf",
+	.event = "event=0x24,umask=0xc0,period=200003",
+	.desc = "Requests from L2 hardware prefetchers",
+	.topic = "cache",
+	.long_desc = "Counts all L2 HW prefetcher requests",
+},
+{
+	.name = "l2_store_lock_rqsts.miss",
+	.event = "event=0x27,umask=0x1,period=200003",
+	.desc = "RFOs that miss cache lines",
+	.topic = "cache",
+	.long_desc = "RFOs that miss cache lines",
+},
+{
+	.name = "l2_store_lock_rqsts.hit_m",
+	.event = "event=0x27,umask=0x8,period=200003",
+	.desc = "RFOs that hit cache lines in M state",
+	.topic = "cache",
+	.long_desc = "RFOs that hit cache lines in M state",
+},
+{
+	.name = "l2_store_lock_rqsts.all",
+	.event = "event=0x27,umask=0xf,period=200003",
+	.desc = "RFOs that access cache lines in any state",
+	.topic = "cache",
+	.long_desc = "RFOs that access cache lines in any state",
+},
+{
+	.name = "l2_l1d_wb_rqsts.miss",
+	.event = "event=0x28,umask=0x1,period=200003",
+	.desc = "Count the number of modified Lines evicted from L1 and missed L2. (Non-rejected WBs from the DCU.)",
+	.topic = "cache",
+	.long_desc = "Not rejected writebacks that missed LLC",
+},
+{
+	.name = "l2_l1d_wb_rqsts.hit_e",
+	.event = "event=0x28,umask=0x4,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in E state",
+	.topic = "cache",
+	.long_desc = "Not rejected writebacks from L1D to L2 cache lines in E state",
+},
+{
+	.name = "l2_l1d_wb_rqsts.hit_m",
+	.event = "event=0x28,umask=0x8,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in M state",
+	.topic = "cache",
+	.long_desc = "Not rejected writebacks from L1D to L2 cache lines in M state",
+},
+{
+	.name = "l2_l1d_wb_rqsts.all",
+	.event = "event=0x28,umask=0xf,period=200003",
+	.desc = "Not rejected writebacks from L1D to L2 cache lines in any state",
+	.topic = "cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100003",
+	.desc = "Core-originated cacheable demand requests missed LLC",
+	.topic = "cache",
+	.long_desc = "This event counts each cache miss condition for references to the last level cache",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=100003",
+	.desc = "Core-originated cacheable demand requests that refer to LLC",
+	.topic = "cache",
+	.long_desc = "This event counts requests originating from the core that reference a cache line in the last level cache",
+},
+{
+	.name = "l1d_pend_miss.pending",
+	.event = "event=0x48,umask=0x1,period=2000003",
+	.desc = "L1D miss oustandings duration in cycles",
+	.topic = "cache",
+	.long_desc = "Increments the number of outstanding L1D misses every cycle. Set Cmask = 1 and Edge =1 to count occurrences",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles",
+	.event = "event=0x48,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding",
+	.topic = "cache",
+},
+{
+	.name = "l1d.replacement",
+	.event = "event=0x51,umask=0x1,period=2000003",
+	.desc = "L1D data line replacements",
+	.topic = "cache",
+	.long_desc = "Counts the number of lines brought into the L1 data cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003",
+	.desc = "Offcore outstanding Demand Data Read transactions in uncore queue",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding Demand Data Read transactions in SQ to uncore. Set Cmask=1 to count cycles",
+},
+{
+	.name = "offcore_requests_outstanding.demand_code_rd",
+	.event = "event=0x60,umask=0x2,period=2000003",
+	.desc = "Offcore outstanding code reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding Demand Code Read transactions in SQ to uncore. Set Cmask=1 to count cycles",
+},
+{
+	.name = "offcore_requests_outstanding.demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003",
+	.desc = "Offcore outstanding RFO store transactions in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding RFO store transactions in SQ to uncore. Set Cmask=1 to count cycles",
+},
+{
+	.name = "offcore_requests_outstanding.all_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003",
+	.desc = "Offcore outstanding cacheable Core Data Read transactions in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding cacheable data read transactions in SQ to uncore. Set Cmask=1 to count cycles",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003,cmask=1",
+	.desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+	.topic = "cache",
+	.long_desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_code_rd",
+	.event = "event=0x60,umask=0x2,period=2000003,cmask=1",
+	.desc = "Offcore outstanding code reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding code reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003,cmask=1",
+	.desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+	.topic = "cache",
+	.long_desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle",
+},
+{
+	.name = "lock_cycles.cache_lock_duration",
+	.event = "event=0x63,umask=0x2,period=2000003",
+	.desc = "Cycles when L1D is locked",
+	.topic = "cache",
+	.long_desc = "Cycles in which the L1D is locked",
+},
+{
+	.name = "offcore_requests.demand_data_rd",
+	.event = "event=0xB0,umask=0x1,period=100003",
+	.desc = "Demand Data Read requests sent to uncore",
+	.topic = "cache",
+	.long_desc = "Demand data read requests sent to uncore",
+},
+{
+	.name = "offcore_requests.demand_code_rd",
+	.event = "event=0xB0,umask=0x2,period=100003",
+	.desc = "Cacheable and noncachaeble code read requests",
+	.topic = "cache",
+	.long_desc = "Demand code read requests sent to uncore",
+},
+{
+	.name = "offcore_requests.demand_rfo",
+	.event = "event=0xB0,umask=0x4,period=100003",
+	.desc = "Demand RFO requests including regular RFOs, locks, ItoM",
+	.topic = "cache",
+	.long_desc = "Demand RFO read requests sent to uncore, including regular RFOs, locks, ItoM",
+},
+{
+	.name = "offcore_requests.all_data_rd",
+	.event = "event=0xB0,umask=0x8,period=100003",
+	.desc = "Demand and prefetch data reads",
+	.topic = "cache",
+	.long_desc = "Data read requests sent to uncore (demand and prefetch)",
+},
+{
+	.name = "offcore_requests_buffer.sq_full",
+	.event = "event=0xB2,umask=0x1,period=2000003",
+	.desc = "Cases when offcore requests buffer cannot take more entries for core",
+	.topic = "cache",
+	.long_desc = "Cases when offcore requests buffer cannot take more entries for core",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_loads",
+	.event = "event=0xD0,umask=0x11,period=100003",
+	.desc = "Retired load uops that miss the STLB (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_stores",
+	.event = "event=0xD0,umask=0x12,period=100003",
+	.desc = "Retired store uops that miss the STLB (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.lock_loads",
+	.event = "event=0xD0,umask=0x21,period=100007",
+	.desc = "Retired load uops with locked access (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.split_loads",
+	.event = "event=0xD0,umask=0x41,period=100003",
+	.desc = "Retired load uops that split across a cacheline boundary (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.split_stores",
+	.event = "event=0xD0,umask=0x42,period=100003",
+	.desc = "Retired store uops that split across a cacheline boundary (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0xD0,umask=0x81,period=2000003",
+	.desc = "All retired load uops (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0xD0,umask=0x82,period=2000003",
+	.desc = "All retired store uops (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.l1_hit",
+	.event = "event=0xD1,umask=0x1,period=2000003",
+	.desc = "Retired load uops with L1 cache hits as data sources (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops with L1 cache hits as data sources (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_hit",
+	.event = "event=0xD1,umask=0x2,period=100003",
+	.desc = "Retired load uops with L2 cache hits as data sources (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops with L2 cache hits as data sources (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.llc_hit",
+	.event = "event=0xD1,umask=0x4,period=50021",
+	.desc = "Retired load uops which data sources were data hits in LLC without snoops required (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source was LLC hit with no snoop required (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_miss",
+	.event = "event=0xD1,umask=0x8,period=100003",
+	.desc = "Retired load uops which data sources following L1 data-cache miss (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source followed an L1 miss (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_miss",
+	.event = "event=0xD1,umask=0x10,period=50021",
+	.desc = "Miss in mid-level (L2) cache. Excludes Unknown data-source (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops that missed L2, excluding unknown sources (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.llc_miss",
+	.event = "event=0xD1,umask=0x20,period=100007",
+	.desc = "Miss in last-level (L3) cache. Excludes Unknown data-source (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source is LLC miss (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.hit_lfb",
+	.event = "event=0xD1,umask=0x40,period=100003",
+	.desc = "Retired load uops which data sources were load uops missed L1 but hit FB due to preceding miss to the same cache line with data not ready (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops which data sources were load uops missed L1 but hit FB due to preceding miss to the same cache line with data not ready (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_miss",
+	.event = "event=0xD2,umask=0x1,period=20011",
+	.desc = "Retired load uops which data sources were LLC hit and cross-core snoop missed in on-pkg core cache (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source was an on-package core cache LLC hit and cross-core snoop missed (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_hit",
+	.event = "event=0xD2,umask=0x2,period=20011",
+	.desc = "Retired load uops which data sources were LLC and cross-core snoop hits in on-pkg core cache (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source was an on-package LLC hit and cross-core snoop hits (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_hitm",
+	.event = "event=0xD2,umask=0x4,period=20011",
+	.desc = "Retired load uops which data sources were HitM responses from shared LLC (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source was an on-package core cache with HitM responses (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_hit_retired.xsnp_none",
+	.event = "event=0xD2,umask=0x8,period=100003",
+	.desc = "Retired load uops which data sources were hits in LLC without snoops required (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uops whose data source was LLC hit with no snoop required (Precise event)",
+},
+{
+	.name = "mem_load_uops_llc_miss_retired.local_dram",
+	.event = "event=0xD3,umask=0x1,period=100007",
+	.desc = "Retired load uops which data sources missed LLC but serviced from local dram",
+	.topic = "cache",
+	.long_desc = "Retired load uop whose Data Source was: local DRAM either Snoop not needed or Snoop Miss (RspI)",
+},
+{
+	.name = "mem_load_uops_llc_miss_retired.remote_dram",
+	.event = "event=0xD3,umask=0xc,period=100007",
+	.desc = "Retired load uops whose data source was remote DRAM (Snoop not needed, Snoop Miss, or Snoop Hit data not forwarded)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_llc_miss_retired.remote_hitm",
+	.event = "event=0xD3,umask=0x10,period=100007",
+	.desc = "Remote cache HITM",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_llc_miss_retired.remote_fwd",
+	.event = "event=0xD3,umask=0x20,period=100007",
+	.desc = "Data forwarded from remote cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.demand_data_rd",
+	.event = "event=0xF0,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "Demand Data Read requests that access L2 cache",
+},
+{
+	.name = "l2_trans.rfo",
+	.event = "event=0xF0,umask=0x2,period=200003",
+	.desc = "RFO requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "RFO requests that access L2 cache",
+},
+{
+	.name = "l2_trans.code_rd",
+	.event = "event=0xF0,umask=0x4,period=200003",
+	.desc = "L2 cache accesses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "L2 cache accesses when fetching instructions",
+},
+{
+	.name = "l2_trans.all_pf",
+	.event = "event=0xF0,umask=0x8,period=200003",
+	.desc = "L2 or LLC HW prefetches that access L2 cache",
+	.topic = "cache",
+	.long_desc = "Any MLC or LLC HW prefetch accessing L2, including rejects",
+},
+{
+	.name = "l2_trans.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200003",
+	.desc = "L1D writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L1D writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_fill",
+	.event = "event=0xF0,umask=0x20,period=200003",
+	.desc = "L2 fill requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L2 fill requests that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_wb",
+	.event = "event=0xF0,umask=0x40,period=200003",
+	.desc = "L2 writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "L2 writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.all_requests",
+	.event = "event=0xF0,umask=0x80,period=200003",
+	.desc = "Transactions accessing L2 pipe",
+	.topic = "cache",
+	.long_desc = "Transactions accessing L2 pipe",
+},
+{
+	.name = "l2_lines_in.i",
+	.event = "event=0xF1,umask=0x1,period=100003",
+	.desc = "L2 cache lines in I state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in I state filling L2",
+},
+{
+	.name = "l2_lines_in.s",
+	.event = "event=0xF1,umask=0x2,period=100003",
+	.desc = "L2 cache lines in S state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in S state filling L2",
+},
+{
+	.name = "l2_lines_in.e",
+	.event = "event=0xF1,umask=0x4,period=100003",
+	.desc = "L2 cache lines in E state filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines in E state filling L2",
+},
+{
+	.name = "l2_lines_in.all",
+	.event = "event=0xF1,umask=0x7,period=100003",
+	.desc = "L2 cache lines filling L2",
+	.topic = "cache",
+	.long_desc = "L2 cache lines filling L2",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x1,period=100003",
+	.desc = "Clean L2 cache lines evicted by demand",
+	.topic = "cache",
+	.long_desc = "Clean L2 cache lines evicted by demand",
+},
+{
+	.name = "l2_lines_out.demand_dirty",
+	.event = "event=0xF2,umask=0x2,period=100003",
+	.desc = "Dirty L2 cache lines evicted by demand",
+	.topic = "cache",
+	.long_desc = "Dirty L2 cache lines evicted by demand",
+},
+{
+	.name = "l2_lines_out.pf_clean",
+	.event = "event=0xF2,umask=0x4,period=100003",
+	.desc = "Clean L2 cache lines evicted by L2 prefetch",
+	.topic = "cache",
+	.long_desc = "Clean L2 cache lines evicted by the MLC prefetcher",
+},
+{
+	.name = "l2_lines_out.pf_dirty",
+	.event = "event=0xF2,umask=0x8,period=100003",
+	.desc = "Dirty L2 cache lines evicted by L2 prefetch",
+	.topic = "cache",
+	.long_desc = "Dirty L2 cache lines evicted by the MLC prefetcher",
+},
+{
+	.name = "l2_lines_out.dirty_all",
+	.event = "event=0xF2,umask=0xa,period=100003",
+	.desc = "Dirty L2 cache lines filling the L2",
+	.topic = "cache",
+	.long_desc = "Dirty L2 cache lines filling the L2",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xF4,umask=0x10,period=100003",
+	.desc = "Split locks in SQ",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd_ge_6",
+	.event = "event=0x60,umask=0x1,period=2000003,cmask=6",
+	.desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue",
+	.topic = "cache",
+	.long_desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles_any",
+	.event = "event=0x48,umask=0x1,any=1,period=2000003,cmask=1",
+	.desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+	.topic = "cache",
+	.long_desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+},
+{
+	.name = "l1d_pend_miss.fb_full",
+	.event = "event=0x48,umask=0x2,period=2000003,cmask=1",
+	.desc = "Cycles a demand request was blocked due to Fill Buffers inavailability",
+	.topic = "cache",
+	.long_desc = "Cycles a demand request was blocked due to Fill Buffers inavailability",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0091",
+	.desc = "Counts demand & prefetch data reads that hit in the LLC and sibling core snoop returned a clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0090",
+	.desc = "Counts all prefetch data reads that hit the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_pf_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0090",
+	.desc = "Counts prefetch data reads that hit in the LLC and sibling core snoop returned a clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c03f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c03f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c03f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c03f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c03f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the LLC and sibling core snoop returned a clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10008",
+	.desc = "Counts all writebacks from the core to the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0004",
+	.desc = "Counts all demand code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0001",
+	.desc = "Counts all demand data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0001",
+	.desc = "Counts demand data reads that hit in the LLC and sibling core snoop returned a clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0002",
+	.desc = "Counts demand data writes (RFOs) that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.lru_hints",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x803c8000",
+	.desc = "Counts L2 hints sent to LLC to keep a line from being evicted out of the core caches",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.other.portio_mmio_uc",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x23ffc08000",
+	.desc = "Counts miscellaneous accesses that include port i/o, MMIO and uncacheable memory accesses",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0040",
+	.desc = "Counts all prefetch (that bring data to L2) code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0010",
+	.desc = "Counts prefetch (that bring data to L2) data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0200",
+	.desc = "Counts all prefetch (that bring data to LLC only) code reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x4003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x1003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and sibling core snoops are not needed as either the core-valid bit is not set or the shared line is present in multiple cores",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_data_rd.llc_hit.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x2003c0080",
+	.desc = "Counts prefetch (that bring data to LLC only) data reads that hit in the LLC and the snoops sent to sibling cores return clean response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.split_lock_uc_lock.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10400",
+	.desc = "Counts requests where the address of an atomic lock instruction spans a cache line boundary or the lock instruction is executed on uncacheable address",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10800",
+	.desc = "Counts non-temporal stores",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_silvermont[] = {
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=200003",
+	.desc = "Counts the number of branch instructions retired.. (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "ALL_BRANCHES counts the number of any branch instructions retired.  Branch prediction predicts the branch target and enables the processor to begin executing instructions long before the branch true execution path is known. All branches utilize the branch prediction unit (BPU) for prediction. This unit predicts the target address not only based on the EIP of the branch but also based on the execution path through which execution reached this EIP. The BPU can efficiently predict the following branch types: conditional branches, direct calls and jumps, indirect calls and jumps, returns (Precise event)",
+},
+{
+	.name = "br_inst_retired.jcc",
+	.event = "event=0xC4,umask=0x7e,period=200003",
+	.desc = "Counts the number of JCC branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "JCC counts the number of conditional branch (JCC) instructions retired. Branch prediction predicts the branch target and enables the processor to begin executing instructions long before the branch true execution path is known. All branches utilize the branch prediction unit (BPU) for prediction. This unit predicts the target address not only based on the EIP of the branch but also based on the execution path through which execution reached this EIP. The BPU can efficiently predict the following branch types: conditional branches, direct calls and jumps, indirect calls and jumps, returns (Precise event)",
+},
+{
+	.name = "br_inst_retired.taken_jcc",
+	.event = "event=0xC4,umask=0xfe,period=200003",
+	.desc = "Counts the number of taken JCC branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "TAKEN_JCC counts the number of taken conditional branch (JCC) instructions retired. Branch prediction predicts the branch target and enables the processor to begin executing instructions long before the branch true execution path is known. All branches utilize the branch prediction unit (BPU) for prediction. This unit predicts the target address not only based on the EIP of the branch but also based on the execution path through which execution reached this EIP. The BPU can efficiently predict the following branch types: conditional branches, direct calls and jumps, indirect calls and jumps, returns (Precise event)",
+},
+{
+	.name = "br_inst_retired.call",
+	.event = "event=0xC4,umask=0xf9,period=200003",
+	.desc = "Counts the number of near CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "CALL counts the number of near CALL branch instructions retired.  Branch prediction predicts the branch target and enables the processor to begin executing instructions long before the branch true execution path is known. All branches utilize the branch prediction unit (BPU) for prediction. This unit predicts the target address not only based on the EIP of the branch but also based on the execution path through which execution reached this EIP. The BPU can efficiently predict the following branch types: conditional branches, direct calls and jumps, indirect calls and jumps, returns (Precise event)",
+},
+{
+	.name = "br_inst_retired.rel_call",
+	.event = "event=0xC4,umask=0xfd,period=200003",
+	.desc = "Counts the number of near relative CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "REL_CALL counts the number of near relative CALL branch instructions retired.  Branch prediction predicts the branch target and enables the processor to begin executing instructions long before the branch true execution path is known. All branches utilize the branch prediction unit (BPU) for prediction. This unit predicts the target address not only based on the EIP of the branch but also based on the execution path through which execution reached this EIP. The BPU can efficiently predict the following branch types: conditional branches, direct calls and jumps, indirect calls and jumps, returns (Precise event)",
+},
+{
+	.name = "br_inst_retired.ind_call",
+	.event = "event=0xC4,umask=0xfb,period=200003",
+	.desc = "Counts the number of near indirect CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "IND_CALL counts the number of near indirect CALL branch instructions retired.  Branch prediction predicts the branch target and enables the processor to begin executing instructions long before the branch true execution path is known. All branches utilize the branch prediction unit (BPU) for prediction. This unit predicts the target address not only based on the EIP of the branch but also based on the execution path through which execution reached this EIP. The BPU can efficiently predict the following branch types: conditional branches, direct calls and jumps, indirect calls and jumps, returns (Precise event)",
+},
+{
+	.name = "br_inst_retired.return",
+	.event = "event=0xC4,umask=0xf7,period=200003",
+	.desc = "Counts the number of near RET branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "RETURN counts the number of near RET branch instructions retired.  Branch prediction predicts the branch target and enables the processor to begin executing instructions long before the branch true execution path is known. All branches utilize the branch prediction unit (BPU) for prediction. This unit predicts the target address not only based on the EIP of the branch but also based on the execution path through which execution reached this EIP. The BPU can efficiently predict the following branch types: conditional branches, direct calls and jumps, indirect calls and jumps, returns (Precise event)",
+},
+{
+	.name = "br_inst_retired.non_return_ind",
+	.event = "event=0xC4,umask=0xeb,period=200003",
+	.desc = "Counts the number of near indirect JMP and near indirect CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "NON_RETURN_IND counts the number of near indirect JMP and near indirect CALL branch instructions retired.  Branch prediction predicts the branch target and enables the processor to begin executing instructions long before the branch true execution path is known. All branches utilize the branch prediction unit (BPU) for prediction. This unit predicts the target address not only based on the EIP of the branch but also based on the execution path through which execution reached this EIP. The BPU can efficiently predict the following branch types: conditional branches, direct calls and jumps, indirect calls and jumps, returns (Precise event)",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0xbf,period=200003",
+	.desc = "Counts the number of far branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "FAR counts the number of far branch instructions retired.  Branch prediction predicts the branch target and enables the processor to begin executing instructions long before the branch true execution path is known. All branches utilize the branch prediction unit (BPU) for prediction. This unit predicts the target address not only based on the EIP of the branch but also based on the execution path through which execution reached this EIP. The BPU can efficiently predict the following branch types: conditional branches, direct calls and jumps, indirect calls and jumps, returns (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=200003",
+	.desc = "Counts the number of mispredicted branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "ALL_BRANCHES counts the number of any mispredicted branch instructions retired. This umask is an architecturally defined event. This event counts the number of retired branch instructions that were mispredicted by the processor, categorized by type. A branch misprediction occurs when the processor predicts that the branch would be taken, but it is not, or vice-versa.  When the misprediction is discovered, all the instructions executed in the wrong (speculative) path must be discarded, and the processor must start fetching from the correct path (Precise event)",
+},
+{
+	.name = "br_misp_retired.jcc",
+	.event = "event=0xC5,umask=0x7e,period=200003",
+	.desc = "Counts the number of mispredicted JCC branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "JCC counts the number of mispredicted conditional branches (JCC) instructions retired.  This event counts the number of retired branch instructions that were mispredicted by the processor, categorized by type. A branch misprediction occurs when the processor predicts that the branch would be taken, but it is not, or vice-versa.  When the misprediction is discovered, all the instructions executed in the wrong (speculative) path must be discarded, and the processor must start fetching from the correct path (Precise event)",
+},
+{
+	.name = "br_misp_retired.taken_jcc",
+	.event = "event=0xC5,umask=0xfe,period=200003",
+	.desc = "Counts the number of mispredicted taken JCC branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "TAKEN_JCC counts the number of mispredicted taken conditional branch (JCC) instructions retired.  This event counts the number of retired branch instructions that were mispredicted by the processor, categorized by type. A branch misprediction occurs when the processor predicts that the branch would be taken, but it is not, or vice-versa.  When the misprediction is discovered, all the instructions executed in the wrong (speculative) path must be discarded, and the processor must start fetching from the correct path (Precise event)",
+},
+{
+	.name = "br_misp_retired.ind_call",
+	.event = "event=0xC5,umask=0xfb,period=200003",
+	.desc = "Counts the number of mispredicted near indirect CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "IND_CALL counts the number of mispredicted near indirect CALL branch instructions retired.  This event counts the number of retired branch instructions that were mispredicted by the processor, categorized by type. A branch misprediction occurs when the processor predicts that the branch would be taken, but it is not, or vice-versa.  When the misprediction is discovered, all the instructions executed in the wrong (speculative) path must be discarded, and the processor must start fetching from the correct path (Precise event)",
+},
+{
+	.name = "br_misp_retired.return",
+	.event = "event=0xC5,umask=0xf7,period=200003",
+	.desc = "Counts the number of mispredicted near RET branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "RETURN counts the number of mispredicted near RET branch instructions retired.  This event counts the number of retired branch instructions that were mispredicted by the processor, categorized by type. A branch misprediction occurs when the processor predicts that the branch would be taken, but it is not, or vice-versa.  When the misprediction is discovered, all the instructions executed in the wrong (speculative) path must be discarded, and the processor must start fetching from the correct path (Precise event)",
+},
+{
+	.name = "br_misp_retired.non_return_ind",
+	.event = "event=0xC5,umask=0xeb,period=200003",
+	.desc = "Counts the number of mispredicted near indirect JMP and near indirect CALL branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "NON_RETURN_IND counts the number of mispredicted near indirect JMP and near indirect CALL branch instructions retired.  This event counts the number of retired branch instructions that were mispredicted by the processor, categorized by type. A branch misprediction occurs when the processor predicts that the branch would be taken, but it is not, or vice-versa.  When the misprediction is discovered, all the instructions executed in the wrong (speculative) path must be discarded, and the processor must start fetching from the correct path (Precise event)",
+},
+{
+	.name = "uops_retired.ms",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "MSROM micro-ops retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of micro-ops retired that were supplied from MSROM",
+},
+{
+	.name = "uops_retired.all",
+	.event = "event=0xC2,umask=0x10,period=2000003",
+	.desc = "Micro-ops retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of micro-ops retired. The processor decodes complex macro instructions into a sequence of simpler micro-ops. Most instructions are composed of one or two micro-ops. Some instructions are decoded into longer sequences such as repeat instructions, floating point transcendental instructions, and assists. In some cases micro-op sequences are fused or whole instructions are fused into one micro-op. See other UOPS_RETIRED events for differentiating retired fused and non-fused micro-ops",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x1,period=200003",
+	.desc = "Self-Modifying Code detected",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of times that a program writes to a code section. Self-modifying code causes a severe penalty in all Intel? architecture processors",
+},
+{
+	.name = "machine_clears.fp_assist",
+	.event = "event=0xC3,umask=0x4,period=200003",
+	.desc = "Stalls due to FP assists",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of times that pipeline stalled due to FP operations needing assists",
+},
+{
+	.name = "machine_clears.all",
+	.event = "event=0xC3,umask=0x8,period=200003",
+	.desc = "Counts all machine clears",
+	.topic = "pipeline",
+	.long_desc = "Machine clears happen when something happens in the machine that causes the hardware to need to take special care to get the right answer. When such a condition is signaled on an instruction, the front end of the machine is notified that it must restart, so no more instructions will be decoded from the current path.  All instructions \"older\" than this one will be allowed to finish.  This instruction and all \"younger\" instructions must be cleared, since they must not be allowed to complete.  Essentially, the hardware waits until the problematic instruction is the oldest instruction in the machine.  This means all older instructions are retired, and all pending stores (from older instructions) are completed.  Then the new path of instructions from the front end are allowed to start into the machine.  There are many conditions that might cause a machine clear (including the receipt of an interrupt, or a trap or a fault).  All those conditions (including but not limited to MACHINE_CLEARS.MEMORY_ORDERING, MACHINE_CLEARS.SMC, and MACHINE_CLEARS.FP_ASSIST) are captured in the ANY event. In addition, some conditions can be specifically counted (i.e. SMC, MEMORY_ORDERING, FP_ASSIST).  However, the sum of SMC, MEMORY_ORDERING, and FP_ASSIST machine clears will not necessarily equal the number of ANY",
+},
+{
+	.name = "no_alloc_cycles.rob_full",
+	.event = "event=0xCA,umask=0x1,period=200003",
+	.desc = "Counts the number of cycles when no uops are allocated and the ROB is full (less than 2 entries available)",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of cycles when no uops are allocated and the ROB is full (less than 2 entries available)",
+},
+{
+	.name = "no_alloc_cycles.mispredicts",
+	.event = "event=0xCA,umask=0x4,period=200003",
+	.desc = "Counts the number of cycles when no uops are allocated and the alloc pipe is stalled waiting for a mispredicted jump to retire.  After the misprediction is detected, the front end will start immediately but the allocate pipe stalls until the mispredicted ",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of cycles when no uops are allocated and the alloc pipe is stalled waiting for a mispredicted jump to retire.  After the misprediction is detected, the front end will start immediately but the allocate pipe stalls until the mispredicted",
+},
+{
+	.name = "no_alloc_cycles.rat_stall",
+	.event = "event=0xCA,umask=0x20,period=200003",
+	.desc = "Counts the number of cycles when no uops are allocated and a RATstall is asserted",
+	.topic = "pipeline",
+},
+{
+	.name = "no_alloc_cycles.not_delivered",
+	.event = "event=0xCA,umask=0x50,period=200003",
+	.desc = "Counts the number of cycles when no uops are allocated, the IQ is empty, and no other condition is blocking allocation",
+	.topic = "pipeline",
+	.long_desc = "The NO_ALLOC_CYCLES.NOT_DELIVERED event is used to measure front-end inefficiencies, i.e. when front-end of the machine is not delivering micro-ops to the back-end and the back-end is not stalled. This event can be used to identify if the machine is truly front-end bound.  When this event occurs, it is an indication that the front-end of the machine is operating at less than its theoretical peak performance.  Background: We can think of the processor pipeline as being divided into 2 broader parts: Front-end and Back-end. Front-end is responsible for fetching the instruction, decoding into micro-ops (uops) in machine understandable format and putting them into a micro-op queue to be consumed by back end. The back-end then takes these micro-ops, allocates the required resources.  When all resources are ready, micro-ops are executed. If the back-end is not ready to accept micro-ops from the front-end, then we do not want to count these as front-end bottlenecks.  However, whenever we have bottlenecks in the back-end, we will have allocation unit stalls and eventually forcing the front-end to wait until the back-end is ready to receive more UOPS. This event counts the cycles only when back-end is requesting more uops and front-end is not able to provide them. Some examples of conditions that cause front-end efficiencies are: Icache misses, ITLB misses, and decoder restrictions that limit the the front-end bandwidth",
+},
+{
+	.name = "no_alloc_cycles.all",
+	.event = "event=0xCA,umask=0x3f,period=200003",
+	.desc = "Counts the number of cycles when no uops are allocated for any reason",
+	.topic = "pipeline",
+	.long_desc = "The NO_ALLOC_CYCLES.ALL event counts the number of cycles when the front-end does not provide any instructions to be allocated for any reason. This event indicates the cycles where an allocation stalls occurs, and no UOPS are allocated in that cycle",
+},
+{
+	.name = "rs_full_stall.mec",
+	.event = "event=0xCB,umask=0x1,period=200003",
+	.desc = "Counts the number of cycles and allocation pipeline is stalled and is waiting for a free MEC reservation station entry.  The cycles should be appropriately counted in case of the cracked ops e.g. In case of a cracked load-op, the load portion is sent to M",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of cycles and allocation pipeline is stalled and is waiting for a free MEC reservation station entry.  The cycles should be appropriately counted in case of the cracked ops e.g. In case of a cracked load-op, the load portion is sent to M",
+},
+{
+	.name = "rs_full_stall.all",
+	.event = "event=0xCB,umask=0x1f,period=200003",
+	.desc = "Counts the number of cycles the Alloc pipeline is stalled when any one of the RSs (IEC, FPC and MEC) is full. This event is a superset of all the individual RS stall event counts",
+	.topic = "pipeline",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions that retire execution. For instructions that consist of multiple micro-ops, this event counts the retirement of the last micro-op of the instruction. The counter continues counting during hardware interrupts, traps, and inside interrupt handlers",
+},
+{
+	.name = "cycles_div_busy.all",
+	.event = "event=0xCD,umask=0x1,period=2000003",
+	.desc = "Cycles the divider is busy.  Does not imply a stall waiting for the divider",
+	.topic = "pipeline",
+	.long_desc = "Cycles the divider is busy.This event counts the cycles when the divide unit is unable to accept a new divide UOP because it is busy processing a previously dispatched UOP. The cycles will be counted irrespective of whether or not another divide UOP is waiting to enter the divide unit (from the RS). This event might count cycles while a divide is in progress even if the RS is empty.  The divide instruction is one of the longest latency instructions in the machine.  Hence, it has a special event associated with it to help determine if divides are delaying the retirement of instructions",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Fixed Counter: Counts the number of instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions that retire.  For instructions that consist of multiple micro-ops, this event counts exactly once, as the last micro-op of the instruction retires.  The event continues counting while instructions retire, including during interrupt service routines caused by hardware interrupts, faults or traps.  Background: Modern microprocessors employ extensive pipelining and speculative techniques.  Since sometimes an instruction is started but never completed, the notion of \"retirement\" is introduced.  A retired instruction is one that commits its states. Or stated differently, an instruction might be abandoned at some point. No instruction is truly finished until it retires.  This counter measures the number of completed instructions.  The fixed event is INST_RETIRED.ANY and the programmable event is INST_RETIRED.ANY_P",
+},
+{
+	.name = "cpu_clk_unhalted.core",
+	.event = "event=0x00,umask=0x2,period=2000003",
+	.desc = "Fixed Counter: Counts the number of unhalted core clock cycles",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of core cycles while the core is not in a halt state. The core enters the halt state when it is running the HLT instruction. This event is a component in many key event ratios.  The core frequency may change from time to time. For this reason this event may have a changing ratio with regards to time. In systems with a constant core frequency, this event can give you a measurement of the elapsed time while the core was not in halt state by dividing the event count by the core frequency. This event is architecturally defined and is a designated fixed counter.  CPU_CLK_UNHALTED.CORE and CPU_CLK_UNHALTED.CORE_P use the core frequency which may change from time to time.  CPU_CLK_UNHALTE.REF_TSC and CPU_CLK_UNHALTED.REF are not affected by core frequency changes but counts as if the core is running at the maximum frequency all the time.  The fixed events are CPU_CLK_UNHALTED.CORE and CPU_CLK_UNHALTED.REF_TSC and the programmable events are CPU_CLK_UNHALTED.CORE_P and CPU_CLK_UNHALTED.REF",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Fixed Counter: Counts the number of unhalted reference clock cycles",
+	.topic = "pipeline",
+	.long_desc = "Counts the number of reference cycles while the core is not in a halt state. The core enters the halt state when it is running the HLT instruction. This event is a component in many key event ratios.  The core frequency may change from time. This event is not affected by core frequency changes but counts as if the core is running at the maximum frequency all the time.  Divide this event count by core frequency to determine the elapsed time while the core was not in halt state.  Divide this event count by core frequency to determine the elapsed time while the core was not in halt state.  This event is architecturally defined and is a designated fixed counter.  CPU_CLK_UNHALTED.CORE and CPU_CLK_UNHALTED.CORE_P use the core frequency which may change from time to time.  CPU_CLK_UNHALTE.REF_TSC and CPU_CLK_UNHALTED.REF are not affected by core frequency changes but counts as if the core is running at the maximum frequency all the time.  The fixed events are CPU_CLK_UNHALTED.CORE and CPU_CLK_UNHALTED.REF_TSC and the programmable events are CPU_CLK_UNHALTED.CORE_P and CPU_CLK_UNHALTED.REF",
+},
+{
+	.name = "cpu_clk_unhalted.core_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Core cycles when core is not halted",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of core cycles while the core is not in a halt state. The core enters the halt state when it is running the HLT instruction. In mobile systems the core frequency may change from time to time. For this reason this event may have a changing ratio with regards to time",
+},
+{
+	.name = "cpu_clk_unhalted.ref",
+	.event = "event=0x0,umask=0x03",
+	.desc = "Reference cycles when core is not halted",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of reference cycles that the core is not in a halt state. The core enters the halt state when it is running the HLT instruction. In mobile systems the core frequency may change from time. This event is not affected by core frequency changes but counts as if the core is running at the maximum frequency all the time",
+},
+{
+	.name = "baclears.all",
+	.event = "event=0xE6,umask=0x1,period=200003",
+	.desc = "Counts the number of baclears",
+	.topic = "pipeline",
+	.long_desc = "The BACLEARS event counts the number of times the front end is resteered, mainly when the Branch Prediction Unit cannot provide a correct prediction and this is corrected by the Branch Address Calculator at the front end.  The BACLEARS.ANY event counts the number of baclears for any type of branch",
+},
+{
+	.name = "baclears.return",
+	.event = "event=0xE6,umask=0x8,period=200003",
+	.desc = "Counts the number of RETURN baclears",
+	.topic = "pipeline",
+	.long_desc = "The BACLEARS event counts the number of times the front end is resteered, mainly when the Branch Prediction Unit cannot provide a correct prediction and this is corrected by the Branch Address Calculator at the front end.  The BACLEARS.RETURN event counts the number of RETURN baclears",
+},
+{
+	.name = "baclears.cond",
+	.event = "event=0xE6,umask=0x10,period=200003",
+	.desc = "Counts the number of JCC baclears",
+	.topic = "pipeline",
+	.long_desc = "The BACLEARS event counts the number of times the front end is resteered, mainly when the Branch Prediction Unit cannot provide a correct prediction and this is corrected by the Branch Address Calculator at the front end.  The BACLEARS.COND event counts the number of JCC (Jump on Condtional Code) baclears",
+},
+{
+	.name = "br_inst_retired.all_taken_branches",
+	.event = "event=0xC4,umask=0x80,period=200003",
+	.desc = "Counts the number of taken branch instructions retired (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "ALL_TAKEN_BRANCHES counts the number of all taken branch instructions retired.  Branch prediction predicts the branch target and enables the processor to begin executing instructions long before the branch true execution path is known. All branches utilize the branch prediction unit (BPU) for prediction. This unit predicts the target address not only based on the EIP of the branch but also based on the execution path through which execution reached this EIP. The BPU can efficiently predict the following branch types: conditional branches, direct calls and jumps, indirect calls and jumps, returns (Must be precise)",
+},
+{
+	.name = "icache.accesses",
+	.event = "event=0x80,umask=0x3,period=200003",
+	.desc = "Instruction fetches",
+	.topic = "frontend",
+	.long_desc = "This event counts all instruction fetches, not including most uncacheable\r\nfetches",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=200003",
+	.desc = "Instruction fetches from Icache",
+	.topic = "frontend",
+	.long_desc = "This event counts all instruction fetches from the instruction cache",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "Icache miss",
+	.topic = "frontend",
+	.long_desc = "This event counts all instruction fetches that miss the Instruction cache or produce memory requests. This includes uncacheable fetches. An instruction fetch miss is counted only once and not once for every cycle it is outstanding",
+},
+{
+	.name = "ms_decoded.ms_entry",
+	.event = "event=0xE7,umask=0x1,period=200003",
+	.desc = "Counts the number of times entered into a ucode flow in the FEC.  Includes inserted flows due to front-end detected faults or assists.  Speculative count",
+	.topic = "frontend",
+	.long_desc = "Counts the number of times the MSROM starts a flow of UOPS. It does not count every time a UOP is read from the microcode ROM.  The most common case that this counts is when a micro-coded instruction is encountered by the front end of the machine.  Other cases include when an instruction encounters a fault, trap, or microcode assist of any sort.  The event will count MSROM startups for UOPS that are speculative, and subsequently cleared by branch mispredict or machine clear.  Background: UOPS are produced by two mechanisms.  Either they are generated by hardware that decodes instructions into UOPS, or they are delivered by a ROM (called the MSROM) that holds UOPS associated with a specific instruction.  MSROM UOPS might also be delivered in response to some condition such as a fault or other exceptional condition.  This event is an excellent mechanism for detecting instructions that require the use of MSROM instructions",
+},
+{
+	.name = "decode_restriction.predecode_wrong",
+	.event = "event=0xE9,umask=0x1,period=200003",
+	.desc = "Counts the number of times a decode restriction reduced the decode throughput due to wrong instruction length prediction",
+	.topic = "frontend",
+	.long_desc = "Counts the number of times a decode restriction reduced the decode throughput due to wrong instruction length prediction",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=200003",
+	.desc = "Stalls due to Memory ordering",
+	.topic = "memory",
+	.long_desc = "This event counts the number of times that pipeline was cleared due to memory ordering issues",
+},
+{
+	.name = "mem_uops_retired.dtlb_miss_loads",
+	.event = "event=0x04,umask=0x8,period=200003",
+	.desc = "Loads missed DTLB (Precise event)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of load ops retired that had DTLB miss (Precise event)",
+},
+{
+	.name = "page_walks.d_side_walks",
+	.event = "event=0x05,umask=0x1,period=100003,edge=1",
+	.desc = "D-side page-walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts when a data (D) page walk is completed or started.  Since a page walk implies a TLB miss, the number of TLB misses can be counted by counting the number of pagewalks",
+},
+{
+	.name = "page_walks.d_side_cycles",
+	.event = "event=0x05,umask=0x1,period=200003",
+	.desc = "Duration of D-side page-walks in core cycles",
+	.topic = "virtual memory",
+	.long_desc = "This event counts every cycle when a D-side (walks due to a load) page walk is in progress. Page walk duration divided by number of page walks is the average duration of page-walks",
+},
+{
+	.name = "page_walks.i_side_walks",
+	.event = "event=0x05,umask=0x2,period=100003,edge=1",
+	.desc = "I-side page-walks",
+	.topic = "virtual memory",
+	.long_desc = "This event counts when an instruction (I) page walk is completed or started.  Since a page walk implies a TLB miss, the number of TLB misses can be counted by counting the number of pagewalks",
+},
+{
+	.name = "page_walks.i_side_cycles",
+	.event = "event=0x05,umask=0x2,period=200003",
+	.desc = "Duration of I-side page-walks in core cycles",
+	.topic = "virtual memory",
+	.long_desc = "This event counts every cycle when a I-side (walks due to an instruction fetch) page walk is in progress. Page walk duration divided by number of page walks is the average duration of page-walks",
+},
+{
+	.name = "page_walks.walks",
+	.event = "event=0x05,umask=0x3,period=100003,edge=1",
+	.desc = "Total page walks that are completed (I-side and D-side)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts when a data (D) page walk or an instruction (I) page walk is completed or started.  Since a page walk implies a TLB miss, the number of TLB misses can be counted by counting the number of pagewalks",
+},
+{
+	.name = "page_walks.cycles",
+	.event = "event=0x05,umask=0x3,period=200003",
+	.desc = "Total cycles for all the page walks. (I-side and D-side)",
+	.topic = "virtual memory",
+	.long_desc = "This event counts every cycle when a data (D) page walk or instruction (I) page walk is in progress.  Since a pagewalk implies a TLB miss, the approximate cost of a TLB miss can be determined from this event",
+},
+{
+	.name = "l2_reject_xq.all",
+	.event = "event=0x30,umask=0x0,period=200003",
+	.desc = "Counts the number of request from the L2 that were not accepted into the XQ",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand and prefetch transactions that the L2 XQ rejects due to a full or near full condition which likely indicates back pressure from the IDI link. The XQ may reject transactions from the L2Q (non-cacheable requests), BBS (L2 misses) and WOB (L2 write-back victims)",
+},
+{
+	.name = "core_reject_l2q.all",
+	.event = "event=0x31,umask=0x0,period=200003",
+	.desc = "Counts the number of request that were not accepted into the L2Q because the L2Q is FULL",
+	.topic = "cache",
+	.long_desc = "Counts the number of (demand and L1 prefetchers) core requests rejected by the L2Q due to a full or nearly full w condition which likely indicates back pressure from L2Q.  It also counts requests that would have gone directly to the XQ, but are rejected due to a full or nearly full condition, indicating back pressure from the IDI link.  The L2Q may also reject transactions  from a core to insure fairness between cores, or to delay a core?s dirty eviction when the address conflicts incoming external snoops.  (Note that L2 prefetcher requests that are dropped are not counted by this event.)",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=200003",
+	.desc = "L2 cache requests from this core",
+	.topic = "cache",
+	.long_desc = "This event counts requests originating from the core that references a cache line in the L2 cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=200003",
+	.desc = "L2 cache request misses",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of L2 cache references and the number of L2 cache misses respectively",
+},
+{
+	.name = "fetch_stall.icache_fill_pending_cycles",
+	.event = "event=0x86,umask=0x4,period=200003",
+	.desc = "Counts the number of cycles the NIP stalls because of an icache miss. This is a cumulative count of cycles the NIP stalled for all icache misses",
+	.topic = "cache",
+},
+{
+	.name = "rehabq.ld_block_st_forward",
+	.event = "event=0x03,umask=0x1,period=200003",
+	.desc = "Loads blocked due to store forward restriction (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts the number of retired loads that were prohibited from receiving forwarded data from the store because of address mismatch (Precise event)",
+},
+{
+	.name = "rehabq.ld_block_std_notready",
+	.event = "event=0x03,umask=0x2,period=200003",
+	.desc = "Loads blocked due to store data not ready",
+	.topic = "cache",
+	.long_desc = "This event counts the cases where a forward was technically possible, but did not occur because the store data was not available at the right time",
+},
+{
+	.name = "rehabq.st_splits",
+	.event = "event=0x03,umask=0x4,period=200003",
+	.desc = "Store uops that split cache line boundary",
+	.topic = "cache",
+	.long_desc = "This event counts the number of retire stores that experienced cache line boundary splits",
+},
+{
+	.name = "rehabq.ld_splits",
+	.event = "event=0x03,umask=0x8,period=200003",
+	.desc = "Load uops that split cache line boundary (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts the number of retire loads that experienced cache line boundary splits (Precise event)",
+},
+{
+	.name = "rehabq.lock",
+	.event = "event=0x03,umask=0x10,period=200003",
+	.desc = "Uops with lock semantics",
+	.topic = "cache",
+	.long_desc = "This event counts the number of retired memory operations with lock semantics. These are either implicit locked instructions such as the XCHG instruction or instructions with an explicit LOCK prefix (0xF0)",
+},
+{
+	.name = "rehabq.sta_full",
+	.event = "event=0x03,umask=0x20,period=200003",
+	.desc = "Store address buffer full",
+	.topic = "cache",
+	.long_desc = "This event counts the number of retired stores that are delayed because there is not a store address buffer available",
+},
+{
+	.name = "rehabq.any_ld",
+	.event = "event=0x03,umask=0x40,period=200003",
+	.desc = "Any reissued load uops",
+	.topic = "cache",
+	.long_desc = "This event counts the number of load uops reissued from Rehabq",
+},
+{
+	.name = "rehabq.any_st",
+	.event = "event=0x03,umask=0x80,period=200003",
+	.desc = "Any reissued store uops",
+	.topic = "cache",
+	.long_desc = "This event counts the number of store uops reissued from Rehabq",
+},
+{
+	.name = "mem_uops_retired.l1_miss_loads",
+	.event = "event=0x04,umask=0x1,period=200003",
+	.desc = "Loads missed L1",
+	.topic = "cache",
+	.long_desc = "This event counts the number of load ops retired that miss in L1 Data cache. Note that prefetch misses will not be counted",
+},
+{
+	.name = "mem_uops_retired.l2_hit_loads",
+	.event = "event=0x04,umask=0x2,period=200003",
+	.desc = "Loads hit L2 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts the number of load ops retired that hit in the L2 (Precise event)",
+},
+{
+	.name = "mem_uops_retired.l2_miss_loads",
+	.event = "event=0x04,umask=0x4,period=100007",
+	.desc = "Loads missed L2 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts the number of load ops retired that miss in the L2 (Precise event)",
+},
+{
+	.name = "mem_uops_retired.utlb_miss",
+	.event = "event=0x04,umask=0x10,period=200003",
+	.desc = "Loads missed UTLB",
+	.topic = "cache",
+	.long_desc = "This event counts the number of load ops retired that had UTLB miss",
+},
+{
+	.name = "mem_uops_retired.hitm",
+	.event = "event=0x04,umask=0x20,period=200003",
+	.desc = "Cross core or cross module hitm (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts the number of load ops retired that got data from the other core or from the other module (Precise event)",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0x04,umask=0x40,period=200003",
+	.desc = "All Loads",
+	.topic = "cache",
+	.long_desc = "This event counts the number of load ops retired",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0x04,umask=0x80,period=200003",
+	.desc = "All Stores",
+	.topic = "cache",
+	.long_desc = "This event counts the number of store ops retired",
+},
+{
+	.name = "offcore_response",
+	.event = "event=0xB7,umask=0x1,period=100007",
+	.desc = "Offcore response can be programmed only with a specific pair of event select and counter MSR, and with specific event codes and predefine mask bit value in a dedicated MSR to specify attributes of the offcore transaction",
+	.topic = "cache",
+	.long_desc = "Offcore response can be programmed only with a specific pair of event select and counter MSR, and with specific event codes and predefine mask bit value in a dedicated MSR to specify attributes of the offcore transaction",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680000044",
+	.desc = "Counts any code reads (demand & prefetch) that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000044",
+	.desc = "Counts any code reads (demand & prefetch) that hit in the other module where modified copies were found in other core's L1 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000044",
+	.desc = "Counts any code reads (demand & prefetch) that miss L2 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.l2_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000044",
+	.desc = "Counts any code reads (demand & prefetch) that miss L2 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010044",
+	.desc = "Counts any code reads (demand & prefetch) that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680000022",
+	.desc = "Counts any rfo reads (demand & prefetch) that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000022",
+	.desc = "Counts any rfo reads (demand & prefetch) that hit in the other module where modified copies were found in other core's L1 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000022",
+	.desc = "Counts any rfo reads (demand & prefetch) that miss L2 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.l2_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000022",
+	.desc = "Counts any rfo reads (demand & prefetch) that miss L2 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_rfo.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010022",
+	.desc = "Counts any rfo reads (demand & prefetch) that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680003091",
+	.desc = "Counts any data read (demand & prefetch) that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000003091",
+	.desc = "Counts any data read (demand & prefetch) that hit in the other module where modified copies were found in other core's L1 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400003091",
+	.desc = "Counts any data read (demand & prefetch) that miss L2 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.l2_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200003091",
+	.desc = "Counts any data read (demand & prefetch) that miss L2 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000013091",
+	.desc = "Counts any data read (demand & prefetch) that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.streaming_stores.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680004800",
+	.desc = "Counts streaming store that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000008008",
+	.desc = "Counts any request that hit in the other module where modified copies were found in other core's L1 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400008008",
+	.desc = "Counts any request that miss L2 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.l2_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200008008",
+	.desc = "Counts any request that miss L2 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.any_request.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000018008",
+	.desc = "Counts any request that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680002000",
+	.desc = "Counts DCU hardware prefetcher data read that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000002000",
+	.desc = "Counts DCU hardware prefetcher data read that hit in the other module where modified copies were found in other core's L1 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400002000",
+	.desc = "Counts DCU hardware prefetcher data read that miss L2 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.l2_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200002000",
+	.desc = "Counts DCU hardware prefetcher data read that miss L2 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l1_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000012000",
+	.desc = "Counts DCU hardware prefetcher data read that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_writes.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680000100",
+	.desc = "Countsof demand RFO requests to write to partial cache lines that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.partial_reads.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680000080",
+	.desc = "Counts demand reads of partial cache lines (including UC and WC) that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680000040",
+	.desc = "Counts code reads generated by L2 prefetchers that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000040",
+	.desc = "Counts code reads generated by L2 prefetchers that miss L2 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_code_rd.l2_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000040",
+	.desc = "Counts code reads generated by L2 prefetchers that miss L2 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680000020",
+	.desc = "Counts RFO requests generated by L2 prefetchers that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000020",
+	.desc = "Counts RFO requests generated by L2 prefetchers that hit in the other module where modified copies were found in other core's L1 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000020",
+	.desc = "Counts RFO requests generated by L2 prefetchers that miss L2 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_rfo.l2_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000020",
+	.desc = "Counts RFO requests generated by L2 prefetchers that miss L2 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680000010",
+	.desc = "Counts data cacheline reads generated by L2 prefetchers that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000010",
+	.desc = "Counts data cacheline reads generated by L2 prefetchers that hit in the other module where modified copies were found in other core's L1 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000010",
+	.desc = "Counts data cacheline reads generated by L2 prefetchers that miss L2 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_l2_data_rd.l2_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000010",
+	.desc = "Counts data cacheline reads generated by L2 prefetchers that miss L2 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680000008",
+	.desc = "Counts writeback (modified to exclusive) that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.corewb.l2_miss.no_snoop_needed",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0080000008",
+	.desc = "Counts writeback (modified to exclusive) that miss L2 with no details on snoop-related information",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000004",
+	.desc = "Counts demand and DCU prefetch instruction cacheline that are are outstanding, per cycle, from the time of the L2 miss to when any response is received",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680000004",
+	.desc = "Counts demand and DCU prefetch instruction cacheline that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000004",
+	.desc = "Counts demand and DCU prefetch instruction cacheline that miss L2 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.l2_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000004",
+	.desc = "Counts demand and DCU prefetch instruction cacheline that miss L2 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_code_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010004",
+	.desc = "Counts demand and DCU prefetch instruction cacheline that have any response type",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000002",
+	.desc = "Counts demand and DCU prefetch RFOs that are are outstanding, per cycle, from the time of the L2 miss to when any response is received",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680000002",
+	.desc = "Counts demand and DCU prefetch RFOs that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000002",
+	.desc = "Counts demand and DCU prefetch RFOs that hit in the other module where modified copies were found in other core's L1 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000002",
+	.desc = "Counts demand and DCU prefetch RFOs that miss L2 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.l2_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000002",
+	.desc = "Counts demand and DCU prefetch RFOs that miss L2 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.outstanding",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x4000000001",
+	.desc = "Counts demand and DCU prefetch data read that are are outstanding, per cycle, from the time of the L2 miss to when any response is received",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_miss.any",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1680000001",
+	.desc = "Counts demand and DCU prefetch data read that miss L2",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_miss.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x1000000001",
+	.desc = "Counts demand and DCU prefetch data read that hit in the other module where modified copies were found in other core's L1 cache",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_miss.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0400000001",
+	.desc = "Counts demand and DCU prefetch data read that miss L2 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.l2_miss.snoop_miss",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0200000001",
+	.desc = "Counts demand and DCU prefetch data read that miss L2 with a snoop miss response",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_data_rd.any_response",
+	.event = "event=0xB7,umask=0x1,period=100007,offcore_rsp=0x0000010001",
+	.desc = "Counts demand and DCU prefetch data read that have any response type",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_event pme_broadwellx[] = {
+{
+	.name = "other_assists.avx_to_sse",
+	.event = "event=0xC1,umask=0x8,period=100003",
+	.desc = "Number of transitions from AVX-256 to legacy SSE when penalty applicable  Spec update: BDM30",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of transitions from AVX-256 to legacy SSE when penalty is applicable  Spec update: BDM30",
+},
+{
+	.name = "other_assists.sse_to_avx",
+	.event = "event=0xC1,umask=0x10,period=100003",
+	.desc = "Number of transitions from SSE to AVX-256 when penalty applicable  Spec update: BDM30",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of transitions from legacy SSE to AVX-256 when penalty is applicable  Spec update: BDM30",
+},
+{
+	.name = "fp_arith_inst_retired.scalar_double",
+	.event = "event=0xC7,umask=0x1,period=2000003",
+	.desc = "Number of SSE/AVX computational scalar double precision floating-point instructions retired.  Each count represents 1 computation. Applies to SSE* and AVX* scalar double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT FM(N)ADD/SUB.  FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.scalar_single",
+	.event = "event=0xC7,umask=0x2,period=2000003",
+	.desc = "Number of SSE/AVX computational scalar single precision floating-point instructions retired.  Each count represents 1 computation. Applies to SSE* and AVX* scalar single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT FM(N)ADD/SUB.  FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.128b_packed_double",
+	.event = "event=0xC7,umask=0x4,period=2000003",
+	.desc = "Number of SSE/AVX computational 128-bit packed double precision floating-point instructions retired.  Each count represents 2 computations. Applies to SSE* and AVX* packed double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.128b_packed_single",
+	.event = "event=0xC7,umask=0x8,period=2000003",
+	.desc = "Number of SSE/AVX computational 128-bit packed single precision floating-point instructions retired.  Each count represents 4 computations. Applies to SSE* and AVX* packed single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.256b_packed_double",
+	.event = "event=0xC7,umask=0x10,period=2000003",
+	.desc = "Number of SSE/AVX computational 256-bit packed double precision floating-point instructions retired.  Each count represents 4 computations. Applies to SSE* and AVX* packed double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_assist.x87_output",
+	.event = "event=0xCA,umask=0x2,period=100003",
+	.desc = "Number of X87 assists due to output value",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of x87 floating point (FP) micro-code assist (numeric overflow/underflow, inexact result) when the output value (destination register) is invalid",
+},
+{
+	.name = "fp_assist.x87_input",
+	.event = "event=0xCA,umask=0x4,period=100003",
+	.desc = "Number of X87 assists due to input value",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts x87 floating point (FP) micro-code assist (invalid operation, denormal operand, SNaN operand) when the input value (one of the source operands to an FP instruction) is invalid",
+},
+{
+	.name = "fp_assist.simd_output",
+	.event = "event=0xCA,umask=0x8,period=100003",
+	.desc = "Number of SIMD FP assists due to Output values",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of SSE* floating point (FP) micro-code assist (numeric overflow/underflow) when the output value (destination register) is invalid. Counting covers only cases involving penalties that require micro-code assist intervention",
+},
+{
+	.name = "fp_assist.simd_input",
+	.event = "event=0xCA,umask=0x10,period=100003",
+	.desc = "Number of SIMD FP assists due to input values",
+	.topic = "floating point",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts any input SSE* FP assist - invalid operation, denormal operand, dividing by zero, SNaN operand. Counting includes only cases involving penalties that required micro-code assist intervention",
+},
+{
+	.name = "fp_assist.any",
+	.event = "event=0xCA,umask=0x1e,cmask=1,period=100003",
+	.desc = "Cycles with any input/output SSE or FP assist",
+	.topic = "floating point",
+	.long_desc = "This event counts cycles with any input and output SSE or x87 FP assist. If an input and output assist are detected on the same cycle the event increments by 1",
+},
+{
+	.name = "fp_arith_inst_retired.256b_packed_single",
+	.event = "event=0xc7,umask=0x20,period=2000003",
+	.desc = "Number of SSE/AVX computational 256-bit packed single precision floating-point instructions retired.  Each count represents 8 computations. Applies to SSE* and AVX* packed single precision floating-point instructions: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element (Precise event)",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.scalar",
+	.event = "event=0xC7,umask=0x3,period=2000003",
+	.desc = "Number of SSE/AVX computational scalar floating-point instructions retired. Applies to SSE* and AVX* scalar, double and single precision floating-point: ADD SUB MUL DIV MIN MAX RSQRT RCP SQRT FM(N)ADD/SUB. FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.packed",
+	.event = "event=0xC7,umask=0x3c,period=2000004",
+	.desc = "Number of SSE/AVX computational packed floating-point instructions retired. Applies to SSE* and AVX*, packed, double and single precision floating-point: ADD SUB MUL DIV MIN MAX RSQRT RCP SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.single",
+	.event = "event=0xC7,umask=0x2a,period=2000005",
+	.desc = "Number of SSE/AVX computational single precision floating-point instructions retired. Applies to SSE* and AVX*scalar, double and single precision floating-point: ADD SUB MUL DIV MIN MAX RCP RSQRT SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element. ?",
+	.topic = "floating point",
+},
+{
+	.name = "fp_arith_inst_retired.double",
+	.event = "event=0xC7,umask=0x15,period=2000006",
+	.desc = "Number of SSE/AVX computational double precision floating-point instructions retired. Applies to SSE* and AVX*scalar, double and single precision floating-point: ADD SUB MUL DIV MIN MAX SQRT DPP FM(N)ADD/SUB.  DPP and FM(N)ADD/SUB instructions count twice as they perform multiple calculations per element.  ?",
+	.topic = "floating point",
+},
+{
+	.name = "inst_retired.any",
+	.event = "event=0xc0",
+	.desc = "Instructions retired from execution",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions retired from execution. For instructions that consist of multiple micro-ops, this event counts the retirement of the last micro-op of the instruction. Counting continues during hardware interrupts, traps, and inside interrupt handlers. \nNotes: INST_RETIRED.ANY is counted by a designated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events. INST_RETIRED.ANY_P is counted by a programmable counter and it is an architectural performance event. \nCounting: Faulting executions of GETSEC/VM entry/VM Exit/MWait will not count as retired instructions",
+},
+{
+	.name = "cpu_clk_unhalted.thread",
+	.event = "event=0x3c",
+	.desc = "Core cycles when the thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of core cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. This event is a component in many key event ratios. The core frequency may change from time to time due to transitions associated with Enhanced Intel SpeedStep Technology or TM2. For this reason this event may have a changing ratio with regards to time. When the core frequency is constant, this event can approximate elapsed time while the core was not in the halt state. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events",
+},
+{
+	.name = "cpu_clk_unhalted.ref_tsc",
+	.event = "event=0x00,umask=0x3,period=2000003",
+	.desc = "Reference cycles when the core is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of reference cycles when the core is not in a halt state. The core enters the halt state when it is running the HLT instruction or the MWAIT instruction. This event is not affected by core frequency changes (for example, P states, TM2 transitions) but has the same incrementing frequency as the time stamp counter. This event can approximate elapsed time while the core was not in a halt state. This event has a constant ratio with the CPU_CLK_UNHALTED.REF_XCLK event. It is counted on a dedicated fixed counter, leaving the four (eight when Hyperthreading is disabled) programmable counters available for other events. \nNote: On all current platforms this event stops counting during 'throttling (TM)' states duty off periods the processor is 'halted'.  This event is clocked by base clock (100 Mhz) on Sandy Bridge. The counter update is done at a lower clock rate then the core clock the overflow status bit for this counter may appear 'sticky'.  After the counter has overflowed and software clears the overflow status bit and resets the counter to less than MAX. The reset value to the counter is not clocked immediately so the overflow status bit will flip 'high (1)' and generate another PMI (if enabled) after which the reset value gets clocked into the counter. Therefore, software will get the interrupt, read the overflow status bit '1 for bit 34 while the counter value is less than MAX. Software should ignore this case",
+},
+{
+	.name = "ld_blocks.store_forward",
+	.event = "event=0x03,umask=0x2,period=100003",
+	.desc = "Cases when loads get true Block-on-Store blocking code preventing store forwarding",
+	.topic = "pipeline",
+	.long_desc = "This event counts how many times the load operation got the true Block-on-Store blocking code preventing store forwarding. This includes cases when:\n - preceding store conflicts with the load (incomplete overlap);\n - store forwarding is impossible due to u-arch limitations;\n - preceding lock RMW operations are not forwarded;\n - store has the no-forward bit set (uncacheable/page-split/masked stores);\n - all-blocking stores are used (mostly, fences and port I/O);\nand others.\nThe most common case is a load blocked due to its address range overlapping with a preceding smaller uncompleted store. Note: This event does not take into account cases of out-of-SW-control (for example, SbTailHit), unknown physical STA, and cases of blocking loads on store due to being non-WB memory type or a lock. These cases are covered by other events.\nSee the table of not supported store forwards in the Optimization Guide",
+},
+{
+	.name = "ld_blocks.no_sr",
+	.event = "event=0x03,umask=0x8,period=100003",
+	.desc = "This event counts the number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use",
+	.topic = "pipeline",
+},
+{
+	.name = "ld_blocks_partial.address_alias",
+	.event = "event=0x07,umask=0x1,period=100003",
+	.desc = "False dependencies in MOB due to partial compare",
+	.topic = "pipeline",
+	.long_desc = "This event counts false dependencies in MOB when the partial comparison upon loose net check and dependency was resolved by the Enhanced Loose net mechanism. This may not result in high performance penalties. Loose net checks can fail when loads and stores are 4k aliased",
+},
+{
+	.name = "int_misc.rat_stall_cycles",
+	.event = "event=0x0D,umask=0x8,period=2000003",
+	.desc = "Cycles when Resource Allocation Table (RAT) external stall is sent to Instruction Decode Queue (IDQ) for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of cycles during which Resource Allocation Table (RAT) external stall is sent to Instruction Decode Queue (IDQ) for the current thread. This also includes the cycles during which the Allocator is serving another thread",
+},
+{
+	.name = "int_misc.recovery_cycles",
+	.event = "event=0x0D,umask=0x3,cmask=1,period=2000003",
+	.desc = "Number of cycles waiting for the checkpoints in Resource Allocation Table (RAT) to be recovered after Nuke due to all other cases except JEClear (e.g. whenever a ucode assist is needed like SSE exception, memory disambiguation, etc...)",
+	.topic = "pipeline",
+	.long_desc = "Cycles checkpoints in Resource Allocation Table (RAT) are recovering from JEClear or machine clear",
+},
+{
+	.name = "uops_issued.any",
+	.event = "event=0x0E,umask=0x1,period=2000003",
+	.desc = "Uops that Resource Allocation Table (RAT) issues to Reservation Station (RS)",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of Uops issued by the Resource Allocation Table (RAT) to the reservation station (RS)",
+},
+{
+	.name = "uops_issued.flags_merge",
+	.event = "event=0x0E,umask=0x10,period=2000003",
+	.desc = "Number of flags-merge uops being allocated. Such uops considered perf sensitive; added by GSR u-arch",
+	.topic = "pipeline",
+	.long_desc = "Number of flags-merge uops being allocated. Such uops considered perf sensitive\n added by GSR u-arch",
+},
+{
+	.name = "uops_issued.slow_lea",
+	.event = "event=0x0E,umask=0x20,period=2000003",
+	.desc = "Number of slow LEA uops being allocated. A uop is generally considered SlowLea if it has 3 sources (e.g. 2 sources + immediate) regardless if as a result of LEA instruction or not",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.single_mul",
+	.event = "event=0x0E,umask=0x40,period=2000003",
+	.desc = "Number of Multiply packed/scalar single precision uops allocated",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_issued.stall_cycles",
+	.event = "inv=1,event=0x0E,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles when Resource Allocation Table (RAT) does not issue Uops to Reservation Station (RS) for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which the Resource Allocation Table (RAT) does not issue any Uops to the reservation station (RS) for the current thread",
+},
+{
+	.name = "arith.fpu_div_active",
+	.event = "event=0x14,umask=0x1,period=2000003",
+	.desc = "Cycles when divider is busy executing divide operations",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of the divide operations executed. Uses edge-detect and a cmask value of 1 on ARITH.FPU_DIV_ACTIVE to get the number of the divide operations executed",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "This is a fixed-frequency event programmed to general counters. It counts when the core is unhalted at 100 Mhz",
+},
+{
+	.name = "cpu_clk_thread_unhalted.one_thread_active",
+	.event = "event=0x3c,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "load_hit_pre.sw_pf",
+	.event = "event=0x4c,umask=0x1,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for software prefetch",
+	.topic = "pipeline",
+	.long_desc = "This event counts all not software-prefetch load dispatches that hit the fill buffer (FB) allocated for the software prefetch. It can also be incremented by some lock instructions. So it should only be used with profiling so that the locks can be excluded by asm inspection of the nearby instructions",
+},
+{
+	.name = "load_hit_pre.hw_pf",
+	.event = "event=0x4C,umask=0x2,period=100003",
+	.desc = "Not software-prefetch load dispatches that hit FB allocated for hardware prefetch",
+	.topic = "pipeline",
+	.long_desc = "This event counts all not software-prefetch load dispatches that hit the fill buffer (FB) allocated for the hardware prefetch",
+},
+{
+	.name = "move_elimination.int_eliminated",
+	.event = "event=0x58,umask=0x1,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.simd_eliminated",
+	.event = "event=0x58,umask=0x2,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.int_not_eliminated",
+	.event = "event=0x58,umask=0x4,period=1000003",
+	.desc = "Number of integer Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "move_elimination.simd_not_eliminated",
+	.event = "event=0x58,umask=0x8,period=1000003",
+	.desc = "Number of SIMD Move Elimination candidate uops that were not eliminated",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_cycles",
+	.event = "event=0x5E,umask=0x1,period=2000003",
+	.desc = "Cycles when Reservation Station (RS) is empty for the thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which the reservation station (RS) is empty for the thread.\nNote: In ST-mode, not active thread should drive 0. This is usually caused by severely costly branch mispredictions, or allocator/FE issues",
+},
+{
+	.name = "ild_stall.lcp",
+	.event = "event=0x87,umask=0x1,period=2000003",
+	.desc = "Stalls caused by changing prefix length of the instruction",
+	.topic = "pipeline",
+	.long_desc = "This event counts stalls occured due to changing prefix length (66, 67 or REX.W when they change the length of the decoded instruction). Occurrences counting is proportional to the number of prefixes in a 16B-line. This may result in the following penalties: three-cycle penalty for each LCP in a 16-byte chunk",
+},
+{
+	.name = "br_inst_exec.nontaken_conditional",
+	.event = "event=0x88,umask=0x41,period=200003",
+	.desc = "Not taken macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts not taken macro-conditional branch instructions",
+},
+{
+	.name = "br_inst_exec.taken_conditional",
+	.event = "event=0x88,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired macro-conditional branch instructions",
+},
+{
+	.name = "br_inst_exec.taken_direct_jump",
+	.event = "event=0x88,umask=0x82,period=200003",
+	.desc = "Taken speculative and retired macro-conditional branch instructions excluding calls and indirects",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired macro-conditional branch instructions excluding calls and indirect branches",
+},
+{
+	.name = "br_inst_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired indirect branches excluding calls and return branches",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_return",
+	.event = "event=0x88,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired indirect branches with return mnemonic",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired indirect branches that have a return mnemonic",
+},
+{
+	.name = "br_inst_exec.taken_direct_near_call",
+	.event = "event=0x88,umask=0x90,period=200003",
+	.desc = "Taken speculative and retired direct near calls",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired direct near calls",
+},
+{
+	.name = "br_inst_exec.taken_indirect_near_call",
+	.event = "event=0x88,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired indirect calls",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired indirect calls including both register and memory indirect",
+},
+{
+	.name = "br_inst_exec.all_conditional",
+	.event = "event=0x88,umask=0xc1,period=200003",
+	.desc = "Speculative and retired macro-conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired macro-conditional branch instructions",
+},
+{
+	.name = "br_inst_exec.all_direct_jmp",
+	.event = "event=0x88,umask=0xc2,period=200003",
+	.desc = "Speculative and retired macro-unconditional branches excluding calls and indirects",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired macro-unconditional branch instructions, excluding calls and indirects",
+},
+{
+	.name = "br_inst_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x88,umask=0xc4,period=200003",
+	.desc = "Speculative and retired indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired indirect branches excluding calls and return branches",
+},
+{
+	.name = "br_inst_exec.all_indirect_near_return",
+	.event = "event=0x88,umask=0xc8,period=200003",
+	.desc = "Speculative and retired indirect return branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired indirect branches that have a return mnemonic",
+},
+{
+	.name = "br_inst_exec.all_direct_near_call",
+	.event = "event=0x88,umask=0xd0,period=200003",
+	.desc = "Speculative and retired direct near calls",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired direct near calls",
+},
+{
+	.name = "br_inst_exec.all_branches",
+	.event = "event=0x88,umask=0xff,period=200003",
+	.desc = "Speculative and retired  branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired branch instructions",
+},
+{
+	.name = "br_misp_exec.nontaken_conditional",
+	.event = "event=0x89,umask=0x41,period=200003",
+	.desc = "Not taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts not taken speculative and retired mispredicted macro conditional branch instructions",
+},
+{
+	.name = "br_misp_exec.taken_conditional",
+	.event = "event=0x89,umask=0x81,period=200003",
+	.desc = "Taken speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired mispredicted macro conditional branch instructions",
+},
+{
+	.name = "br_misp_exec.taken_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0x84,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired mispredicted indirect branches excluding calls and returns",
+},
+{
+	.name = "br_misp_exec.taken_return_near",
+	.event = "event=0x89,umask=0x88,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect branches with return mnemonic",
+	.topic = "pipeline",
+	.long_desc = "This event counts taken speculative and retired mispredicted indirect branches that have a return mnemonic",
+},
+{
+	.name = "br_misp_exec.all_conditional",
+	.event = "event=0x89,umask=0xc1,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired mispredicted macro conditional branch instructions",
+},
+{
+	.name = "br_misp_exec.all_indirect_jump_non_call_ret",
+	.event = "event=0x89,umask=0xc4,period=200003",
+	.desc = "Mispredicted indirect branches excluding calls and returns",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken mispredicted indirect branches excluding calls and returns",
+},
+{
+	.name = "br_misp_exec.all_branches",
+	.event = "event=0x89,umask=0xff,period=200003",
+	.desc = "Speculative and retired mispredicted macro conditional branches",
+	.topic = "pipeline",
+	.long_desc = "This event counts both taken and not taken speculative and retired mispredicted branch instructions",
+},
+{
+	.name = "uops_dispatched_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 0",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 0",
+},
+{
+	.name = "uops_dispatched_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 1",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 1",
+},
+{
+	.name = "uops_dispatched_port.port_2",
+	.event = "event=0xA1,umask=0x4,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 2",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 2",
+},
+{
+	.name = "uops_dispatched_port.port_3",
+	.event = "event=0xA1,umask=0x8,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 3",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 3",
+},
+{
+	.name = "uops_dispatched_port.port_4",
+	.event = "event=0xA1,umask=0x10,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 4",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 4",
+},
+{
+	.name = "uops_dispatched_port.port_5",
+	.event = "event=0xA1,umask=0x20,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 5",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 5",
+},
+{
+	.name = "uops_dispatched_port.port_6",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 6",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 6",
+},
+{
+	.name = "uops_dispatched_port.port_7",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 7",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 7",
+},
+{
+	.name = "resource_stalls.any",
+	.event = "event=0xA2,umask=0x1,period=2000003",
+	.desc = "Resource-related stall cycles",
+	.topic = "pipeline",
+	.long_desc = "This event counts resource-related stall cycles. Reasons for stalls can be as follows:\n - *any* u-arch structure got full (LB, SB, RS, ROB, BOB, LM, Physical Register Reclaim Table (PRRT), or Physical History Table (PHT) slots)\n - *any* u-arch structure got empty (like INT/SIMD FreeLists)\n - FPU control word (FPCW), MXCSR\nand others. This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "resource_stalls.rs",
+	.event = "event=0xA2,umask=0x4,period=2000003",
+	.desc = "Cycles stalled due to no eligible RS entry available",
+	.topic = "pipeline",
+	.long_desc = "This event counts stall cycles caused by absence of eligible entries in the reservation station (RS). This may result from RS overflow, or from RS deallocation because of the RS array Write Port allocation scheme (each RS entry has two write ports instead of four. As a result, empty entries could not be used, although RS is not really full). This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "resource_stalls.sb",
+	.event = "event=0xA2,umask=0x8,period=2000003",
+	.desc = "Cycles stalled due to no store buffers available. (not including draining form sync)",
+	.topic = "pipeline",
+	.long_desc = "This event counts stall cycles caused by the store buffer (SB) overflow (excluding draining from synch). This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "resource_stalls.rob",
+	.event = "event=0xA2,umask=0x10,period=2000003",
+	.desc = "Cycles stalled due to re-order buffer full",
+	.topic = "pipeline",
+	.long_desc = "This event counts ROB full stall cycles. This counts cycles that the pipeline backend blocked uop delivery from the front end",
+},
+{
+	.name = "cycle_activity.cycles_l2_pending",
+	.event = "event=0xA3,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles the CPU has at least one pending  demand* load request missing the L2 cache",
+},
+{
+	.name = "cycle_activity.cycles_l1d_pending",
+	.event = "event=0xA3,umask=0x8,cmask=8,period=2000003",
+	.desc = "Cycles while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles the CPU has at least one pending  demand load request missing the L1 data cache",
+},
+{
+	.name = "cycle_activity.cycles_ldm_pending",
+	.event = "event=0xA3,umask=0x2,cmask=2,period=2000003",
+	.desc = "Cycles while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles the CPU has at least one pending  demand load request (that is cycles with non-completed load waiting for its data from memory subsystem)",
+},
+{
+	.name = "cycle_activity.cycles_no_execute",
+	.event = "event=0xA3,umask=0x4,cmask=4,period=2000003",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port",
+},
+{
+	.name = "cycle_activity.stalls_l2_pending",
+	.event = "event=0xA3,umask=0x5,cmask=5,period=2000003",
+	.desc = "Execution stalls while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port, while there was at least one pending demand* load request missing the L2 cache.(as a footprint) * includes also L1 HW prefetch requests that may or may not be required by demands",
+},
+{
+	.name = "cycle_activity.stalls_ldm_pending",
+	.event = "event=0xA3,umask=0x6,cmask=6,period=2000003",
+	.desc = "Execution stalls while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port, while there was at least one pending demand load request",
+},
+{
+	.name = "cycle_activity.stalls_l1d_pending",
+	.event = "event=0xA3,umask=0xc,cmask=12,period=2000003",
+	.desc = "Execution stalls while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+	.long_desc = "Counts number of cycles nothing is executed on any execution port, while there was at least one pending demand load request missing the L1 data cache",
+},
+{
+	.name = "lsd.uops",
+	.event = "event=0xA8,umask=0x1,period=2000003",
+	.desc = "Number of Uops delivered by the LSD",
+	.topic = "pipeline",
+	.long_desc = "Number of Uops delivered by the LSD",
+},
+{
+	.name = "uops_executed.thread",
+	.event = "event=0xB1,umask=0x1,period=2000003",
+	.desc = "Counts the number of uops to be executed per-thread each cycle",
+	.topic = "pipeline",
+	.long_desc = "Number of uops to be executed per-thread each cycle",
+},
+{
+	.name = "uops_executed.core",
+	.event = "event=0xB1,umask=0x2,period=2000003",
+	.desc = "Number of uops executed on the core",
+	.topic = "pipeline",
+	.long_desc = "Number of uops executed from any thread",
+},
+{
+	.name = "uops_executed.stall_cycles",
+	.event = "inv=1,event=0xB1,umask=0x1,cmask=1,period=2000003",
+	.desc = "Counts number of cycles no uops were dispatched to be executed on this thread",
+	.topic = "pipeline",
+	.long_desc = "This event counts cycles during which no uops were dispatched from the Reservation Station (RS) per thread",
+},
+{
+	.name = "inst_retired.any_p",
+	.event = "event=0xc0",
+	.desc = "Number of instructions retired. General Counter   - architectural event  Spec update: BDM61",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of instructions (EOMs) retired. Counting covers macro-fused instructions individually (that is, increments by two)  Spec update: BDM61",
+},
+{
+	.name = "inst_retired.x87",
+	.event = "event=0xC0,umask=0x2,period=2000003",
+	.desc = "FP operations  retired. X87 FP operations that have no exceptions:",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts FP operations retired. For X87 FP operations that have no exceptions counting also includes flows that have several X87, or flows that use X87 uops in the exception handling",
+},
+{
+	.name = "inst_retired.prec_dist",
+	.event = "event=0xC0,umask=0x1,period=2000003",
+	.desc = "Precise instruction retired event with HW to reduce effect of PEBS shadow in IP distribution  Spec update: BDM11, BDM55 (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version (that is, uses PEBS) of the event that counts instructions retired  Spec update: BDM11, BDM55 (Must be precise)",
+},
+{
+	.name = "other_assists.any_wb_assist",
+	.event = "event=0xC1,umask=0x40,period=100003",
+	.desc = "Number of times any microcode assist is invoked by HW upon uop writeback",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_retired.all",
+	.event = "event=0xC2,umask=0x1,period=2000003",
+	.desc = "Actually retired uops  Supports address when precise (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This event counts all actually retired uops. Counting increments by two for micro-fused uops, and by one for macro-fused and other uops. Maximal increment value for one cycle is eight  Supports address when precise (Precise event)",
+},
+{
+	.name = "uops_retired.retire_slots",
+	.event = "event=0xC2,umask=0x2,period=2000003",
+	.desc = "Retirement slots used (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts the number of retirement slots used (Precise event)",
+},
+{
+	.name = "uops_retired.stall_cycles",
+	.event = "inv=1,event=0xC2,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles without actually retired uops",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts cycles without actually retired uops",
+},
+{
+	.name = "uops_retired.total_cycles",
+	.event = "inv=1,event=0xC2,umask=0x1,cmask=10,period=2000003",
+	.desc = "Cycles with less than 10 actually retired uops",
+	.topic = "pipeline",
+	.long_desc = "Number of cycles using always true condition (uops_ret < 16) applied to non PEBS uops retired event",
+},
+{
+	.name = "machine_clears.cycles",
+	.event = "event=0xC3,umask=0x1,period=2000003",
+	.desc = "Cycles there was a Nuke. Account for both thread-specific and All Thread Nukes",
+	.topic = "pipeline",
+	.long_desc = "This event counts both thread-specific (TS) and all-thread (AT) nukes",
+},
+{
+	.name = "machine_clears.smc",
+	.event = "event=0xC3,umask=0x4,period=100003",
+	.desc = "Self-modifying code (SMC) detected",
+	.topic = "pipeline",
+	.long_desc = "This event counts self-modifying code (SMC) detected, which causes a machine clear",
+},
+{
+	.name = "machine_clears.maskmov",
+	.event = "event=0xC3,umask=0x20,period=100003",
+	.desc = "This event counts the number of executed Intel AVX masked load operations that refer to an illegal address range with the mask bits set to 0",
+	.topic = "pipeline",
+	.long_desc = "Maskmov false fault - counts number of time ucode passes through Maskmov flow due to instruction's mask being 0 while the flow was completed without raising a fault",
+},
+{
+	.name = "br_inst_retired.conditional",
+	.event = "event=0xC4,umask=0x1,period=400009",
+	.desc = "Conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.near_call",
+	.event = "event=0xC4,umask=0x2,period=100007",
+	.desc = "Direct and indirect near call instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts both direct and indirect near call instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.all_branches",
+	.event = "event=0xC4,umask=0x0,period=400009",
+	.desc = "All (macro) branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts all (macro) branch instructions retired",
+},
+{
+	.name = "br_inst_retired.near_return",
+	.event = "event=0xC4,umask=0x8,period=100007",
+	.desc = "Return instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts return instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.not_taken",
+	.event = "event=0xC4,umask=0x10,period=400009",
+	.desc = "Not taken branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts not taken branch instructions retired",
+},
+{
+	.name = "br_inst_retired.near_taken",
+	.event = "event=0xC4,umask=0x20,period=400009",
+	.desc = "Taken branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts taken branch instructions retired (Precise event)",
+},
+{
+	.name = "br_inst_retired.far_branch",
+	.event = "event=0xC4,umask=0x40,period=100007",
+	.desc = "Far branch instructions retired  Spec update: BDW98",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts far branch instructions retired  Spec update: BDW98",
+},
+{
+	.name = "br_inst_retired.all_branches_pebs",
+	.event = "event=0xC4,umask=0x4,period=400009",
+	.desc = "All (macro) branch instructions retired. (Precise Event - PEBS)  Spec update: BDW98 (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version of BR_INST_RETIRED.ALL_BRANCHES that counts all (macro) branch instructions retired  Spec update: BDW98 (Must be precise)",
+},
+{
+	.name = "br_misp_retired.conditional",
+	.event = "event=0xC5,umask=0x1,period=400009",
+	.desc = "Mispredicted conditional branch instructions retired (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts mispredicted conditional branch instructions retired (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches",
+	.event = "event=0xC5,umask=0x0,period=400009",
+	.desc = "All mispredicted macro branch instructions retired",
+	.topic = "pipeline",
+	.long_desc = "This event counts all mispredicted macro branch instructions retired",
+},
+{
+	.name = "br_misp_retired.ret",
+	.event = "event=0xC5,umask=0x8,period=100007",
+	.desc = "This event counts the number of mispredicted ret instructions retired. Non PEBS (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts mispredicted return instructions retired (Precise event)",
+},
+{
+	.name = "br_misp_retired.all_branches_pebs",
+	.event = "event=0xC5,umask=0x4,period=400009",
+	.desc = "Mispredicted macro branch instructions retired. (Precise Event - PEBS) (Must be precise)",
+	.topic = "pipeline",
+	.long_desc = "This is a precise version of BR_MISP_RETIRED.ALL_BRANCHES that counts all mispredicted macro branch instructions retired (Must be precise)",
+},
+{
+	.name = "rob_misc_events.lbr_inserts",
+	.event = "event=0xCC,umask=0x20,period=2000003",
+	.desc = "Count cases of saving new LBR",
+	.topic = "pipeline",
+	.long_desc = "This event counts cases of saving new LBR records by hardware. This assumes proper enabling of LBRs and takes into account LBR filtering done by the LBR_SELECT register",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p",
+	.event = "event=0x3C,umask=0x0,period=2000003",
+	.desc = "Thread cycles when thread is not in halt state",
+	.topic = "pipeline",
+	.long_desc = "This is an architectural event that counts the number of thread cycles while the thread is not in a halt state. The thread enters the halt state when it is running the HLT instruction. The core frequency may change from time to time due to power or thermal throttling. For this reason, this event may have a changing ratio with regards to wall clock time",
+},
+{
+	.name = "br_misp_exec.taken_indirect_near_call",
+	.event = "event=0x89,umask=0xa0,period=200003",
+	.desc = "Taken speculative and retired mispredicted indirect calls",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_0_core",
+	.event = "event=0xA1,umask=0x1,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 0",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_1_core",
+	.event = "event=0xA1,umask=0x2,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 1",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_2_core",
+	.event = "event=0xA1,umask=0x4,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 2",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_3_core",
+	.event = "event=0xA1,umask=0x8,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 3",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_4_core",
+	.event = "event=0xA1,umask=0x10,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 4",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_5_core",
+	.event = "event=0xA1,umask=0x20,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 5",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_6_core",
+	.event = "event=0xA1,umask=0x40,any=1,period=2000003",
+	.desc = "Cycles per core when uops are exectuted in port 6",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_7_core",
+	.event = "event=0xA1,umask=0x80,any=1,period=2000003",
+	.desc = "Cycles per core when uops are dispatched to port 7",
+	.topic = "pipeline",
+},
+{
+	.name = "br_misp_retired.near_taken",
+	.event = "event=0xC5,umask=0x20,period=400009",
+	.desc = "number of near branch instructions retired that were mispredicted and taken (Precise event)",
+	.topic = "pipeline",
+	.long_desc = "Number of near branch instructions retired that were mispredicted and taken (Precise event)",
+},
+{
+	.name = "uops_executed.cycles_ge_1_uop_exec",
+	.event = "event=0xB1,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles where at least 1 uop was executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.cycles_ge_2_uops_exec",
+	.event = "event=0xB1,umask=0x1,cmask=2,period=2000003",
+	.desc = "Cycles where at least 2 uops were executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.cycles_ge_3_uops_exec",
+	.event = "event=0xB1,umask=0x1,cmask=3,period=2000003",
+	.desc = "Cycles where at least 3 uops were executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.cycles_ge_4_uops_exec",
+	.event = "event=0xB1,umask=0x1,cmask=4,period=2000003",
+	.desc = "Cycles where at least 4 uops were executed per-thread",
+	.topic = "pipeline",
+},
+{
+	.name = "baclears.any",
+	.event = "event=0xe6,umask=0x1f,period=100003",
+	.desc = "Counts the total number when the front end is resteered, mainly when the BPU cannot provide a correct prediction and this is corrected by other branch handling mechanisms at the front end",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l1d_miss",
+	.event = "event=0xA3,umask=0x8,cmask=8,period=2000003",
+	.desc = "Cycles while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_l2_miss",
+	.event = "event=0xA3,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.cycles_mem_any",
+	.event = "event=0xA3,umask=0x2,cmask=2,period=2000003",
+	.desc = "Cycles while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_total",
+	.event = "event=0xA3,umask=0x4,cmask=4,period=2000003",
+	.desc = "Total execution stalls",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l1d_miss",
+	.event = "event=0xA3,umask=0xc,cmask=12,period=2000003",
+	.desc = "Execution stalls while L1 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_l2_miss",
+	.event = "event=0xA3,umask=0x5,cmask=5,period=2000003",
+	.desc = "Execution stalls while L2 cache miss demand load is outstanding",
+	.topic = "pipeline",
+},
+{
+	.name = "cycle_activity.stalls_mem_any",
+	.event = "event=0xA3,umask=0x6,cmask=6,period=2000003",
+	.desc = "Execution stalls while memory subsystem has an outstanding load",
+	.topic = "pipeline",
+},
+{
+	.name = "machine_clears.count",
+	.event = "edge=1,event=0xC3,umask=0x1,cmask=1,period=100003",
+	.desc = "Number of machine clears (nukes) of any type",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_4_uops",
+	.event = "event=0xA8,umask=0x1,cmask=4,period=2000003",
+	.desc = "Cycles 4 Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "rs_events.empty_end",
+	.event = "edge=1,inv=1,event=0x5E,umask=0x1,cmask=1,period=200003",
+	.desc = "Counts end of periods where the Reservation Station (RS) was empty. Could be useful to precisely locate Frontend Latency Bound issues",
+	.topic = "pipeline",
+},
+{
+	.name = "lsd.cycles_active",
+	.event = "event=0xA8,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles Uops delivered by the LSD, but didn't come from the decoder",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed_port.port_0",
+	.event = "event=0xA1,umask=0x1,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 0",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 0",
+},
+{
+	.name = "uops_executed_port.port_1",
+	.event = "event=0xA1,umask=0x2,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 1",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 1",
+},
+{
+	.name = "uops_executed_port.port_2",
+	.event = "event=0xA1,umask=0x4,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 2",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 2",
+},
+{
+	.name = "uops_executed_port.port_3",
+	.event = "event=0xA1,umask=0x8,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 3",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 3",
+},
+{
+	.name = "uops_executed_port.port_4",
+	.event = "event=0xA1,umask=0x10,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 4",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 4",
+},
+{
+	.name = "uops_executed_port.port_5",
+	.event = "event=0xA1,umask=0x20,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 5",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 5",
+},
+{
+	.name = "uops_executed_port.port_6",
+	.event = "event=0xA1,umask=0x40,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 6",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 6",
+},
+{
+	.name = "uops_executed_port.port_7",
+	.event = "event=0xA1,umask=0x80,period=2000003",
+	.desc = "Cycles per thread when uops are executed in port 7",
+	.topic = "pipeline",
+	.long_desc = "This event counts, on the per-thread basis, cycles during which uops are dispatched from the Reservation Station (RS) to port 7",
+},
+{
+	.name = "uop_dispatches_cancelled.simd_prf",
+	.event = "event=0xA0,umask=0x3,period=2000003",
+	.desc = "Micro-op dispatches cancelled due to insufficient SIMD physical register file read ports",
+	.topic = "pipeline",
+	.long_desc = "This event counts the number of micro-operations cancelled after they were dispatched from the scheduler to the execution units when the total number of physical register read ports across all dispatch ports exceeds the read bandwidth of the physical register file.  The SIMD_PRF subevent applies to the following instructions: VDPPS, DPPS, VPCMPESTRI, PCMPESTRI, VPCMPESTRM, PCMPESTRM, VFMADD*, VFMADDSUB*, VFMSUB*, VMSUBADD*, VFNMADD*, VFNMSUB*.  See the Broadwell Optimization Guide for more information",
+},
+{
+	.name = "cpu_clk_unhalted.thread_any",
+	.event = "event=0x3c,any=1",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.thread_p_any",
+	.event = "event=0x3C,umask=0x0,any=1,period=2000003",
+	.desc = "Core cycles when at least one thread on the physical core is not in halt state",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_thread_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "int_misc.recovery_cycles_any",
+	.event = "event=0x0D,umask=0x3,any=1,cmask=1,period=2000003",
+	.desc = "Core cycles the allocator was stalled due to recovery from earlier clear event for any thread running on the physical core (e.g. misprediction or memory nuke)",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_1",
+	.event = "event=0xb1,umask=0x2,cmask=1,period=2000003",
+	.desc = "Cycles at least 1 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_2",
+	.event = "event=0xb1,umask=0x2,cmask=2,period=2000003",
+	.desc = "Cycles at least 2 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_3",
+	.event = "event=0xb1,umask=0x2,cmask=3,period=2000003",
+	.desc = "Cycles at least 3 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_ge_4",
+	.event = "event=0xb1,umask=0x2,cmask=4,period=2000003",
+	.desc = "Cycles at least 4 micro-op is executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "uops_executed.core_cycles_none",
+	.event = "inv=1,event=0xb1,umask=0x2,period=2000003",
+	.desc = "Cycles with no micro-ops executed from any thread on physical core",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk",
+	.event = "event=0x3C,umask=0x1,period=2000003",
+	.desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+	.long_desc = "Reference cycles when the thread is unhalted (counts at 100 MHz rate)",
+},
+{
+	.name = "cpu_clk_unhalted.ref_xclk_any",
+	.event = "event=0x3C,umask=0x1,any=1,period=2000003",
+	.desc = "Reference cycles when the at least one thread on the physical core is unhalted (counts at 100 MHz rate)",
+	.topic = "pipeline",
+},
+{
+	.name = "cpu_clk_unhalted.one_thread_active",
+	.event = "event=0x3C,umask=0x2,period=2000003",
+	.desc = "Count XClk pulses when this thread is unhalted and the other thread is halted",
+	.topic = "pipeline",
+},
+{
+	.name = "idq.empty",
+	.event = "event=0x79,umask=0x2,period=2000003",
+	.desc = "Instruction Decode Queue (IDQ) empty cycles",
+	.topic = "frontend",
+	.long_desc = "This counts the number of cycles that the instruction decoder queue is empty and can indicate that the application may be bound in the front end.  It does not determine whether there are uops being delivered to the Alloc stage since uops can be delivered by bypass skipping the Instruction Decode Queue (IDQ) when it is empty",
+},
+{
+	.name = "idq.mite_uops",
+	.event = "event=0x79,umask=0x4,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq.dsb_uops",
+	.event = "event=0x79,umask=0x8,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_dsb_uops",
+	.event = "event=0x79,umask=0x10,period=2000003",
+	.desc = "Uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops initiated by Decode Stream Buffer (DSB) that are being delivered to Instruction Decode Queue (IDQ) while the Microcode Sequencer (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_mite_uops",
+	.event = "event=0x79,umask=0x20,period=2000003",
+	.desc = "Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops initiated by MITE and delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_uops",
+	.event = "event=0x79,umask=0x30,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the total number of uops delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may \"bypass\" the IDQ. Uops maybe initiated by Decode Stream Buffer (DSB) or MITE",
+},
+{
+	.name = "idq.ms_cycles",
+	.event = "event=0x79,umask=0x30,cmask=1,period=2000003",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) while the Microcode Sequenser (MS) is busy. Counting includes uops that may \"bypass\" the IDQ. Uops maybe initiated by Decode Stream Buffer (DSB) or MITE",
+},
+{
+	.name = "idq.mite_cycles",
+	.event = "event=0x79,umask=0x4,cmask=1,period=2000003",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.dsb_cycles",
+	.event = "event=0x79,umask=0x8,cmask=1,period=2000003",
+	.desc = "Cycles when uops are being delivered to Instruction Decode Queue (IDQ) from Decode Stream Buffer (DSB) path",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops are being delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_dsb_cycles",
+	.event = "event=0x79,umask=0x10,cmask=1,period=2000003",
+	.desc = "Cycles when uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which uops initiated by Decode Stream Buffer (DSB) are being delivered to Instruction Decode Queue (IDQ) while the Microcode Sequencer (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.ms_dsb_occur",
+	.event = "edge=1,event=0x79,umask=0x10,cmask=1,period=2000003",
+	.desc = "Deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while Microcode Sequenser (MS) is busy",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of deliveries to Instruction Decode Queue (IDQ) initiated by Decode Stream Buffer (DSB) while the Microcode Sequencer (MS) is busy. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.all_dsb_cycles_4_uops",
+	.event = "event=0x79,umask=0x18,cmask=4,period=2000003",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles 4  uops were  delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.all_dsb_cycles_any_uops",
+	.event = "event=0x79,umask=0x18,cmask=1,period=2000003",
+	.desc = "Cycles Decode Stream Buffer (DSB) is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles  uops were  delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path. Counting includes uops that may \"bypass\" the IDQ",
+},
+{
+	.name = "idq.all_mite_cycles_4_uops",
+	.event = "event=0x79,umask=0x24,cmask=4,period=2000003",
+	.desc = "Cycles MITE is delivering 4 Uops",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles 4  uops were  delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq.all_mite_cycles_any_uops",
+	.event = "event=0x79,umask=0x24,cmask=1,period=2000003",
+	.desc = "Cycles MITE is delivering any Uop",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of cycles  uops were delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "idq.mite_all_uops",
+	.event = "event=0x79,umask=0x3c,period=2000003",
+	.desc = "Uops delivered to Instruction Decode Queue (IDQ) from MITE path",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops delivered to Instruction Decode Queue (IDQ) from the MITE path. Counting includes uops that may \"bypass\" the IDQ. This also means that uops are not being delivered from the Decode Stream Buffer (DSB)",
+},
+{
+	.name = "icache.hit",
+	.event = "event=0x80,umask=0x1,period=2000003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Reads. both cacheable and noncacheable, including UC fetches",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of both cacheable and noncacheable Instruction Cache, Streaming Buffer and Victim Cache Reads including UC fetches",
+},
+{
+	.name = "icache.misses",
+	.event = "event=0x80,umask=0x2,period=200003",
+	.desc = "Number of Instruction Cache, Streaming Buffer and Victim Cache Misses. Includes Uncacheable accesses",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of instruction cache, streaming buffer and victim cache misses. Counting includes UC accesses",
+},
+{
+	.name = "icache.ifdata_stall",
+	.event = "event=0x80,umask=0x4,period=2000003",
+	.desc = "Cycles where a code fetch is stalled due to L1 instruction-cache miss",
+	.topic = "frontend",
+	.long_desc = "This event counts cycles during which the demand fetch waits for data (wfdM104H) from L2 or iSB (opportunistic hit)",
+},
+{
+	.name = "idq_uops_not_delivered.core",
+	.event = "event=0x9C,umask=0x1,period=2000003",
+	.desc = "Uops not delivered to Resource Allocation Table (RAT) per thread when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts the number of uops not delivered to Resource Allocation Table (RAT) per thread adding ?4 ? x? when Resource Allocation Table (RAT) is not stalled and Instruction Decode Queue (IDQ) delivers x uops to Resource Allocation Table (RAT) (where x belongs to {0,1,2,3}). Counting does not cover cases when:\n a. IDQ-Resource Allocation Table (RAT) pipe serves the other thread;\n b. Resource Allocation Table (RAT) is stalled for the thread (including uop drops and clear BE conditions); \n c. Instruction Decode Queue (IDQ) delivers four uops",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_0_uops_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=4,period=2000003",
+	.desc = "Cycles per thread when 4 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts, on the per-thread basis, cycles when no uops are delivered to Resource Allocation Table (RAT). IDQ_Uops_Not_Delivered.core =4",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_1_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=3,period=2000003",
+	.desc = "Cycles per thread when 3 or more uops are not delivered to Resource Allocation Table (RAT) when backend of the machine is not stalled",
+	.topic = "frontend",
+	.long_desc = "This event counts, on the per-thread basis, cycles when less than 1 uop is  delivered to Resource Allocation Table (RAT). IDQ_Uops_Not_Delivered.core >=3",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_2_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=2,period=2000003",
+	.desc = "Cycles with less than 2 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_le_3_uop_deliv.core",
+	.event = "event=0x9C,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles with less than 3 uops delivered by the front end",
+	.topic = "frontend",
+},
+{
+	.name = "idq_uops_not_delivered.cycles_fe_was_ok",
+	.event = "inv=1,event=0x9C,umask=0x1,cmask=1,period=2000003",
+	.desc = "Counts cycles FE delivered 4 uops or Resource Allocation Table (RAT) was stalling FE",
+	.topic = "frontend",
+},
+{
+	.name = "dsb2mite_switches.penalty_cycles",
+	.event = "event=0xAB,umask=0x2,period=2000003",
+	.desc = "Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles",
+	.topic = "frontend",
+	.long_desc = "This event counts Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles. These cycles do not include uops routed through because of the switch itself, for example, when Instruction Decode Queue (IDQ) pre-allocation is unavailable, or Instruction Decode Queue (IDQ) is full. SBD-to-MITE switch true penalty cycles happen after the merge mux (MM) receives Decode Stream Buffer (DSB) Sync-indication until receiving the first MITE uop. \nMM is placed before Instruction Decode Queue (IDQ) to merge uops being fed from the MITE and Decode Stream Buffer (DSB) paths. Decode Stream Buffer (DSB) inserts the Sync-indication whenever a Decode Stream Buffer (DSB)-to-MITE switch occurs.\nPenalty: A Decode Stream Buffer (DSB) hit followed by a Decode Stream Buffer (DSB) miss can cost up to six cycles in which no uops are delivered to the IDQ. Most often, such switches from the Decode Stream Buffer (DSB) to the legacy pipeline cost 0?2 cycles",
+},
+{
+	.name = "idq.ms_switches",
+	.event = "edge=1,event=0x79,umask=0x30,cmask=1,period=2000003",
+	.desc = "Number of switches from DSB (Decode Stream Buffer) or MITE (legacy decode pipeline) to the Microcode Sequencer",
+	.topic = "frontend",
+},
+{
+	.name = "cpl_cycles.ring0",
+	.event = "event=0x5C,umask=0x1,period=2000003",
+	.desc = "Unhalted core cycles when the thread is in ring 0",
+	.topic = "other",
+	.long_desc = "This event counts the unhalted core cycles during which the thread is in the ring 0 privileged mode",
+},
+{
+	.name = "cpl_cycles.ring123",
+	.event = "event=0x5C,umask=0x2,period=2000003",
+	.desc = "Unhalted core cycles when thread is in rings 1, 2, or 3",
+	.topic = "other",
+	.long_desc = "This event counts unhalted core cycles during which the thread is in rings 1, 2, or 3",
+},
+{
+	.name = "cpl_cycles.ring0_trans",
+	.event = "edge=1,event=0x5C,umask=0x1,cmask=1,period=100007",
+	.desc = "Number of intervals between processor halts while thread is in ring 0",
+	.topic = "other",
+	.long_desc = "This event counts when there is a transition from ring 1,2 or 3 to ring0",
+},
+{
+	.name = "lock_cycles.split_lock_uc_lock_duration",
+	.event = "event=0x63,umask=0x1,period=2000003",
+	.desc = "Cycles when L1 and L2 are locked due to UC or split lock",
+	.topic = "other",
+	.long_desc = "This event counts cycles in which the L1 and L2 are locked due to a UC lock or split lock. A lock is asserted in case of locked memory access, due to noncacheable memory, locked operation that spans two cache lines, or a page walk from the noncacheable page table. L1D and L2 locks have a very high performance penalty and it is highly recommended to avoid such access",
+},
+{
+	.name = "misalign_mem_ref.loads",
+	.event = "event=0x05,umask=0x1,period=2000003",
+	.desc = "Speculative cache line split load uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "This event counts speculative cache-line split load uops dispatched to the L1 cache",
+},
+{
+	.name = "misalign_mem_ref.stores",
+	.event = "event=0x05,umask=0x2,period=2000003",
+	.desc = "Speculative cache line split STA uops dispatched to L1 cache",
+	.topic = "memory",
+	.long_desc = "This event counts speculative cache line split store-address (STA) uops dispatched to the L1 cache",
+},
+{
+	.name = "tx_mem.abort_conflict",
+	.event = "event=0x54,umask=0x1,period=2000003",
+	.desc = "Number of times a TSX line had a cache conflict",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX line had a cache conflict",
+},
+{
+	.name = "tx_mem.abort_capacity_write",
+	.event = "event=0x54,umask=0x2,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to an evicted line caused by a transaction overflow",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to an evicted line caused by a transaction overflow",
+},
+{
+	.name = "tx_mem.abort_hle_store_to_elided_lock",
+	.event = "event=0x54,umask=0x4,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to a non-release/commit store to lock",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to a non-release/commit store to lock",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_not_empty",
+	.event = "event=0x54,umask=0x8,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to commit but Lock Buffer not empty",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to commit but Lock Buffer not empty",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_mismatch",
+	.event = "event=0x54,umask=0x10,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to release/commit but data and address mismatch",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to release/commit but data and address mismatch",
+},
+{
+	.name = "tx_mem.abort_hle_elision_buffer_unsupported_alignment",
+	.event = "event=0x54,umask=0x20,period=2000003",
+	.desc = "Number of times a TSX Abort was triggered due to attempting an unsupported alignment from Lock Buffer",
+	.topic = "memory",
+	.long_desc = "Number of times a TSX Abort was triggered due to attempting an unsupported alignment from Lock Buffer",
+},
+{
+	.name = "tx_mem.hle_elision_buffer_full",
+	.event = "event=0x54,umask=0x40,period=2000003",
+	.desc = "Number of times we could not allocate Lock Buffer",
+	.topic = "memory",
+	.long_desc = "Number of times we could not allocate Lock Buffer",
+},
+{
+	.name = "tx_exec.misc1",
+	.event = "event=0x5d,umask=0x1,period=2000003",
+	.desc = "Counts the number of times a class of instructions that may cause a transactional abort was executed. Since this is the count of execution, it may not always cause a transactional abort",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by  a flowmarker",
+},
+{
+	.name = "tx_exec.misc2",
+	.event = "event=0x5d,umask=0x2,period=2000003",
+	.desc = "Counts the number of times a class of instructions (e.g., vzeroupper) that may cause a transactional abort was executed inside a transactional region",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by  a vzeroupper instruction",
+},
+{
+	.name = "tx_exec.misc3",
+	.event = "event=0x5d,umask=0x4,period=2000003",
+	.desc = "Counts the number of times an instruction execution caused the transactional nest count supported to be exceeded",
+	.topic = "memory",
+	.long_desc = "Unfriendly TSX abort triggered by a nest count that is too deep",
+},
+{
+	.name = "tx_exec.misc4",
+	.event = "event=0x5d,umask=0x8,period=2000003",
+	.desc = "Counts the number of times a XBEGIN instruction was executed inside an HLE transactional region",
+	.topic = "memory",
+	.long_desc = "RTM region detected inside HLE",
+},
+{
+	.name = "tx_exec.misc5",
+	.event = "event=0x5d,umask=0x10,period=2000003",
+	.desc = "Counts the number of times an HLE XACQUIRE instruction was executed inside an RTM transactional region",
+	.topic = "memory",
+},
+{
+	.name = "machine_clears.memory_ordering",
+	.event = "event=0xC3,umask=0x2,period=100003",
+	.desc = "Counts the number of machine clears due to memory order conflicts",
+	.topic = "memory",
+	.long_desc = "This event counts the number of memory ordering Machine Clears detected. Memory Ordering Machine Clears can result from one of the following:\n1. memory disambiguation,\n2. external snoop, or\n3. cross SMT-HW-thread snoop (stores) hitting load buffer",
+},
+{
+	.name = "hle_retired.start",
+	.event = "event=0xc8,umask=0x1,period=2000003",
+	.desc = "Number of times we entered an HLE region; does not count nested transactions",
+	.topic = "memory",
+	.long_desc = "Number of times we entered an HLE region\n does not count nested transactions",
+},
+{
+	.name = "hle_retired.commit",
+	.event = "event=0xc8,umask=0x2,period=2000003",
+	.desc = "Number of times HLE commit succeeded",
+	.topic = "memory",
+	.long_desc = "Number of times HLE commit succeeded",
+},
+{
+	.name = "hle_retired.aborted",
+	.event = "event=0xc8,umask=0x4,period=2000003",
+	.desc = "Number of times HLE abort was triggered (Precise event)",
+	.topic = "memory",
+	.long_desc = "Number of times HLE abort was triggered (Precise event)",
+},
+{
+	.name = "hle_retired.aborted_misc1",
+	.event = "event=0xc8,umask=0x8,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to various memory events (e.g., read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times an HLE abort was attributed to a Memory condition (See TSX_Memory event for additional details)",
+},
+{
+	.name = "hle_retired.aborted_misc2",
+	.event = "event=0xc8,umask=0x10,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to uncommon conditions",
+	.topic = "memory",
+	.long_desc = "Number of times the TSX watchdog signaled an HLE abort",
+},
+{
+	.name = "hle_retired.aborted_misc3",
+	.event = "event=0xc8,umask=0x20,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to HLE-unfriendly instructions",
+	.topic = "memory",
+	.long_desc = "Number of times a disallowed operation caused an HLE abort",
+},
+{
+	.name = "hle_retired.aborted_misc4",
+	.event = "event=0xc8,umask=0x40,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to incompatible memory type",
+	.topic = "memory",
+	.long_desc = "Number of times HLE caused a fault",
+},
+{
+	.name = "hle_retired.aborted_misc5",
+	.event = "event=0xc8,umask=0x80,period=2000003",
+	.desc = "Number of times an HLE execution aborted due to none of the previous 4 categories (e.g. interrupts)",
+	.topic = "memory",
+	.long_desc = "Number of times HLE aborted and was not due to the abort conditions in subevents 3-6",
+},
+{
+	.name = "rtm_retired.start",
+	.event = "event=0xc9,umask=0x1,period=2000003",
+	.desc = "Number of times we entered an RTM region; does not count nested transactions",
+	.topic = "memory",
+	.long_desc = "Number of times we entered an RTM region\n does not count nested transactions",
+},
+{
+	.name = "rtm_retired.commit",
+	.event = "event=0xc9,umask=0x2,period=2000003",
+	.desc = "Number of times RTM commit succeeded",
+	.topic = "memory",
+	.long_desc = "Number of times RTM commit succeeded",
+},
+{
+	.name = "rtm_retired.aborted",
+	.event = "event=0xc9,umask=0x4,period=2000003",
+	.desc = "Number of times RTM abort was triggered (Precise event)",
+	.topic = "memory",
+	.long_desc = "Number of times RTM abort was triggered  (Precise event)",
+},
+{
+	.name = "rtm_retired.aborted_misc1",
+	.event = "event=0xc9,umask=0x8,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to various memory events (e.g. read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times an RTM abort was attributed to a Memory condition (See TSX_Memory event for additional details)",
+},
+{
+	.name = "rtm_retired.aborted_misc2",
+	.event = "event=0xc9,umask=0x10,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to various memory events (e.g., read/write capacity and conflicts)",
+	.topic = "memory",
+	.long_desc = "Number of times the TSX watchdog signaled an RTM abort",
+},
+{
+	.name = "rtm_retired.aborted_misc3",
+	.event = "event=0xc9,umask=0x20,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to HLE-unfriendly instructions",
+	.topic = "memory",
+	.long_desc = "Number of times a disallowed operation caused an RTM abort",
+},
+{
+	.name = "rtm_retired.aborted_misc4",
+	.event = "event=0xc9,umask=0x40,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to incompatible memory type",
+	.topic = "memory",
+	.long_desc = "Number of times a RTM caused a fault",
+},
+{
+	.name = "rtm_retired.aborted_misc5",
+	.event = "event=0xc9,umask=0x80,period=2000003",
+	.desc = "Number of times an RTM execution aborted due to none of the previous 4 categories (e.g. interrupt)",
+	.topic = "memory",
+	.long_desc = "Number of times RTM aborted and was not due to the abort conditions in subevents 3-6",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_4",
+	.event = "event=0xCD,umask=0x1,period=100003,ldlat=0x4",
+	.desc = "Loads with latency value being above 4  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above four  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_8",
+	.event = "event=0xCD,umask=0x1,period=50021,ldlat=0x8",
+	.desc = "Loads with latency value being above 8  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above eight  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_16",
+	.event = "event=0xCD,umask=0x1,period=20011,ldlat=0x10",
+	.desc = "Loads with latency value being above 16  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 16  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_32",
+	.event = "event=0xCD,umask=0x1,period=100007,ldlat=0x20",
+	.desc = "Loads with latency value being above 32  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 32  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_64",
+	.event = "event=0xCD,umask=0x1,period=2003,ldlat=0x40",
+	.desc = "Loads with latency value being above 64  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 64  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_128",
+	.event = "event=0xCD,umask=0x1,period=1009,ldlat=0x80",
+	.desc = "Loads with latency value being above 128  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 128  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_256",
+	.event = "event=0xCD,umask=0x1,period=503,ldlat=0x100",
+	.desc = "Loads with latency value being above 256  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 256  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "mem_trans_retired.load_latency_gt_512",
+	.event = "event=0xCD,umask=0x1,period=101,ldlat=0x200",
+	.desc = "Loads with latency value being above 512  Spec update: BDM100, BDM35 (Must be precise)",
+	.topic = "memory",
+	.long_desc = "This event counts loads with latency value being above 512  Spec update: BDM100, BDM35 (Must be precise)",
+},
+{
+	.name = "offcore_response.all_requests.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc08fff",
+	.desc = "Counts all requests that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x087fc007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the L3 and clean or shared data is transferred from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103fc007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the L3 and the modified data is transferred from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x063bc007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the L3 and the data is returned from remote dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x06040007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_reads.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc007f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0604000244",
+	.desc = "Counts all demand & prefetch code reads that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00244",
+	.desc = "Counts all demand & prefetch code reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0604000122",
+	.desc = "Counts all demand & prefetch RFOs that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_rfo.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00122",
+	.desc = "Counts all demand & prefetch RFOs that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.remote_hit_forward",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x087fc00091",
+	.desc = "Counts all demand & prefetch data reads that miss the L3 and clean or shared data is transferred from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103fc00091",
+	.desc = "Counts all demand & prefetch data reads that miss the L3 and the modified data is transferred from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.remote_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x063bc00091",
+	.desc = "Counts all demand & prefetch data reads that miss the L3 and the data is returned from remote dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.local_dram",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x0604000091",
+	.desc = "Counts all demand & prefetch data reads that miss the L3 and the data is returned from local dram",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00091",
+	.desc = "Counts all demand & prefetch data reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00200",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.pf_llc_rfo.llc_miss.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3fbfc00100",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that miss in the L3",
+	.topic = "memory",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_miss.remote_hitm",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x103fc00002",
+	.desc = "Counts all demand data writes (RFOs) that miss the L3 and the modified data is transferred from remote cache",
+	.topic = "memory",
+},
+{
+	.name = "dtlb_load_misses.miss_causes_a_walk",
+	.event = "event=0x08,umask=0x1,period=100003",
+	.desc = "Load misses in all DTLB levels that cause page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_4k",
+	.event = "event=0x08,umask=0x2,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (4K)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_2m_4m",
+	.event = "event=0x08,umask=0x4,period=2000003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes (2M/4M)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_completed_1g",
+	.event = "event=0x08,umask=0x8,period=2000003",
+	.desc = "Load miss in all TLB levels causes a page walk that completes. (1G)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts load misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.walk_duration",
+	.event = "event=0x08,umask=0x10,period=2000003",
+	.desc = "Cycles when PMH is busy with page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of cycles while PMH is busy with the page walk  Spec update: BDM69",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit_4k",
+	.event = "event=0x08,umask=0x20,period=2000003",
+	.desc = "Load misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit_2m",
+	.event = "event=0x08,umask=0x40,period=2000003",
+	.desc = "Load misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.miss_causes_a_walk",
+	.event = "event=0x49,umask=0x1,period=100003",
+	.desc = "Store misses in all DTLB levels that cause page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_4k",
+	.event = "event=0x49,umask=0x2,period=100003",
+	.desc = "Store miss in all TLB levels causes a page walk that completes. (4K)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_2m_4m",
+	.event = "event=0x49,umask=0x4,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks (2M/4M)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_completed_1g",
+	.event = "event=0x49,umask=0x8,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks (1G)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.walk_duration",
+	.event = "event=0x49,umask=0x10,period=100003",
+	.desc = "Cycles when PMH is busy with page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of cycles while PMH is busy with the page walk  Spec update: BDM69",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit_4k",
+	.event = "event=0x49,umask=0x20,period=100003",
+	.desc = "Store misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit_2m",
+	.event = "event=0x49,umask=0x40,period=100003",
+	.desc = "Store misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+},
+{
+	.name = "ept.walk_cycles",
+	.event = "event=0x4F,umask=0x10,period=2000003",
+	.desc = "Cycle count for an Extended Page table walk",
+	.topic = "virtual memory",
+	.long_desc = "This event counts cycles for an extended page table walk. The Extended Page directory cache differs from standard TLB caches by the operating system that use it. Virtual machine operating systems use the extended page directory cache, while guest operating systems use the standard TLB caches",
+},
+{
+	.name = "itlb_misses.miss_causes_a_walk",
+	.event = "event=0x85,umask=0x1,period=100003",
+	.desc = "Misses at all ITLB levels that cause page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause page walks of any page size (4K/2M/4M/1G)  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_completed_4k",
+	.event = "event=0x85,umask=0x2,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (4K)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (4K page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_completed_2m_4m",
+	.event = "event=0x85,umask=0x4,period=100003",
+	.desc = "Code miss in all TLB levels causes a page walk that completes. (2M/4M)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (2M and 4M page sizes). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_completed_1g",
+	.event = "event=0x85,umask=0x8,period=100003",
+	.desc = "Store miss in all TLB levels causes a page walk that completes. (1G)  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts store misses in all DTLB levels that cause a completed page walk (1G  page size). The page walk can end with or without a fault  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.walk_duration",
+	.event = "event=0x85,umask=0x10,period=100003",
+	.desc = "Cycles when PMH is busy with page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of cycles while PMH is busy with the page walk  Spec update: BDM69",
+},
+{
+	.name = "itlb_misses.stlb_hit_4k",
+	.event = "event=0x85,umask=0x20,period=100003",
+	.desc = "Core misses that miss the  DTLB and hit the STLB (4K)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.stlb_hit_2m",
+	.event = "event=0x85,umask=0x40,period=100003",
+	.desc = "Code misses that miss the  DTLB and hit the STLB (2M)",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb.itlb_flush",
+	.event = "event=0xAE,umask=0x1,period=100007",
+	.desc = "Flushing of the Instruction TLB (ITLB) pages, includes 4k/2M/4M pages",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of flushes of the big or small ITLB pages. Counting include both TLB Flush (covering all sets) and TLB Set Clear (set-specific)",
+},
+{
+	.name = "page_walker_loads.dtlb_l1",
+	.event = "event=0xBC,umask=0x11,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L1+FB  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.itlb_l1",
+	.event = "event=0xBC,umask=0x21,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L1+FB  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_l2",
+	.event = "event=0xBC,umask=0x12,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L2  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.itlb_l2",
+	.event = "event=0xBC,umask=0x22,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L2  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_l3",
+	.event = "event=0xBC,umask=0x14,period=2000003",
+	.desc = "Number of DTLB page walker hits in the L3 + XSNP  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.itlb_l3",
+	.event = "event=0xBC,umask=0x24,period=2000003",
+	.desc = "Number of ITLB page walker hits in the L3 + XSNP  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "page_walker_loads.dtlb_memory",
+	.event = "event=0xBC,umask=0x18,period=2000003",
+	.desc = "Number of DTLB page walker hits in Memory  Spec update: BDM69, BDM98",
+	.topic = "virtual memory",
+},
+{
+	.name = "tlb_flush.dtlb_thread",
+	.event = "event=0xBD,umask=0x1,period=100007",
+	.desc = "DTLB flush attempts of the thread-specific entries",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of DTLB flush attempts of the thread-specific entries",
+},
+{
+	.name = "tlb_flush.stlb_any",
+	.event = "event=0xBD,umask=0x20,period=100007",
+	.desc = "STLB flush attempts",
+	.topic = "virtual memory",
+	.long_desc = "This event counts the number of any STLB flush attempts (such as entire, VPID, PCID, InvPage, CR3 write, and so on)",
+},
+{
+	.name = "dtlb_load_misses.walk_completed",
+	.event = "event=0x08,umask=0xe,period=100003",
+	.desc = "Demand load Miss in all translation lookaside buffer (TLB) levels causes a page walk that completes of any page size  Spec update: BDM69",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_load_misses.stlb_hit",
+	.event = "event=0x08,umask=0x60,period=2000003",
+	.desc = "Load operations that miss the first DTLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.walk_completed",
+	.event = "event=0x49,umask=0xe,period=100003",
+	.desc = "Store misses in all DTLB levels that cause completed page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+},
+{
+	.name = "dtlb_store_misses.stlb_hit",
+	.event = "event=0x49,umask=0x60,period=100003",
+	.desc = "Store operations that miss the first TLB level but hit the second and do not cause page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.walk_completed",
+	.event = "event=0x85,umask=0xe,period=100003",
+	.desc = "Misses in all ITLB levels that cause completed page walks  Spec update: BDM69",
+	.topic = "virtual memory",
+},
+{
+	.name = "itlb_misses.stlb_hit",
+	.event = "event=0x85,umask=0x60,period=100003",
+	.desc = "Operations that miss the first ITLB level but hit the second and do not cause any page walks",
+	.topic = "virtual memory",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_miss",
+	.event = "event=0x24,umask=0x21,period=200003",
+	.desc = "Demand Data Read miss L2, no rejects",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests that miss L2 cache. Only not rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.demand_data_rd_hit",
+	.event = "event=0x24,umask=0x41,period=200003",
+	.desc = "Demand Data Read requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests that hit L2 cache. Only not rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.l2_pf_miss",
+	.event = "event=0x24,umask=0x30,period=200003",
+	.desc = "L2 prefetch requests that miss L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of requests from the L2 hardware prefetchers that miss L2 cache",
+},
+{
+	.name = "l2_rqsts.l2_pf_hit",
+	.event = "event=0x24,umask=0x50,period=200003",
+	.desc = "L2 prefetch requests that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of requests from the L2 hardware prefetchers that hit L2 cache. L3 prefetch new types",
+},
+{
+	.name = "l2_rqsts.all_demand_data_rd",
+	.event = "event=0x24,umask=0xe1,period=200003",
+	.desc = "Demand Data Read requests",
+	.topic = "cache",
+	.long_desc = "This event counts the number of demand Data Read requests (including requests from L1D hardware prefetchers). These loads may hit or miss L2 cache. Only non rejected loads are counted",
+},
+{
+	.name = "l2_rqsts.all_rfo",
+	.event = "event=0x24,umask=0xe2,period=200003",
+	.desc = "RFO requests to L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of RFO (read for ownership) requests to L2 cache. L2 RFO requests include both L1D demand RFO misses as well as L1D RFO prefetches",
+},
+{
+	.name = "l2_rqsts.all_code_rd",
+	.event = "event=0x24,umask=0xe4,period=200003",
+	.desc = "L2 code requests",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of L2 code requests",
+},
+{
+	.name = "l2_rqsts.all_pf",
+	.event = "event=0x24,umask=0xf8,period=200003",
+	.desc = "Requests from L2 hardware prefetchers",
+	.topic = "cache",
+	.long_desc = "This event counts the total number of requests from the L2 hardware prefetchers",
+},
+{
+	.name = "l2_demand_rqsts.wb_hit",
+	.event = "event=0x27,umask=0x50,period=200003",
+	.desc = "Not rejected writebacks that hit L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts the number of WB requests that hit L2 cache",
+},
+{
+	.name = "longest_lat_cache.miss",
+	.event = "event=0x2E,umask=0x41,period=100003",
+	.desc = "Core-originated cacheable demand requests missed L3",
+	.topic = "cache",
+	.long_desc = "This event counts core-originated cacheable demand requests that miss the last level cache (LLC). Demand requests include loads, RFOs, and hardware prefetches from L1D, and instruction fetches from IFU",
+},
+{
+	.name = "longest_lat_cache.reference",
+	.event = "event=0x2E,umask=0x4f,period=100003",
+	.desc = "Core-originated cacheable demand requests that refer to L3",
+	.topic = "cache",
+	.long_desc = "This event counts core-originated cacheable demand requests that refer to the last level cache (LLC). Demand requests include loads, RFOs, and hardware prefetches from L1D, and instruction fetches from IFU",
+},
+{
+	.name = "l1d_pend_miss.pending",
+	.event = "event=0x48,umask=0x1,period=2000003",
+	.desc = "L1D miss oustandings duration in cycles",
+	.topic = "cache",
+	.long_desc = "This event counts duration of L1D miss outstanding, that is each cycle number of Fill Buffers (FB) outstanding required by Demand Reads. FB either is held by demand loads, or it is held by non-demand loads and gets hit at least once by demand. The valid outstanding interval is defined until the FB deallocation by one of the following ways: from FB allocation, if FB is allocated by demand; from the demand Hit FB, if it is allocated by hardware or software prefetch.\nNote: In the L1D, a Demand Read contains cacheable or noncacheable demand loads, including ones causing cache-line splits and reads due to page walks resulted from any request type",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles",
+	.event = "event=0x48,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles with L1D load Misses outstanding",
+	.topic = "cache",
+	.long_desc = "This event counts duration of L1D miss outstanding in cycles",
+},
+{
+	.name = "l1d.replacement",
+	.event = "event=0x51,umask=0x1,period=2000003",
+	.desc = "L1D data line replacements",
+	.topic = "cache",
+	.long_desc = "This event counts L1D data line replacements including opportunistic replacements, and replacements that require stall-for-replace or block-for-replace",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd",
+	.event = "event=0x60,umask=0x1,period=2000003",
+	.desc = "Offcore outstanding Demand Data Read transactions in uncore queue  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding Demand Data Read transactions in the super queue (SQ) every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor. See the corresponding Umask under OFFCORE_REQUESTS.\nNote: A prefetch promoted to Demand is counted from the promotion point  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.demand_code_rd",
+	.event = "event=0x60,umask=0x2,period=2000003",
+	.desc = "Offcore outstanding code reads transactions in SuperQueue (SQ), queue to uncore, every cycle  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding Code Reads transactions in the super queue every cycle. The \"Offcore outstanding\" state of the transaction lasts from the L2 miss until the sending transaction completion to requestor (SQ deallocation). See the corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.demand_rfo",
+	.event = "event=0x60,umask=0x4,period=2000003",
+	.desc = "Offcore outstanding RFO store transactions in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding RFO (store) transactions in the super queue (SQ) every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.all_data_rd",
+	.event = "event=0x60,umask=0x8,period=2000003",
+	.desc = "Offcore outstanding cacheable Core Data Read transactions in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding cacheable Core Data Read transactions in the super queue every cycle. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_data_rd",
+	.event = "event=0x60,umask=0x1,cmask=1,period=2000003",
+	.desc = "Cycles when offcore outstanding Demand Data Read transactions are present in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts cycles when offcore outstanding Demand Data Read transactions are present in the super queue (SQ). A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation)  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_data_rd",
+	.event = "event=0x60,umask=0x8,cmask=1,period=2000003",
+	.desc = "Cycles when offcore outstanding cacheable Core Data Read transactions are present in SuperQueue (SQ), queue to uncore  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts cycles when offcore outstanding cacheable Core Data Read transactions are present in the super queue. A transaction is considered to be in the Offcore outstanding state between L2 miss and transaction completion sent to requestor (SQ de-allocation). See corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "offcore_requests_outstanding.cycles_with_demand_rfo",
+	.event = "event=0x60,umask=0x4,cmask=1,period=2000003",
+	.desc = "Offcore outstanding demand rfo reads transactions in SuperQueue (SQ), queue to uncore, every cycle  Spec update: BDM76",
+	.topic = "cache",
+	.long_desc = "This event counts the number of offcore outstanding demand rfo Reads transactions in the super queue every cycle. The \"Offcore outstanding\" state of the transaction lasts from the L2 miss until the sending transaction completion to requestor (SQ deallocation). See the corresponding Umask under OFFCORE_REQUESTS  Spec update: BDM76",
+},
+{
+	.name = "lock_cycles.cache_lock_duration",
+	.event = "event=0x63,umask=0x2,period=2000003",
+	.desc = "Cycles when L1D is locked",
+	.topic = "cache",
+	.long_desc = "This event counts the number of cycles when the L1D is locked. It is a superset of the 0x1 mask (BUS_LOCK_CLOCKS.BUS_LOCK_DURATION)",
+},
+{
+	.name = "offcore_requests.demand_data_rd",
+	.event = "event=0xB0,umask=0x1,period=100003",
+	.desc = "Demand Data Read requests sent to uncore",
+	.topic = "cache",
+	.long_desc = "This event counts the Demand Data Read requests sent to uncore. Use it in conjunction with OFFCORE_REQUESTS_OUTSTANDING to determine average latency in the uncore",
+},
+{
+	.name = "offcore_requests.demand_code_rd",
+	.event = "event=0xB0,umask=0x2,period=100003",
+	.desc = "Cacheable and noncachaeble code read requests",
+	.topic = "cache",
+	.long_desc = "This event counts both cacheable and noncachaeble code read requests",
+},
+{
+	.name = "offcore_requests.demand_rfo",
+	.event = "event=0xB0,umask=0x4,period=100003",
+	.desc = "Demand RFO requests including regular RFOs, locks, ItoM",
+	.topic = "cache",
+	.long_desc = "This event counts the demand RFO (read for ownership) requests including regular RFOs, locks, ItoM",
+},
+{
+	.name = "offcore_requests.all_data_rd",
+	.event = "event=0xB0,umask=0x8,period=100003",
+	.desc = "Demand and prefetch data reads",
+	.topic = "cache",
+	.long_desc = "This event counts the demand and prefetch data reads. All Core Data Reads include cacheable \"Demands\" and L2 prefetchers (not L3 prefetchers). Counting also covers reads due to page walks resulted from any request type",
+},
+{
+	.name = "offcore_requests_buffer.sq_full",
+	.event = "event=0xb2,umask=0x1,period=2000003",
+	.desc = "Offcore requests buffer cannot take more entries for this thread core",
+	.topic = "cache",
+	.long_desc = "This event counts the number of cases when the offcore requests buffer cannot take more entries for the core. This can happen when the superqueue does not contain eligible entries, or when L1D writeback pending FIFO requests is full.\nNote: Writeback pending FIFO has six entries",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_loads",
+	.event = "event=0xD0,umask=0x11,period=100003",
+	.desc = "Retired load uops that miss the STLB  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts load uops with true STLB miss retired to the architected path. True STLB miss is an uop triggering page walk that gets completed without blocks, and later gets retired. This page walk can end up with or without a fault  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.stlb_miss_stores",
+	.event = "event=0xD0,umask=0x12,period=100003",
+	.desc = "Retired store uops that miss the STLB  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts store uops with true STLB miss retired to the architected path. True STLB miss is an uop triggering page walk that gets completed without blocks, and later gets retired. This page walk can end up with or without a fault  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.lock_loads",
+	.event = "event=0xD0,umask=0x21,period=100007",
+	.desc = "Retired load uops with locked access  Supports address when precise.  Spec update: BDM35 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts load uops with locked access retired to the architected path  Supports address when precise.  Spec update: BDM35 (Precise event)",
+},
+{
+	.name = "mem_uops_retired.split_loads",
+	.event = "event=0xD0,umask=0x41,period=100003",
+	.desc = "Retired load uops that split across a cacheline boundary  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts line-splitted load uops retired to the architected path. A line split is across 64B cache-line which includes a page split (4K)  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.split_stores",
+	.event = "event=0xD0,umask=0x42,period=100003",
+	.desc = "Retired store uops that split across a cacheline boundary  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts line-splitted store uops retired to the architected path. A line split is across 64B cache-line which includes a page split (4K)  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.all_loads",
+	.event = "event=0xD0,umask=0x81,period=2000003",
+	.desc = "All retired load uops  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts load uops retired to the architected path with a filter on bits 0 and 1 applied.\nNote: This event counts AVX-256bit load/store double-pump memory uops as a single uop at retirement. This event also counts SW prefetches  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_uops_retired.all_stores",
+	.event = "event=0xD0,umask=0x82,period=2000003",
+	.desc = "All retired store uops  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This event counts store uops retired to the architected path with a filter on bits 0 and 1 applied.\nNote: This event counts AVX-256bit load/store double-pump memory uops as a single uop at retirement  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_hit",
+	.event = "event=0xD1,umask=0x1,period=2000003",
+	.desc = "Retired load uops with L1 cache hits as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were hits in the nearest-level (L1) cache.\nNote: Only two data-sources of L1/FB are applicable for AVX-256bit  even though the corresponding AVX load could be serviced by a deeper level in the memory hierarchy. Data source is reported for the Low-half load. This event also counts SW prefetches independent of the actual data source  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_hit",
+	.event = "event=0xD1,umask=0x2,period=100003",
+	.desc = "Retired load uops with L2 cache hits as data sources  Supports address when precise.  Spec update: BDM35 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were hits in the mid-level (L2) cache  Supports address when precise.  Spec update: BDM35 (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l3_hit",
+	.event = "event=0xD1,umask=0x4,period=50021",
+	.desc = "Retired load uops which data sources were data hits in L3 without snoops required  Supports address when precise.  Spec update: BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were data hits in the last-level (L3) cache without snoops required  Supports address when precise.  Spec update: BDM100 (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l1_miss",
+	.event = "event=0xD1,umask=0x8,period=100003",
+	.desc = "Retired load uops misses in L1 cache as data sources  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were misses in the nearest-level (L1) cache. Counting excludes unknown and UC data source  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l2_miss",
+	.event = "event=0xD1,umask=0x10,period=50021",
+	.desc = "Miss in mid-level (L2) cache. Excludes Unknown data-source  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were misses in the mid-level (L2) cache. Counting excludes unknown and UC data source  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_retired.l3_miss",
+	.event = "event=0xD1,umask=0x20,period=100007",
+	.desc = "Miss in last-level (L3) cache. Excludes Unknown data-source  Supports address when precise.  Spec update: BDM100, BDE70 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_retired.hit_lfb",
+	.event = "event=0xD1,umask=0x40,period=100003",
+	.desc = "Retired load uops which data sources were load uops missed L1 but hit FB due to preceding miss to the same cache line with data not ready  Supports address when precise (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were load uops missed L1 but hit a fill buffer due to a preceding miss to the same cache line with the data not ready.\nNote: Only two data-sources of L1/FB are applicable for AVX-256bit  even though the corresponding AVX load could be serviced by a deeper level in the memory hierarchy. Data source is reported for the Low-half load  Supports address when precise (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_miss",
+	.event = "event=0xD2,umask=0x1,period=20011",
+	.desc = "Retired load uops which data sources were L3 hit and cross-core snoop missed in on-pkg core cache  Supports address when precise.  Spec update: BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were L3 Hit and a cross-core snoop missed in the on-pkg core cache  Supports address when precise.  Spec update: BDM100 (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_hit",
+	.event = "event=0xD2,umask=0x2,period=20011",
+	.desc = "Retired load uops which data sources were L3 and cross-core snoop hits in on-pkg core cache  Supports address when precise.  Spec update: BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were L3 hit and a cross-core snoop hit in the on-pkg core cache  Supports address when precise.  Spec update: BDM100 (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_hitm",
+	.event = "event=0xD2,umask=0x4,period=20011",
+	.desc = "Retired load uops which data sources were HitM responses from shared L3  Supports address when precise.  Spec update: BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were HitM responses from a core on same socket (shared L3)  Supports address when precise.  Spec update: BDM100 (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_hit_retired.xsnp_none",
+	.event = "event=0xD2,umask=0x8,period=100003",
+	.desc = "Retired load uops which data sources were hits in L3 without snoops required  Supports address when precise.  Spec update: BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "This is a non-precise version (that is, does not use PEBS) of the event that counts retired load uops which data sources were hits in the last-level (L3) cache without snoops required  Supports address when precise.  Spec update: BDM100 (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_miss_retired.local_dram",
+	.event = "event=0xD3,umask=0x1,period=100007",
+	.desc = "Data from local DRAM either Snoop not needed or Snoop Miss (RspI)  Supports address when precise.  Spec update: BDE70, BDM100 (Precise event)",
+	.topic = "cache",
+	.long_desc = "Retired load uop whose Data Source was: local DRAM either Snoop not needed or Snoop Miss (RspI)  Supports address when precise.  Spec update: BDE70, BDM100 (Precise event)",
+},
+{
+	.name = "mem_load_uops_l3_miss_retired.remote_dram",
+	.event = "event=0xD3,umask=0x4,period=100007",
+	.desc = "Retired load uop whose Data Source was: remote DRAM either Snoop not needed or Snoop Miss (RspI)  Supports address when precise.  Spec update: BDE70 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_miss_retired.remote_hitm",
+	.event = "event=0xD3,umask=0x10,period=100007",
+	.desc = "Retired load uop whose Data Source was: Remote cache HITM  Supports address when precise.  Spec update: BDE70 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "mem_load_uops_l3_miss_retired.remote_fwd",
+	.event = "event=0xD3,umask=0x20,period=100007",
+	.desc = "Retired load uop whose Data Source was: forwarded from remote cache  Supports address when precise.  Spec update: BDE70 (Precise event)",
+	.topic = "cache",
+},
+{
+	.name = "l2_trans.demand_data_rd",
+	.event = "event=0xF0,umask=0x1,period=200003",
+	.desc = "Demand Data Read requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts Demand Data Read requests that access L2 cache, including rejects",
+},
+{
+	.name = "l2_trans.rfo",
+	.event = "event=0xF0,umask=0x2,period=200003",
+	.desc = "RFO requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts Read for Ownership (RFO) requests that access L2 cache",
+},
+{
+	.name = "l2_trans.code_rd",
+	.event = "event=0xF0,umask=0x4,period=200003",
+	.desc = "L2 cache accesses when fetching instructions",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache accesses when fetching instructions",
+},
+{
+	.name = "l2_trans.all_pf",
+	.event = "event=0xF0,umask=0x8,period=200003",
+	.desc = "L2 or L3 HW prefetches that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L2 or L3 HW prefetches that access L2 cache including rejects",
+},
+{
+	.name = "l2_trans.l1d_wb",
+	.event = "event=0xF0,umask=0x10,period=200003",
+	.desc = "L1D writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L1D writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_fill",
+	.event = "event=0xF0,umask=0x20,period=200003",
+	.desc = "L2 fill requests that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L2 fill requests that access L2 cache",
+},
+{
+	.name = "l2_trans.l2_wb",
+	.event = "event=0xF0,umask=0x40,period=200003",
+	.desc = "L2 writebacks that access L2 cache",
+	.topic = "cache",
+	.long_desc = "This event counts L2 writebacks that access L2 cache",
+},
+{
+	.name = "l2_trans.all_requests",
+	.event = "event=0xF0,umask=0x80,period=200003",
+	.desc = "Transactions accessing L2 pipe",
+	.topic = "cache",
+	.long_desc = "This event counts transactions that access the L2 pipe including snoops, pagewalks, and so on",
+},
+{
+	.name = "l2_lines_in.i",
+	.event = "event=0xF1,umask=0x1,period=100003",
+	.desc = "L2 cache lines in I state filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines in the Invalidate state filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_in.s",
+	.event = "event=0xF1,umask=0x2,period=100003",
+	.desc = "L2 cache lines in S state filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines in the Shared state filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_in.e",
+	.event = "event=0xF1,umask=0x4,period=100003",
+	.desc = "L2 cache lines in E state filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines in the Exclusive state filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_in.all",
+	.event = "event=0xF1,umask=0x7,period=100003",
+	.desc = "L2 cache lines filling L2",
+	.topic = "cache",
+	.long_desc = "This event counts the number of L2 cache lines filling the L2. Counting does not cover rejects",
+},
+{
+	.name = "l2_lines_out.demand_clean",
+	.event = "event=0xF2,umask=0x5,period=100003",
+	.desc = "Clean L2 cache lines evicted by demand",
+	.topic = "cache",
+},
+{
+	.name = "sq_misc.split_lock",
+	.event = "event=0xf4,umask=0x10,period=100003",
+	.desc = "Split locks in SQ",
+	.topic = "cache",
+	.long_desc = "This event counts the number of split locks in the super queue",
+},
+{
+	.name = "l2_rqsts.rfo_hit",
+	.event = "event=0x24,umask=0x42,period=200003",
+	.desc = "RFO requests that hit L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.rfo_miss",
+	.event = "event=0x24,umask=0x22,period=200003",
+	.desc = "RFO requests that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.code_rd_hit",
+	.event = "event=0x24,umask=0x44,period=200003",
+	.desc = "L2 cache hits when fetching instructions, code reads",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.code_rd_miss",
+	.event = "event=0x24,umask=0x24,period=200003",
+	.desc = "L2 cache misses when fetching instructions",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_demand_miss",
+	.event = "event=0x24,umask=0x27,period=200003",
+	.desc = "Demand requests that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.all_demand_references",
+	.event = "event=0x24,umask=0xe7,period=200003",
+	.desc = "Demand requests to L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.miss",
+	.event = "event=0x24,umask=0x3f,period=200003",
+	.desc = "All requests that miss L2 cache",
+	.topic = "cache",
+},
+{
+	.name = "l2_rqsts.references",
+	.event = "event=0x24,umask=0xff,period=200003",
+	.desc = "All L2 requests",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response",
+	.event = "event=0xB7,umask=0x1,period=100003",
+	.desc = "Offcore response can be programmed only with a specific pair of event select and counter MSR, and with specific event codes and predefine mask bit value in a dedicated MSR to specify attributes of the offcore transaction",
+	.topic = "cache",
+},
+{
+	.name = "offcore_requests_outstanding.demand_data_rd_ge_6",
+	.event = "event=0x60,umask=0x1,cmask=6,period=2000003",
+	.desc = "Cycles with at least 6 offcore outstanding Demand Data Read transactions in uncore queue  Spec update: BDM76",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.pending_cycles_any",
+	.event = "event=0x48,umask=0x1,any=1,cmask=1,period=2000003",
+	.desc = "Cycles with L1D load Misses outstanding from any thread on physical core",
+	.topic = "cache",
+},
+{
+	.name = "l1d_pend_miss.fb_full",
+	.event = "event=0x48,umask=0x2,cmask=1,period=2000003",
+	.desc = "Cycles a demand request was blocked due to Fill Buffers inavailability",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_requests.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c8fff",
+	.desc = "Counts all requests that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c07f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_reads.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c07f7",
+	.desc = "Counts all data/code/rfo reads (demand & prefetch) that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_code_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0244",
+	.desc = "Counts all demand & prefetch code reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0122",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_rfo.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0122",
+	.desc = "Counts all demand & prefetch RFOs that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0091",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.all_data_rd.llc_hit.hit_other_core_no_fwd",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x04003c0091",
+	.desc = "Counts all demand & prefetch data reads that hit in the L3 and the snoops to sibling cores hit in either E/S state and the line is not forwarded",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_code_rd.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0200",
+	.desc = "Counts prefetch (that bring data to LLC only) code reads that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.pf_llc_rfo.llc_hit.any_response",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x3f803c0100",
+	.desc = "Counts all prefetch (that bring data to LLC only) RFOs that hit in the L3",
+	.topic = "cache",
+},
+{
+	.name = "offcore_response.demand_rfo.llc_hit.hitm_other_core",
+	.event = "event=0xB7,umask=0x1,period=100003,offcore_rsp=0x10003c0002",
+	.desc = "Counts all demand data writes (RFOs) that hit in the L3 and the snoop to one of the sibling cores hits the line in M state and the line is forwarded",
+	.topic = "cache",
+},
+{
+	.name = 0,
+	.event = 0,
+	.desc = 0,
+},
+};
+struct pmu_events_map pmu_events_map[] = {
+{
+	.cpuid = "GenuineIntel-6-56",
+	.version = "v5",
+	.type = "core",
+	.table = pme_broadwellde
+},
+{
+	.cpuid = "GenuineIntel-6-3D",
+	.version = "v17",
+	.type = "core",
+	.table = pme_broadwell
+},
+{
+	.cpuid = "GenuineIntel-6-47",
+	.version = "v17",
+	.type = "core",
+	.table = pme_broadwell
+},
+{
+	.cpuid = "GenuineIntel-6-4F",
+	.version = "v10",
+	.type = "core",
+	.table = pme_broadwellx
+},
+{
+	.cpuid = "GenuineIntel-6-1C",
+	.version = "v4",
+	.type = "core",
+	.table = pme_bonnell
+},
+{
+	.cpuid = "GenuineIntel-6-26",
+	.version = "v4",
+	.type = "core",
+	.table = pme_bonnell
+},
+{
+	.cpuid = "GenuineIntel-6-27",
+	.version = "v4",
+	.type = "core",
+	.table = pme_bonnell
+},
+{
+	.cpuid = "GenuineIntel-6-36",
+	.version = "v4",
+	.type = "core",
+	.table = pme_bonnell
+},
+{
+	.cpuid = "GenuineIntel-6-35",
+	.version = "v4",
+	.type = "core",
+	.table = pme_bonnell
+},
+{
+	.cpuid = "GenuineIntel-6-5C",
+	.version = "v8",
+	.type = "core",
+	.table = pme_goldmont
+},
+{
+	.cpuid = "GenuineIntel-6-3C",
+	.version = "v24",
+	.type = "core",
+	.table = pme_haswell
+},
+{
+	.cpuid = "GenuineIntel-6-45",
+	.version = "v24",
+	.type = "core",
+	.table = pme_haswell
+},
+{
+	.cpuid = "GenuineIntel-6-46",
+	.version = "v24",
+	.type = "core",
+	.table = pme_haswell
+},
+{
+	.cpuid = "GenuineIntel-6-3F",
+	.version = "v17",
+	.type = "core",
+	.table = pme_haswellx
+},
+{
+	.cpuid = "GenuineIntel-6-3A",
+	.version = "v18",
+	.type = "core",
+	.table = pme_ivybridge
+},
+{
+	.cpuid = "GenuineIntel-6-3E",
+	.version = "v19",
+	.type = "core",
+	.table = pme_ivytown
+},
+{
+	.cpuid = "GenuineIntel-6-2D",
+	.version = "v20",
+	.type = "core",
+	.table = pme_jaketown
+},
+{
+	.cpuid = "GenuineIntel-6-57",
+	.version = "v9",
+	.type = "core",
+	.table = pme_knightslanding
+},
+{
+	.cpuid = "GenuineIntel-6-1E",
+	.version = "v2",
+	.type = "core",
+	.table = pme_nehalemep
+},
+{
+	.cpuid = "GenuineIntel-6-1F",
+	.version = "v2",
+	.type = "core",
+	.table = pme_nehalemep
+},
+{
+	.cpuid = "GenuineIntel-6-1A",
+	.version = "v2",
+	.type = "core",
+	.table = pme_nehalemep
+},
+{
+	.cpuid = "GenuineIntel-6-2E",
+	.version = "v2",
+	.type = "core",
+	.table = pme_nehalemex
+},
+{
+	.cpuid = "GenuineIntel-6-4E",
+	.version = "v24",
+	.type = "core",
+	.table = pme_skylake
+},
+{
+	.cpuid = "GenuineIntel-6-5E",
+	.version = "v24",
+	.type = "core",
+	.table = pme_skylake
+},
+{
+	.cpuid = "GenuineIntel-6-8E",
+	.version = "v24",
+	.type = "core",
+	.table = pme_skylake
+},
+{
+	.cpuid = "GenuineIntel-6-9E",
+	.version = "v24",
+	.type = "core",
+	.table = pme_skylake
+},
+{
+	.cpuid = "GenuineIntel-6-37",
+	.version = "v13",
+	.type = "core",
+	.table = pme_silvermont
+},
+{
+	.cpuid = "GenuineIntel-6-4D",
+	.version = "v13",
+	.type = "core",
+	.table = pme_silvermont
+},
+{
+	.cpuid = "GenuineIntel-6-4C",
+	.version = "v13",
+	.type = "core",
+	.table = pme_silvermont
+},
+{
+	.cpuid = "GenuineIntel-6-2A",
+	.version = "v15",
+	.type = "core",
+	.table = pme_sandybridge
+},
+{
+	.cpuid = "GenuineIntel-6-2C",
+	.version = "v2",
+	.type = "core",
+	.table = pme_westmereep_dp
+},
+{
+	.cpuid = "GenuineIntel-6-2C",
+	.version = "v2",
+	.type = "core",
+	.table = pme_westmereep_dp
+},
+{
+	.cpuid = "GenuineIntel-6-25",
+	.version = "v2",
+	.type = "core",
+	.table = pme_westmereep_sp
+},
+{
+	.cpuid = "GenuineIntel-6-2F",
+	.version = "v2",
+	.type = "core",
+	.table = pme_westmereex
+},
+{
+	.cpuid = 0,
+	.version = 0,
+	.type = 0,
+	.table = 0,
+},
+};
